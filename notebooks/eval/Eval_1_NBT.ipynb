{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vltfd6l6n9cl",
    "outputId": "e000c568-9dc5-442e-8a65-0e2d4f668e4a"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# EVALUATION FRAMEWORK - SETUP CELL\n",
    "# Run this first in every notebook\n",
    "# ========================================\n",
    "\n",
    "# 1. Mount Google Drive (for data persistence)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# 2. Set working directory\n",
    "import os\n",
    "WORK_DIR = '/content/drive/MyDrive/data_preparation_project_2026'\n",
    "os.makedirs(WORK_DIR, exist_ok=True)\n",
    "os.chdir(WORK_DIR)\n",
    "\n",
    "# Create folder structure\n",
    "os.makedirs('data/baseline', exist_ok=True)\n",
    "os.makedirs('data/corrupted', exist_ok=True)\n",
    "os.makedirs('data/cleaned', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('figures', exist_ok=True)\n",
    "\n",
    "print(f\"‚úÖ Working directory: {WORK_DIR}\")\n",
    "print(f\"‚úÖ Folder structure created\")\n",
    "\n",
    "# 3. Install required packages\n",
    "# !pip install -q tensorflow-data-validation  # For Part 2\n",
    "!pip install -q statsmodels  # For Part 3 (McNemar's test)\n",
    "\n",
    "# 4. Import common libraries\n",
    "# import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from scipy.stats import ttest_rel, ks_2samp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plot style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "print(\"‚úÖ All libraries loaded\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VdLquXRXov4I"
   },
   "outputs": [],
   "source": [
    "!pip install datasets scikit-learn pandas numpy setuptools -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ELsRrsRno-6R",
    "outputId": "31365ca5-b96b-4795-cf30-88463864f822"
   },
   "outputs": [],
   "source": [
    "!pip install jenga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kk-Jk8IBpLpg",
    "outputId": "aa734cfb-2733-42d2-9856-5e3ae5d9f5c2"
   },
   "outputs": [],
   "source": [
    "beauty_path = \"/content/drive/MyDrive/data_preparation_project_2026/data/raw/All_Beauty.jsonl.gz\"\n",
    "\n",
    "import os\n",
    "print(\"exists:\", os.path.exists(beauty_path))\n",
    "print(\"size (MB):\", os.path.getsize(beauty_path) / (1024*1024))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ybwAqLFZpUDz",
    "outputId": "46f32bf4-3467-4995-c704-85aadb0bf72e"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open(beauty_path, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
    "    for i in range(3):\n",
    "        print(\"LINE\", i, \":\", f.readline()[:300])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u3FsiajppCjj",
    "outputId": "ec6eb52b-54f4-4d35-9159-ddc10d512fe0"
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "with gzip.open(beauty_path, \"rt\", encoding=\"utf-8\") as f:\n",
    "    for _ in range(5):\n",
    "        print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 224
    },
    "id": "M5ShBHx_pIGQ",
    "outputId": "361416ee-4043-430e-a892-ee33e5274c74"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "beauty_df = pd.read_json(beauty_path, lines=True, compression=\"gzip\")\n",
    "print(len(beauty_df))\n",
    "beauty_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fdoj6klhpcC0",
    "outputId": "287091f3-3394-4b06-a5ca-c3409aa1d840"
   },
   "outputs": [],
   "source": [
    "!ls\n",
    "\n",
    "!pwd\n",
    "\n",
    "!cd /content/drive/MyDrive/data_preparation_project_2026"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CVMEKyAw44i"
   },
   "source": [
    "### Part 1: Baseline Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C5dEnicQpifV",
    "outputId": "6e725a8b-4c6d-4fca-95ef-36807e9cbb61"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Quick script to check corrupted file sizes and clean bad checkpoints\n",
    "\"\"\"\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
    "FIXED_DIR = os.path.join(BASE_DIR, \"fixed\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CHECKING CORRUPTED FILES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "corrupted_dir = os.path.join(FIXED_DIR, \"corrupted\")\n",
    "if os.path.exists(corrupted_dir):\n",
    "    files = sorted([f for f in os.listdir(corrupted_dir) if f.endswith('.csv')])\n",
    "    for f in files:\n",
    "        path = os.path.join(corrupted_dir, f)\n",
    "        df = pd.read_csv(path)\n",
    "        print(f\"{f:40s} ‚Üí {len(df):,} rows\")\n",
    "else:\n",
    "    print(\"‚ùå No corrupted directory found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DELETING ALL CHECKPOINTS (force re-run)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "checkpoint_dir = os.path.join(FIXED_DIR, \"checkpoints\")\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.json')]\n",
    "    for f in files:\n",
    "        path = os.path.join(checkpoint_dir, f)\n",
    "        os.remove(path)\n",
    "        print(f\"‚úÖ Deleted: {f}\")\n",
    "    print(f\"\\n‚úÖ Deleted {len(files)} checkpoints\")\n",
    "else:\n",
    "    print(\"‚ùå No checkpoint directory found\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHECKING BASELINE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "baseline_path = os.path.join(FIXED_DIR, \"baseline/amazon_clean_with_rowid.csv\")\n",
    "if os.path.exists(baseline_path):\n",
    "    df = pd.read_csv(baseline_path)\n",
    "    print(f\"Baseline with row_id: {len(df):,} rows\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "else:\n",
    "    print(\"‚ùå No baseline found\")\n",
    "\n",
    "print(\"\\n‚úÖ Ready to re-run evaluation pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h7WDHlg1ptJ1",
    "outputId": "e018d5b2-3369-4c1f-d724-57de927caa05"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BATCH 1 EVALUATION - First 4 Corruptions\n",
    "Runs: missing_values, broken_chars, swapped_text, missing_labels\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
    "FIXED_DIR = os.path.join(BASE_DIR, \"fixed_batch1\")\n",
    "TFIDF_MAX_FEATURES = 5000\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "SUBSET_SIZE = 100000\n",
    "\n",
    "# =========================\n",
    "# IMPORT TEAM CODE\n",
    "# =========================\n",
    "sys.path.append(os.path.join(BASE_DIR, \"team_code\"))\n",
    "import inject_extreme as inject\n",
    "import clean\n",
    "\n",
    "# =========================\n",
    "# UTILITIES\n",
    "# =========================\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "\n",
    "def ensure_dirs():\n",
    "    for sub in [\"baseline\", \"corrupted\", \"cleaned\", \"results\", \"splits\"]:\n",
    "        os.makedirs(os.path.join(FIXED_DIR, sub), exist_ok=True)\n",
    "\n",
    "def prepare_baseline_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"label\", \"text\"])\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    df = df[df[\"text\"] != \"nan\"]\n",
    "    df = df[df[\"text\"].str.len() > 0]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"row_id\"] = df.index\n",
    "    return df\n",
    "\n",
    "def valid_row_mask(df: pd.DataFrame) -> pd.Series:\n",
    "    text = df[\"text\"].astype(str)\n",
    "    label = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
    "    mask = label.notna() & text.notna()\n",
    "    mask &= (text != \"nan\") & (text.str.len() > 0)\n",
    "    mask &= label.apply(lambda x: float(x).is_integer())\n",
    "    return mask\n",
    "\n",
    "def build_model():\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, stop_words=\"english\")),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(df_train, df_test, allow_invalid=False):\n",
    "    if allow_invalid:\n",
    "        train_valid = df_train.copy()\n",
    "        test_valid = df_test.copy()\n",
    "        train_valid['text'] = train_valid['text'].fillna('')\n",
    "        test_valid['text'] = test_valid['text'].fillna('')\n",
    "        train_valid['label'] = pd.to_numeric(train_valid['label'], errors='coerce').fillna(3).astype(int)\n",
    "        test_valid['label'] = pd.to_numeric(test_valid['label'], errors='coerce').fillna(3).astype(int)\n",
    "    else:\n",
    "        train_mask = valid_row_mask(df_train)\n",
    "        test_mask = valid_row_mask(df_test)\n",
    "        train_valid = df_train[train_mask].copy()\n",
    "        test_valid = df_test[test_mask].copy()\n",
    "\n",
    "    X_train = train_valid[\"text\"].astype(str).values\n",
    "    y_train = pd.to_numeric(train_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
    "    X_test = test_valid[\"text\"].astype(str).values\n",
    "    y_test = pd.to_numeric(test_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metrics = compute_metrics(y_test, y_pred)\n",
    "    stats = {\n",
    "        \"train_total\": len(df_train),\n",
    "        \"train_valid\": len(train_valid),\n",
    "        \"test_total\": len(df_test),\n",
    "        \"test_valid\": len(test_valid),\n",
    "    }\n",
    "\n",
    "    del model, X_train, y_train, X_test, y_test, train_valid, test_valid\n",
    "    clear_memory()\n",
    "    return metrics, stats\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH 1 EVALUATION - First 4 Corruptions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ensure_dirs()\n",
    "\n",
    "# Load baseline\n",
    "print(\"\\nüìä Loading baseline...\")\n",
    "baseline_path = os.path.join(BASE_DIR, \"data/baseline/amazon_reviews_1M.csv\")\n",
    "df_raw = pd.read_csv(baseline_path)\n",
    "print(f\"   Loaded: {len(df_raw):,} rows\")\n",
    "\n",
    "if len(df_raw) > SUBSET_SIZE:\n",
    "    df_raw = df_raw.sample(n=SUBSET_SIZE, random_state=RANDOM_STATE)\n",
    "    print(f\"   Sampled: {SUBSET_SIZE:,} rows\")\n",
    "\n",
    "df_baseline = prepare_baseline_data(df_raw)\n",
    "del df_raw\n",
    "clear_memory()\n",
    "print(f\"   Cleaned: {len(df_baseline):,} rows\")\n",
    "\n",
    "# Split\n",
    "df_train_clean, df_test_clean = train_test_split(\n",
    "    df_baseline, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df_baseline[\"label\"]\n",
    ")\n",
    "train_ids = set(df_train_clean[\"row_id\"].tolist())\n",
    "test_ids = set(df_test_clean[\"row_id\"].tolist())\n",
    "print(f\"   Split: {len(df_train_clean):,} train / {len(df_test_clean):,} test\")\n",
    "\n",
    "# Baseline eval\n",
    "print(\"\\nüìà Baseline evaluation...\")\n",
    "metrics_base, stats_base = train_and_evaluate(df_train_clean, df_test_clean, allow_invalid=False)\n",
    "print(f\"   Acc: {metrics_base['accuracy']:.4f} | Prec: {metrics_base['precision']:.4f} | Rec: {metrics_base['recall']:.4f} | F1: {metrics_base['f1']:.4f}\")\n",
    "\n",
    "all_results = []\n",
    "def record_result(exp_id, stage, metrics, stats):\n",
    "    row = {\"experiment\": exp_id, \"stage\": stage, **stats, **metrics}\n",
    "    all_results.append(row)\n",
    "\n",
    "record_result(\"BASELINE\", \"CLEAN\", metrics_base, stats_base)\n",
    "\n",
    "# Experiments\n",
    "experiments = [\n",
    "    (\"01_missing_values\", inject.apply_missing_values, {}),\n",
    "    (\"02_broken_chars\", inject.apply_broken_characters, {}),\n",
    "    (\"03_swapped_text\", inject.apply_swapped_text, {}),\n",
    "    (\"04_missing_labels\", inject.apply_missing_labels, {}),\n",
    "]\n",
    "\n",
    "for idx, (exp_id, corrupt_func, kwargs) in enumerate(experiments, start=1):\n",
    "    print(f\"\\n[{idx}/4] {exp_id}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Corrupt\n",
    "    print(\"   üî¥ Corrupting...\")\n",
    "    df_corrupt = corrupt_func(df_baseline.copy(), **kwargs)\n",
    "    df_corrupt.to_csv(os.path.join(FIXED_DIR, f\"corrupted/{exp_id}.csv\"), index=False)\n",
    "\n",
    "    # Split\n",
    "    df_train_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(train_ids)].copy()\n",
    "    df_test_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(test_ids)].copy()\n",
    "\n",
    "    # Eval corrupted\n",
    "    print(\"   üî¥ Training on CORRUPTED...\")\n",
    "    metrics_corrupt, stats_corrupt = train_and_evaluate(df_train_corrupt, df_test_corrupt, allow_invalid=True)\n",
    "    record_result(exp_id, \"CORRUPTED\", metrics_corrupt, stats_corrupt)\n",
    "    print(f\"      Acc: {metrics_corrupt['accuracy']:.4f} | Prec: {metrics_corrupt['precision']:.4f} | Rec: {metrics_corrupt['recall']:.4f} | F1: {metrics_corrupt['f1']:.4f}\")\n",
    "\n",
    "    del df_train_corrupt, df_test_corrupt\n",
    "    clear_memory()\n",
    "\n",
    "    # Clean\n",
    "    print(\"   üü¢ Cleaning...\")\n",
    "    df_cleaned = clean.clean_all(df_corrupt)\n",
    "    df_cleaned.to_csv(os.path.join(FIXED_DIR, f\"cleaned/{exp_id}_cleaned.csv\"), index=False)\n",
    "    del df_corrupt\n",
    "    clear_memory()\n",
    "\n",
    "    # Split\n",
    "    df_train_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(train_ids)].copy()\n",
    "    df_test_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(test_ids)].copy()\n",
    "\n",
    "    # Eval cleaned\n",
    "    print(\"   üü¢ Training on CLEANED...\")\n",
    "    metrics_cleaned, stats_cleaned = train_and_evaluate(df_train_cleaned, df_test_cleaned, allow_invalid=False)\n",
    "    record_result(exp_id, \"CLEANED\", metrics_cleaned, stats_cleaned)\n",
    "    print(f\"      Acc: {metrics_cleaned['accuracy']:.4f} | Prec: {metrics_cleaned['precision']:.4f} | Rec: {metrics_cleaned['recall']:.4f} | F1: {metrics_cleaned['f1']:.4f}\")\n",
    "\n",
    "    # Recovery\n",
    "    acc_drop = metrics_base['accuracy'] - metrics_corrupt['accuracy']\n",
    "    acc_recovery = metrics_cleaned['accuracy'] - metrics_corrupt['accuracy']\n",
    "    recovery_pct = (acc_recovery / acc_drop * 100) if acc_drop != 0 else 0\n",
    "    print(f\"      üìä Recovery: {recovery_pct:.1f}%\")\n",
    "\n",
    "    del df_train_cleaned, df_test_cleaned, df_cleaned\n",
    "    clear_memory()\n",
    "\n",
    "# Save results\n",
    "results_path = os.path.join(FIXED_DIR, \"results/batch1_results.csv\")\n",
    "pd.DataFrame(all_results).to_csv(results_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH 1 COMPLETE!\")\n",
    "print(f\"‚úÖ Results: {results_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wbvms0dgp_UD",
    "outputId": "bc61042f-d274-40b1-bea1-d08972cc1fe9"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "BATCH 2 EVALUATION - Remaining 6 Corruptions\n",
    "Runs: swapped_labels_manual, noise_injection, truncation, combined_jenga, combined_manual, doomsday\n",
    "\"\"\"\n",
    "\n",
    "import os, sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
    "FIXED_DIR = os.path.join(BASE_DIR, \"fixed_batch2\")\n",
    "TFIDF_MAX_FEATURES = 50000\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "SUBSET_SIZE = 100000\n",
    "\n",
    "# =========================\n",
    "# IMPORT TEAM CODE\n",
    "# =========================\n",
    "sys.path.append(os.path.join(BASE_DIR, \"team_code\"))\n",
    "import inject_extreme as inject\n",
    "import clean\n",
    "\n",
    "# =========================\n",
    "# UTILITIES\n",
    "# =========================\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "\n",
    "def ensure_dirs():\n",
    "    for sub in [\"baseline\", \"corrupted\", \"cleaned\", \"results\", \"splits\"]:\n",
    "        os.makedirs(os.path.join(FIXED_DIR, sub), exist_ok=True)\n",
    "\n",
    "def prepare_baseline_data(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df[\"text\"] = df[\"text\"].astype(str)\n",
    "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"label\", \"text\"])\n",
    "    df[\"label\"] = df[\"label\"].astype(int)\n",
    "    df = df[df[\"text\"] != \"nan\"]\n",
    "    df = df[df[\"text\"].str.len() > 0]\n",
    "    df = df.reset_index(drop=True)\n",
    "    df[\"row_id\"] = df.index\n",
    "    return df\n",
    "\n",
    "def valid_row_mask(df: pd.DataFrame) -> pd.Series:\n",
    "    text = df[\"text\"].astype(str)\n",
    "    label = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
    "    mask = label.notna() & text.notna()\n",
    "    mask &= (text != \"nan\") & (text.str.len() > 0)\n",
    "    mask &= label.apply(lambda x: float(x).is_integer())\n",
    "    return mask\n",
    "\n",
    "def build_model():\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, stop_words=\"english\")),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(df_train, df_test, allow_invalid=False):\n",
    "    if allow_invalid:\n",
    "        train_valid = df_train.copy()\n",
    "        test_valid = df_test.copy()\n",
    "        train_valid['text'] = train_valid['text'].fillna('')\n",
    "        test_valid['text'] = test_valid['text'].fillna('')\n",
    "        train_valid['label'] = pd.to_numeric(train_valid['label'], errors='coerce').fillna(3).astype(int)\n",
    "        test_valid['label'] = pd.to_numeric(test_valid['label'], errors='coerce').fillna(3).astype(int)\n",
    "    else:\n",
    "        train_mask = valid_row_mask(df_train)\n",
    "        test_mask = valid_row_mask(df_test)\n",
    "        train_valid = df_train[train_mask].copy()\n",
    "        test_valid = df_test[test_mask].copy()\n",
    "\n",
    "    X_train = train_valid[\"text\"].astype(str).values\n",
    "    y_train = pd.to_numeric(train_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
    "    X_test = test_valid[\"text\"].astype(str).values\n",
    "    y_test = pd.to_numeric(test_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metrics = compute_metrics(y_test, y_pred)\n",
    "    stats = {\n",
    "        \"train_total\": len(df_train),\n",
    "        \"train_valid\": len(train_valid),\n",
    "        \"test_total\": len(df_test),\n",
    "        \"test_valid\": len(test_valid),\n",
    "    }\n",
    "\n",
    "    del model, X_train, y_train, X_test, y_test, train_valid, test_valid\n",
    "    clear_memory()\n",
    "    return metrics, stats\n",
    "\n",
    "# =========================\n",
    "# MAIN\n",
    "# =========================\n",
    "print(\"=\"*80)\n",
    "print(\"BATCH 2 EVALUATION - Remaining 6 Corruptions\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "ensure_dirs()\n",
    "\n",
    "# Load baseline\n",
    "print(\"\\nüìä Loading baseline...\")\n",
    "baseline_path = os.path.join(BASE_DIR, \"data/baseline/amazon_reviews_1M.csv\")\n",
    "df_raw = pd.read_csv(baseline_path)\n",
    "print(f\"   Loaded: {len(df_raw):,} rows\")\n",
    "\n",
    "if len(df_raw) > SUBSET_SIZE:\n",
    "    df_raw = df_raw.sample(n=SUBSET_SIZE, random_state=RANDOM_STATE)\n",
    "    print(f\"   Sampled: {SUBSET_SIZE:,} rows\")\n",
    "\n",
    "df_baseline = prepare_baseline_data(df_raw)\n",
    "del df_raw\n",
    "clear_memory()\n",
    "print(f\"   Cleaned: {len(df_baseline):,} rows\")\n",
    "\n",
    "# Split\n",
    "df_train_clean, df_test_clean = train_test_split(\n",
    "    df_baseline, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df_baseline[\"label\"]\n",
    ")\n",
    "train_ids = set(df_train_clean[\"row_id\"].tolist())\n",
    "test_ids = set(df_test_clean[\"row_id\"].tolist())\n",
    "print(f\"   Split: {len(df_train_clean):,} train / {len(df_test_clean):,} test\")\n",
    "\n",
    "# Baseline eval\n",
    "print(\"\\nüìà Baseline evaluation...\")\n",
    "metrics_base, stats_base = train_and_evaluate(df_train_clean, df_test_clean, allow_invalid=False)\n",
    "print(f\"   Acc: {metrics_base['accuracy']:.4f} | Prec: {metrics_base['precision']:.4f} | Rec: {metrics_base['recall']:.4f} | F1: {metrics_base['f1']:.4f}\")\n",
    "\n",
    "all_results = []\n",
    "def record_result(exp_id, stage, metrics, stats):\n",
    "    row = {\"experiment\": exp_id, \"stage\": stage, **stats, **metrics}\n",
    "    all_results.append(row)\n",
    "\n",
    "record_result(\"BASELINE\", \"CLEAN\", metrics_base, stats_base)\n",
    "\n",
    "# Experiments\n",
    "experiments = [\n",
    "    (\"05_swapped_labels_manual\", inject.apply_swapped_labels_manual, {}),\n",
    "    (\"06_noise_injection\", inject.apply_noise_injection, {}),\n",
    "    (\"07_truncation\", inject.apply_truncation, {}),\n",
    "    (\"08_combined_jenga\", inject.apply_combined_jenga, {}),\n",
    "    (\"09_combined_manual\", inject.apply_combined_manual, {}),\n",
    "    (\"10_doomsday\", inject.apply_doomsday, {}),\n",
    "]\n",
    "\n",
    "for idx, (exp_id, corrupt_func, kwargs) in enumerate(experiments, start=1):\n",
    "    print(f\"\\n[{idx}/6] {exp_id}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # Corrupt\n",
    "    print(\"   üî¥ Corrupting...\")\n",
    "    df_corrupt = corrupt_func(df_baseline.copy(), **kwargs)\n",
    "    df_corrupt.to_csv(os.path.join(FIXED_DIR, f\"corrupted/{exp_id}.csv\"), index=False)\n",
    "\n",
    "    # Split\n",
    "    df_train_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(train_ids)].copy()\n",
    "    df_test_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(test_ids)].copy()\n",
    "\n",
    "    # Eval corrupted\n",
    "    print(\"   üî¥ Training on CORRUPTED...\")\n",
    "    metrics_corrupt, stats_corrupt = train_and_evaluate(df_train_corrupt, df_test_corrupt, allow_invalid=True)\n",
    "    record_result(exp_id, \"CORRUPTED\", metrics_corrupt, stats_corrupt)\n",
    "    print(f\"      Acc: {metrics_corrupt['accuracy']:.4f} | Prec: {metrics_corrupt['precision']:.4f} | Rec: {metrics_corrupt['recall']:.4f} | F1: {metrics_corrupt['f1']:.4f}\")\n",
    "\n",
    "    del df_train_corrupt, df_test_corrupt\n",
    "    clear_memory()\n",
    "\n",
    "    # Clean\n",
    "    print(\"   üü¢ Cleaning...\")\n",
    "    df_cleaned = clean.clean_all(df_corrupt)\n",
    "    df_cleaned.to_csv(os.path.join(FIXED_DIR, f\"cleaned/{exp_id}_cleaned.csv\"), index=False)\n",
    "    del df_corrupt\n",
    "    clear_memory()\n",
    "\n",
    "    # Split\n",
    "    df_train_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(train_ids)].copy()\n",
    "    df_test_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(test_ids)].copy()\n",
    "\n",
    "    # Eval cleaned\n",
    "    print(\"   üü¢ Training on CLEANED...\")\n",
    "    metrics_cleaned, stats_cleaned = train_and_evaluate(df_train_cleaned, df_test_cleaned, allow_invalid=False)\n",
    "    record_result(exp_id, \"CLEANED\", metrics_cleaned, stats_cleaned)\n",
    "    print(f\"      Acc: {metrics_cleaned['accuracy']:.4f} | Prec: {metrics_cleaned['precision']:.4f} | Rec: {metrics_cleaned['recall']:.4f} | F1: {metrics_cleaned['f1']:.4f}\")\n",
    "\n",
    "    # Recovery\n",
    "    acc_drop = metrics_base['accuracy'] - metrics_corrupt['accuracy']\n",
    "    acc_recovery = metrics_cleaned['accuracy'] - metrics_corrupt['accuracy']\n",
    "    recovery_pct = (acc_recovery / acc_drop * 100) if acc_drop != 0 else 0\n",
    "    print(f\"      üìä Recovery: {recovery_pct:.1f}%\")\n",
    "\n",
    "    del df_train_cleaned, df_test_cleaned, df_cleaned\n",
    "    clear_memory()\n",
    "\n",
    "# Save results\n",
    "results_path = os.path.join(FIXED_DIR, \"results/batch2_results.csv\")\n",
    "pd.DataFrame(all_results).to_csv(results_path, index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BATCH 2 COMPLETE!\")\n",
    "print(f\"‚úÖ Results: {results_path}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfY6-d9JwyPE"
   },
   "source": [
    "### Part 2: Statitical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lpsh0LcGsKMC",
    "outputId": "2c16ae15-8722-4701-ce13-e65a42152c3f"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Statistical Analysis - CORRECTED\n",
    "Uses absolute improvements and proper interpretation\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"STATISTICAL ANALYSIS - CORRECTED\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load results\n",
    "batch1 = pd.read_csv(os.path.join(BASE_DIR, \"fixed_batch1/results/batch1_results.csv\"))\n",
    "batch2 = pd.read_csv(os.path.join(BASE_DIR, \"fixed_batch2/results/batch2_results.csv\"))\n",
    "df_all = pd.concat([batch1, batch2], ignore_index=True)\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(df_all)} result rows\")\n",
    "\n",
    "# Get baseline\n",
    "baseline_acc = df_all[df_all['stage'] == 'CLEAN']['accuracy'].values[0]\n",
    "\n",
    "# ============================================\n",
    "# ANALYSIS\n",
    "# ============================================\n",
    "stats_results = []\n",
    "\n",
    "experiments = df_all['experiment'].unique()\n",
    "experiments = [e for e in experiments if e != 'BASELINE']\n",
    "\n",
    "for exp in experiments:\n",
    "    exp_data = df_all[df_all['experiment'] == exp]\n",
    "\n",
    "    if len(exp_data) < 2:\n",
    "        continue\n",
    "\n",
    "    corrupted = exp_data[exp_data['stage'] == 'CORRUPTED']\n",
    "    cleaned = exp_data[exp_data['stage'] == 'CLEANED']\n",
    "\n",
    "    if len(corrupted) == 0 or len(cleaned) == 0:\n",
    "        continue\n",
    "\n",
    "    acc_corrupt = corrupted['accuracy'].values[0]\n",
    "    acc_clean = cleaned['accuracy'].values[0]\n",
    "\n",
    "    # Calculate metrics\n",
    "    damage = baseline_acc - acc_corrupt\n",
    "    recovery_abs = acc_clean - acc_corrupt\n",
    "    recovery_pct = (recovery_abs / damage * 100) if damage != 0 else 0\n",
    "\n",
    "    # Interpret damage\n",
    "    def interpret_damage(d):\n",
    "        abs_d = abs(d)\n",
    "        if abs_d < 0.03:\n",
    "            return \"Minimal\"\n",
    "        elif abs_d < 0.08:\n",
    "            return \"Moderate\"\n",
    "        elif abs_d < 0.15:\n",
    "            return \"Severe\"\n",
    "        else:\n",
    "            return \"Critical\"\n",
    "\n",
    "    # Interpret recovery\n",
    "    def interpret_recovery(r):\n",
    "        if r < 0:\n",
    "            return \"Negative (Cleaning Hurt)\"\n",
    "        elif r < 0.01:\n",
    "            return \"Negligible\"\n",
    "        elif r < 0.05:\n",
    "            return \"Small\"\n",
    "        elif r < 0.10:\n",
    "            return \"Moderate\"\n",
    "        else:\n",
    "            return \"Large\"\n",
    "\n",
    "    # Statistical significance (using sample size and improvement)\n",
    "    # For n=20k test samples, improvements >1% are typically significant\n",
    "    n_test = 19997  # Your test size\n",
    "\n",
    "    # Standard error of accuracy difference\n",
    "    se = np.sqrt((acc_corrupt * (1 - acc_corrupt) + acc_clean * (1 - acc_clean)) / n_test)\n",
    "\n",
    "    # Z-score\n",
    "    if se > 0:\n",
    "        z_score = recovery_abs / se\n",
    "        # Two-tailed p-value approximation\n",
    "        from scipy.stats import norm\n",
    "        p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
    "    else:\n",
    "        p_value = 1.0\n",
    "\n",
    "    sig = \"***\" if p_value < 0.001 else (\"**\" if p_value < 0.01 else (\"*\" if p_value < 0.05 else \"ns\"))\n",
    "\n",
    "    stats_results.append({\n",
    "        'Experiment': exp.replace('_', ' ').title(),\n",
    "        'Baseline': f\"{baseline_acc:.3f}\",\n",
    "        'Corrupted': f\"{acc_corrupt:.3f}\",\n",
    "        'Cleaned': f\"{acc_clean:.3f}\",\n",
    "        'Damage': f\"{damage:.3f}\",\n",
    "        'Damage Level': interpret_damage(damage),\n",
    "        'Recovery (Œî)': f\"{recovery_abs:+.3f}\",\n",
    "        'Recovery (%)': f\"{recovery_pct:.1f}%\",\n",
    "        'Recovery Level': interpret_recovery(recovery_abs),\n",
    "        'p-value': f\"{p_value:.4f}\",\n",
    "        'Significant': sig,\n",
    "    })\n",
    "\n",
    "df_stats = pd.DataFrame(stats_results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CORRUPTION IMPACT & RECOVERY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n\" + df_stats.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "stats_path = os.path.join(BASE_DIR, \"results/statistical_analysis_corrected.csv\")\n",
    "os.makedirs(os.path.join(BASE_DIR, \"results\"), exist_ok=True)\n",
    "df_stats.to_csv(stats_path, index=False)\n",
    "print(f\"\\n‚úÖ Saved: {stats_path}\")\n",
    "\n",
    "# ============================================\n",
    "# SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nüìä Corruption Damage Levels:\")\n",
    "for level in ['Minimal', 'Moderate', 'Severe', 'Critical']:\n",
    "    count = len(df_stats[df_stats['Damage Level'] == level])\n",
    "    print(f\"   {level:12s}: {count}/{len(df_stats)}\")\n",
    "\n",
    "print(f\"\\nüìä Recovery Effectiveness:\")\n",
    "for level in ['Negative (Cleaning Hurt)', 'Negligible', 'Small', 'Moderate', 'Large']:\n",
    "    count = len(df_stats[df_stats['Recovery Level'] == level])\n",
    "    print(f\"   {level:30s}: {count}/{len(df_stats)}\")\n",
    "\n",
    "print(f\"\\nüìä Statistical Significance:\")\n",
    "for sig in ['***', '**', '*', 'ns']:\n",
    "    count = len(df_stats[df_stats['Significant'] == sig])\n",
    "    label = \"p<0.001\" if sig == '***' else (\"p<0.01\" if sig == '**' else (\"p<0.05\" if sig == '*' else \"p‚â•0.05\"))\n",
    "    print(f\"   {sig:4s} ({label:7s}): {count}/{len(df_stats)}\")\n",
    "\n",
    "# ============================================\n",
    "# KEY FINDINGS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Most damaging\n",
    "most_damage_idx = df_stats['Damage'].str.replace('-', '').astype(float).idxmax()\n",
    "most_damage = df_stats.iloc[most_damage_idx]\n",
    "print(f\"\\nüí• Most Damaging Corruption:\")\n",
    "print(f\"   {most_damage['Experiment']}\")\n",
    "print(f\"   {most_damage['Baseline']} ‚Üí {most_damage['Corrupted']} (Œî = {most_damage['Damage']})\")\n",
    "print(f\"   Level: {most_damage['Damage Level']}\")\n",
    "\n",
    "# Best recovery\n",
    "best_recovery_idx = df_stats['Recovery (Œî)'].str.replace('+', '').astype(float).idxmax()\n",
    "best_recovery = df_stats.iloc[best_recovery_idx]\n",
    "print(f\"\\nüéØ Best Recovery:\")\n",
    "print(f\"   {best_recovery['Experiment']}\")\n",
    "print(f\"   Improvement: {best_recovery['Recovery (Œî)']} ({best_recovery['Recovery (%)']})\")\n",
    "print(f\"   Level: {best_recovery['Recovery Level']}\")\n",
    "print(f\"   Significance: {best_recovery['Significant']} (p={best_recovery['p-value']})\")\n",
    "\n",
    "# Worst recovery (most negative)\n",
    "worst_recovery_idx = df_stats['Recovery (Œî)'].str.replace('+', '').astype(float).idxmin()\n",
    "worst_recovery = df_stats.iloc[worst_recovery_idx]\n",
    "print(f\"\\n‚ùå Worst Recovery (Cleaning Hurt):\")\n",
    "print(f\"   {worst_recovery['Experiment']}\")\n",
    "print(f\"   Change: {worst_recovery['Recovery (Œî)']} ({worst_recovery['Recovery (%)']})\")\n",
    "print(f\"   Level: {worst_recovery['Recovery Level']}\")\n",
    "\n",
    "# Count significant improvements\n",
    "sig_count = len(df_stats[df_stats['Significant'].isin(['*', '**', '***'])])\n",
    "print(f\"\\nüìä Overall: {sig_count}/{len(df_stats)} corruptions showed statistically significant recovery\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìñ Interpretation Guide:\")\n",
    "print(\"\\n   Damage Levels (absolute accuracy drop):\")\n",
    "print(\"   ‚Ä¢ Minimal:   < 3%\")\n",
    "print(\"   ‚Ä¢ Moderate:  3-8%\")\n",
    "print(\"   ‚Ä¢ Severe:    8-15%\")\n",
    "print(\"   ‚Ä¢ Critical:  > 15%\")\n",
    "print(\"\\n   Recovery Levels (absolute accuracy gain from cleaning):\")\n",
    "print(\"   ‚Ä¢ Negative:  Cleaning made it worse\")\n",
    "print(\"   ‚Ä¢ Negligible: < 1% improvement\")\n",
    "print(\"   ‚Ä¢ Small:     1-5% improvement\")\n",
    "print(\"   ‚Ä¢ Moderate:  5-10% improvement\")\n",
    "print(\"   ‚Ä¢ Large:     > 10% improvement\")\n",
    "print(\"\\n   Significance:\")\n",
    "print(\"   ‚Ä¢ ***: p < 0.001 (highly significant)\")\n",
    "print(\"   ‚Ä¢ **:  p < 0.01  (very significant)\")\n",
    "print(\"   ‚Ä¢ *:   p < 0.05  (significant)\")\n",
    "print(\"   ‚Ä¢ ns:  p ‚â• 0.05  (not significant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4lMDnWPP2Cuw"
   },
   "source": [
    "# TFDV\n",
    "\n",
    "Change Runtime on Collab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "yHJGV3d1vPwW",
    "outputId": "87b29fd3-fa6b-435f-ef62-2257f26d77da"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "\n",
    "!pip install \\\n",
    "  tensorflow==2.15.1 \\\n",
    "  tensorflow-metadata==1.15.0 \\\n",
    "  tensorflow-data-validation==1.15.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ukbkNU_bxL7d",
    "outputId": "0a3b127c-1cda-4602-ff03-ab207d21b072"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "print(\" Python:\", sys.version)\n",
    "print(\" TensorFlow:\", tf.__version__)\n",
    "print(\" TFDV:\", tfdv.__version__)\n",
    "print(\"\\n TFDV successfully installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7LAD4Tobz-Df",
    "outputId": "2942ec0f-cb2d-41a1-ff47-6758bde79ada"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
    "\n",
    "print(\"CHECKING YOUR ACTUAL STRUCTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check what exists\n",
    "for batch in ['fixed_batch1', 'fixed_batch2']:\n",
    "    print(f\"\\n{batch}:\")\n",
    "    for sub in ['baseline', 'corrupted', 'cleaned', 'results']:\n",
    "        path = os.path.join(BASE_DIR, batch, sub)\n",
    "        if os.path.exists(path):\n",
    "            files = os.listdir(path)\n",
    "            print(f\"  ‚úÖ {sub}/ ({len(files)} files)\")\n",
    "        else:\n",
    "            print(f\"  ‚ùå {sub}/ (missing)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w_SQwa-_ynf3",
    "outputId": "daa459a2-cdde-4634-d9bd-c3a72254552c"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PART 2: TensorFlow Data Validation (TFDV)\n",
    "Detect corruption at the data level before model training\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow_data_validation as tfdv\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 2: TENSORFLOW DATA VALIDATION (TFDV)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nGoal: Detect corruption BEFORE training models\")\n",
    "print(\"Show which corruptions can be caught via data validation\\n\")\n",
    "\n",
    "# ============================================\n",
    "# 1. LOAD BASELINE & GENERATE SCHEMA\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Generate Schema from Clean Baseline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Try multiple possible baseline locations\n",
    "baseline_paths = [\n",
    "    os.path.join(BASE_DIR, \"fixed/baseline/amazon_clean_with_rowid.csv\"),\n",
    "    os.path.join(BASE_DIR, \"fixed_batch1/baseline/amazon_clean_with_rowid.csv\"),\n",
    "    os.path.join(BASE_DIR, \"data/baseline/amazon_reviews_1M.csv\"),\n",
    "]\n",
    "\n",
    "df_baseline = None\n",
    "baseline_path = None\n",
    "\n",
    "for path in baseline_paths:\n",
    "    if os.path.exists(path):\n",
    "        baseline_path = path\n",
    "        df_baseline = pd.read_csv(path)\n",
    "        break\n",
    "\n",
    "if df_baseline is None:\n",
    "    print(\"‚ùå ERROR: Could not find baseline file in any expected location:\")\n",
    "    for path in baseline_paths:\n",
    "        print(f\"   - {path}\")\n",
    "    print(\"\\nPlease check your file paths!\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"‚úÖ Loaded baseline from: {baseline_path}\")\n",
    "print(f\"   {len(df_baseline):,} rows\")\n",
    "print(f\"   Columns: {list(df_baseline.columns)}\")\n",
    "\n",
    "# Generate statistics from baseline\n",
    "print(\"\\nüìä Generating baseline statistics...\")\n",
    "baseline_stats = tfdv.generate_statistics_from_dataframe(df_baseline[['text', 'label']])\n",
    "\n",
    "# Infer schema\n",
    "print(\"üìã Inferring schema from baseline...\")\n",
    "schema = tfdv.infer_schema(statistics=baseline_stats)\n",
    "\n",
    "print(\"\\n‚úÖ Schema inferred:\")\n",
    "print(f\"   Features: {[f.name for f in schema.feature]}\")\n",
    "\n",
    "# ============================================\n",
    "# 2. VALIDATE CORRUPTED DATASETS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Validate Corrupted Datasets Against Schema\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# All corrupted datasets - try multiple directories\n",
    "corrupted_files = [\n",
    "    ('01_missing_values', ['fixed_batch1', 'fixed']),\n",
    "    ('02_broken_chars', ['fixed_batch1', 'fixed']),\n",
    "    ('03_swapped_text', ['fixed_batch1', 'fixed']),\n",
    "    ('04_missing_labels', ['fixed_batch1', 'fixed']),\n",
    "    ('05_swapped_labels_manual', ['fixed_batch2', 'fixed']),\n",
    "    ('06_noise_injection', ['fixed_batch2', 'fixed']),\n",
    "    ('07_truncation', ['fixed_batch2', 'fixed']),\n",
    "    ('08_combined_jenga', ['fixed_batch2', 'fixed']),\n",
    "    ('09_combined_manual', ['fixed_batch2', 'fixed']),\n",
    "    ('10_doomsday', ['fixed_batch2', 'fixed']),\n",
    "]\n",
    "\n",
    "tfdv_results = []\n",
    "\n",
    "for exp_id, batch_dirs in corrupted_files:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Validating: {exp_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    try:\n",
    "        # Try multiple possible paths\n",
    "        df_corrupt = None\n",
    "        corrupt_path = None\n",
    "\n",
    "        for batch_dir in batch_dirs:\n",
    "            path = os.path.join(BASE_DIR, batch_dir, f\"corrupted/{exp_id}.csv\")\n",
    "            if os.path.exists(path):\n",
    "                corrupt_path = path\n",
    "                df_corrupt = pd.read_csv(path)\n",
    "                break\n",
    "\n",
    "        if df_corrupt is None:\n",
    "            print(f\"   ‚ö†Ô∏è  File not found, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"   Loaded: {len(df_corrupt):,} rows\")\n",
    "\n",
    "        # Generate statistics\n",
    "        print(\"   üìä Generating statistics...\")\n",
    "        corrupt_stats = tfdv.generate_statistics_from_dataframe(df_corrupt[['text', 'label']])\n",
    "\n",
    "        # Validate against schema\n",
    "        print(\"   üîç Validating against schema...\")\n",
    "        anomalies = tfdv.validate_statistics(statistics=corrupt_stats, schema=schema)\n",
    "\n",
    "        # Parse anomalies\n",
    "        anomaly_info = MessageToDict(anomalies)\n",
    "\n",
    "        # Count anomalies\n",
    "        n_anomalies = len(anomaly_info.get('anomalyInfo', {}))\n",
    "\n",
    "        # Extract specific anomaly types\n",
    "        anomaly_types = []\n",
    "        missing_features = 0\n",
    "        high_missing = 0\n",
    "        unexpected_values = 0\n",
    "\n",
    "        for feature_name, anomaly_data in anomaly_info.get('anomalyInfo', {}).items():\n",
    "            reason = anomaly_data.get('shortDescription', '')\n",
    "            anomaly_types.append(f\"{feature_name}: {reason}\")\n",
    "\n",
    "            if 'missing' in reason.lower():\n",
    "                missing_features += 1\n",
    "                # Check severity\n",
    "                if 'column dropped' in reason.lower() or 'high' in reason.lower():\n",
    "                    high_missing += 1\n",
    "\n",
    "            if 'unexpected' in reason.lower() or 'out of bounds' in reason.lower():\n",
    "                unexpected_values += 1\n",
    "\n",
    "        # Calculate drift (compare distributions)\n",
    "        print(\"   üìâ Calculating distribution drift...\")\n",
    "\n",
    "        # For text: check completeness\n",
    "        text_complete_baseline = df_baseline['text'].notna().sum() / len(df_baseline)\n",
    "        text_complete_corrupt = df_corrupt['text'].notna().sum() / len(df_corrupt)\n",
    "        text_drift = abs(text_complete_baseline - text_complete_corrupt)\n",
    "\n",
    "        # For label: check completeness + distribution\n",
    "        label_complete_baseline = df_baseline['label'].notna().sum() / len(df_baseline)\n",
    "        label_complete_corrupt = df_corrupt['label'].notna().sum() / len(df_corrupt)\n",
    "        label_drift = abs(label_complete_baseline - label_complete_corrupt)\n",
    "\n",
    "        # Distribution shift (KS test would be here, simplified to completeness for now)\n",
    "        overall_drift = (text_drift + label_drift) / 2\n",
    "\n",
    "        # Detectability score (can TFDV catch this corruption?)\n",
    "        if n_anomalies > 0 or overall_drift > 0.05:\n",
    "            detectability = \"High\"\n",
    "        elif overall_drift > 0.01:\n",
    "            detectability = \"Medium\"\n",
    "        else:\n",
    "            detectability = \"Low\"\n",
    "\n",
    "        # Print results\n",
    "        print(f\"\\n   ‚úÖ TFDV Results:\")\n",
    "        print(f\"      Anomalies detected: {n_anomalies}\")\n",
    "        print(f\"      Missing features: {missing_features}\")\n",
    "        print(f\"      Unexpected values: {unexpected_values}\")\n",
    "        print(f\"      Text completeness: {text_complete_corrupt:.1%} (Œî = {text_drift:.1%})\")\n",
    "        print(f\"      Label completeness: {label_complete_corrupt:.1%} (Œî = {label_drift:.1%})\")\n",
    "        print(f\"      Overall drift: {overall_drift:.1%}\")\n",
    "        print(f\"      Detectability: {detectability}\")\n",
    "\n",
    "        # Store results\n",
    "        tfdv_results.append({\n",
    "            'Experiment': exp_id.replace('_', ' ').title(),\n",
    "            'Anomalies': n_anomalies,\n",
    "            'Missing Features': missing_features,\n",
    "            'Unexpected Values': unexpected_values,\n",
    "            'Text Completeness': f\"{text_complete_corrupt:.1%}\",\n",
    "            'Text Drift': f\"{text_drift:.1%}\",\n",
    "            'Label Completeness': f\"{label_complete_corrupt:.1%}\",\n",
    "            'Label Drift': f\"{label_drift:.1%}\",\n",
    "            'Overall Drift': f\"{overall_drift:.1%}\",\n",
    "            'Detectability': detectability,\n",
    "            'Anomaly Details': '; '.join(anomaly_types[:3]) if anomaly_types else 'None'\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        tfdv_results.append({\n",
    "            'Experiment': exp_id.replace('_', ' ').title(),\n",
    "            'Anomalies': 'Error',\n",
    "            'Missing Features': 'Error',\n",
    "            'Unexpected Values': 'Error',\n",
    "            'Text Completeness': 'Error',\n",
    "            'Text Drift': 'Error',\n",
    "            'Label Completeness': 'Error',\n",
    "            'Label Drift': 'Error',\n",
    "            'Overall Drift': 'Error',\n",
    "            'Detectability': 'Error',\n",
    "            'Anomaly Details': str(e)\n",
    "        })\n",
    "\n",
    "# ============================================\n",
    "# 3. SAVE RESULTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TFDV RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_tfdv = pd.DataFrame(tfdv_results)\n",
    "print(\"\\n\" + df_tfdv.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "tfdv_path = os.path.join(BASE_DIR, \"results/PART2_TFDV_Analysis.csv\")\n",
    "os.makedirs(os.path.join(BASE_DIR, \"results\"), exist_ok=True)\n",
    "df_tfdv.to_csv(tfdv_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved: {tfdv_path}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. SUMMARY STATISTICS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETECTABILITY ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Filter out errors\n",
    "df_tfdv_valid = df_tfdv[df_tfdv['Detectability'] != 'Error']\n",
    "\n",
    "print(f\"\\nüìä Corruption Detectability via TFDV:\")\n",
    "for level in ['High', 'Medium', 'Low']:\n",
    "    count = len(df_tfdv_valid[df_tfdv_valid['Detectability'] == level])\n",
    "    total = len(df_tfdv_valid)\n",
    "    pct = count / total * 100 if total > 0 else 0\n",
    "    print(f\"   {level:8s}: {count}/{total} ({pct:.0f}%)\")\n",
    "\n",
    "# Average drift by detectability\n",
    "if len(df_tfdv_valid) > 0:\n",
    "    print(f\"\\nüìä Average Drift by Detectability:\")\n",
    "    for level in ['High', 'Medium', 'Low']:\n",
    "        subset = df_tfdv_valid[df_tfdv_valid['Detectability'] == level]\n",
    "        if len(subset) > 0:\n",
    "            # Convert drift strings to float\n",
    "            drifts = subset['Overall Drift'].str.rstrip('%').astype(float)\n",
    "            avg_drift = drifts.mean()\n",
    "            print(f\"   {level:8s}: {avg_drift:.1f}% average drift\")\n",
    "\n",
    "# Anomaly detection rate\n",
    "if len(df_tfdv_valid) > 0:\n",
    "    detected = len(df_tfdv_valid[df_tfdv_valid['Anomalies'] > 0])\n",
    "    total = len(df_tfdv_valid)\n",
    "    print(f\"\\nüìä Detection Rate:\")\n",
    "    print(f\"   {detected}/{total} corruptions generated anomalies ({detected/total*100:.0f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PART 2 COMPLETE - TFDV ANALYSIS DONE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîë Key Insight:\")\n",
    "print(\"   TFDV can detect structural corruptions (missing values, labels)\")\n",
    "print(\"   but struggles with semantic corruptions (swapped labels, noise)\")\n",
    "print(\"   This validates the need for model-based evaluation (Part 1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lMZKqwWhy-sU",
    "outputId": "a0136765-ebab-4fcb-8425-9034761678bb"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PART 2: TENSORFLOW DATA VALIDATION (TFDV) - COMPREHENSIVE\n",
    "Complete end-to-end data validation analysis\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tensorflow_data_validation as tfdv\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "from scipy.stats import ks_2samp, entropy\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PART 2: TENSORFLOW DATA VALIDATION (TFDV) - COMPREHENSIVE\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nEnd-to-end data quality assessment:\")\n",
    "print(\"1. Schema validation & anomaly detection\")\n",
    "print(\"2. Distribution drift & skew analysis\")\n",
    "print(\"3. Feature-level statistics\")\n",
    "print(\"4. Corruption detectability scoring\")\n",
    "print()\n",
    "\n",
    "# ============================================\n",
    "# 1. LOAD BASELINE & GENERATE SCHEMA\n",
    "# ============================================\n",
    "print(\"=\"*80)\n",
    "print(\"STEP 1: Generate Schema from Clean Baseline\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "baseline_paths = [\n",
    "    os.path.join(BASE_DIR, \"fixed/baseline/amazon_clean_with_rowid.csv\"),\n",
    "    os.path.join(BASE_DIR, \"data/baseline/amazon_reviews_1M.csv\"),\n",
    "]\n",
    "\n",
    "baseline_path = None\n",
    "for path in baseline_paths:\n",
    "    if os.path.exists(path):\n",
    "        baseline_path = path\n",
    "        break\n",
    "\n",
    "if baseline_path is None:\n",
    "    print(\"‚ùå ERROR: Could not find baseline file\")\n",
    "    sys.exit(1)\n",
    "\n",
    "print(f\"‚úÖ Loaded baseline from: {baseline_path}\")\n",
    "df_baseline = pd.read_csv(baseline_path)\n",
    "\n",
    "# Clean if needed\n",
    "if 'amazon_reviews_1M.csv' in baseline_path:\n",
    "    print(\"   üßπ Cleaning raw baseline...\")\n",
    "    df_baseline[\"text\"] = df_baseline[\"text\"].astype(str)\n",
    "    df_baseline[\"label\"] = pd.to_numeric(df_baseline[\"label\"], errors=\"coerce\")\n",
    "    df_baseline = df_baseline.dropna(subset=[\"label\", \"text\"])\n",
    "    df_baseline[\"label\"] = df_baseline[\"label\"].astype(int)\n",
    "    df_baseline = df_baseline[df_baseline[\"text\"] != \"nan\"]\n",
    "    df_baseline = df_baseline[df_baseline[\"text\"].str.len() > 0]\n",
    "    if len(df_baseline) > 100000:\n",
    "        df_baseline = df_baseline.sample(n=100000, random_state=42)\n",
    "\n",
    "print(f\"   {len(df_baseline):,} rows\")\n",
    "\n",
    "# Generate baseline statistics\n",
    "print(\"\\nüìä Generating baseline statistics...\")\n",
    "baseline_stats = tfdv.generate_statistics_from_dataframe(df_baseline[['text', 'label']])\n",
    "\n",
    "# Infer schema\n",
    "print(\"üìã Inferring schema...\")\n",
    "schema = tfdv.infer_schema(statistics=baseline_stats)\n",
    "\n",
    "# Get baseline label distribution\n",
    "baseline_label_dist = df_baseline['label'].value_counts(normalize=True).sort_index()\n",
    "print(f\"\\nüìä Baseline Label Distribution:\")\n",
    "for label, pct in baseline_label_dist.items():\n",
    "    print(f\"   Label {label}: {pct:.1%}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Schema inferred with {len(schema.feature)} features\")\n",
    "\n",
    "# ============================================\n",
    "# 2. COMPREHENSIVE VALIDATION\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Comprehensive Corruption Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "corrupted_files = [\n",
    "    ('01_missing_values', ['fixed', 'fixed_batch1']),\n",
    "    ('02_broken_chars', ['fixed', 'fixed_batch1']),\n",
    "    ('03_swapped_text', ['fixed', 'fixed_batch1']),\n",
    "    ('04_missing_labels', ['fixed', 'fixed_batch1']),\n",
    "    ('05_swapped_labels_manual', ['fixed', 'fixed_batch2']),\n",
    "    ('06_noise_injection', ['fixed', 'fixed_batch2']),\n",
    "    ('07_truncation', ['fixed', 'fixed_batch2']),\n",
    "    ('08_combined_jenga', ['fixed', 'fixed_batch2']),\n",
    "    ('09_combined_manual', ['fixed', 'fixed_batch2']),\n",
    "    ('10_doomsday', ['fixed', 'fixed_batch2']),\n",
    "]\n",
    "\n",
    "tfdv_results = []\n",
    "\n",
    "for exp_id, batch_dirs in corrupted_files:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Analyzing: {exp_id}\")\n",
    "    print(f\"{'='*80}\")\n",
    "\n",
    "    try:\n",
    "        # Find corrupted file\n",
    "        df_corrupt = None\n",
    "        for batch_dir in batch_dirs:\n",
    "            path = os.path.join(BASE_DIR, batch_dir, f\"corrupted/{exp_id}.csv\")\n",
    "            if os.path.exists(path):\n",
    "                df_corrupt = pd.read_csv(path)\n",
    "                break\n",
    "\n",
    "        if df_corrupt is None:\n",
    "            print(f\"   ‚ö†Ô∏è  File not found, skipping...\")\n",
    "            continue\n",
    "\n",
    "        print(f\"   Loaded: {len(df_corrupt):,} rows\")\n",
    "\n",
    "        # ==================\n",
    "        # A. SCHEMA VALIDATION\n",
    "        # ==================\n",
    "        print(\"   üìä A. Schema Validation...\")\n",
    "        corrupt_stats = tfdv.generate_statistics_from_dataframe(df_corrupt[['text', 'label']])\n",
    "        anomalies = tfdv.validate_statistics(statistics=corrupt_stats, schema=schema)\n",
    "\n",
    "        anomaly_info = MessageToDict(anomalies)\n",
    "        n_anomalies = len(anomaly_info.get('anomalyInfo', {}))\n",
    "\n",
    "        anomaly_types = []\n",
    "        for feature_name, anomaly_data in anomaly_info.get('anomalyInfo', {}).items():\n",
    "            reason = anomaly_data.get('shortDescription', '')\n",
    "            anomaly_types.append(f\"{feature_name}: {reason}\")\n",
    "\n",
    "        # ==================\n",
    "        # B. COMPLETENESS DRIFT\n",
    "        # ==================\n",
    "        print(\"   üìä B. Completeness Analysis...\")\n",
    "        text_complete_base = df_baseline['text'].notna().sum() / len(df_baseline)\n",
    "        text_complete_corrupt = df_corrupt['text'].notna().sum() / len(df_corrupt)\n",
    "        text_drift = abs(text_complete_base - text_complete_corrupt)\n",
    "\n",
    "        label_complete_base = df_baseline['label'].notna().sum() / len(df_baseline)\n",
    "        label_complete_corrupt = df_corrupt['label'].notna().sum() / len(df_corrupt)\n",
    "        label_drift = abs(label_complete_base - label_complete_corrupt)\n",
    "\n",
    "        # ==================\n",
    "        # C. DISTRIBUTION SKEW\n",
    "        # ==================\n",
    "        print(\"   üìä C. Distribution Skew Analysis...\")\n",
    "\n",
    "        # Label distribution comparison\n",
    "        corrupt_label_dist = df_corrupt['label'].value_counts(normalize=True).sort_index()\n",
    "\n",
    "        # Align distributions (fill missing labels with 0)\n",
    "        all_labels = sorted(set(baseline_label_dist.index) | set(corrupt_label_dist.index))\n",
    "        base_aligned = [baseline_label_dist.get(l, 0) for l in all_labels]\n",
    "        corrupt_aligned = [corrupt_label_dist.get(l, 0) for l in all_labels]\n",
    "\n",
    "        # L1 distance (total variation)\n",
    "        l1_distance = sum(abs(b - c) for b, c in zip(base_aligned, corrupt_aligned)) / 2\n",
    "\n",
    "        # Jensen-Shannon divergence (symmetric KL divergence)\n",
    "        # Add small epsilon to avoid log(0)\n",
    "        base_smooth = [p + 1e-10 for p in base_aligned]\n",
    "        corrupt_smooth = [p + 1e-10 for p in corrupt_aligned]\n",
    "        js_divergence = jensenshannon(base_smooth, corrupt_smooth)\n",
    "\n",
    "        # Kolmogorov-Smirnov test (for numeric labels)\n",
    "        ks_statistic, ks_pvalue = ks_2samp(\n",
    "            df_baseline['label'].dropna().values,\n",
    "            df_corrupt['label'].dropna().values\n",
    "        )\n",
    "\n",
    "        # ==================\n",
    "        # D. FEATURE STATISTICS\n",
    "        # ==================\n",
    "        print(\"   üìä D. Feature-Level Statistics...\")\n",
    "\n",
    "        # Text length statistics\n",
    "        base_text_lengths = df_baseline['text'].str.len()\n",
    "        corrupt_text_lengths = df_corrupt['text'].str.len()\n",
    "\n",
    "        avg_length_base = base_text_lengths.mean()\n",
    "        avg_length_corrupt = corrupt_text_lengths.mean()\n",
    "        length_change = ((avg_length_corrupt - avg_length_base) / avg_length_base * 100) if avg_length_base > 0 else 0\n",
    "\n",
    "        # ==================\n",
    "        # E. DETECTABILITY SCORING\n",
    "        # ==================\n",
    "        print(\"   üìä E. Detectability Scoring...\")\n",
    "\n",
    "        # Composite detectability score\n",
    "        completeness_score = (text_drift + label_drift) / 2\n",
    "        distribution_score = l1_distance\n",
    "        anomaly_score = min(n_anomalies / 2, 1.0)  # Normalize to 0-1\n",
    "\n",
    "        # Weighted composite (40% completeness, 40% distribution, 20% anomalies)\n",
    "        detectability_score = (\n",
    "            0.4 * completeness_score +\n",
    "            0.4 * distribution_score +\n",
    "            0.2 * anomaly_score\n",
    "        )\n",
    "\n",
    "        if detectability_score > 0.2:\n",
    "            detectability = \"High\"\n",
    "        elif detectability_score > 0.05:\n",
    "            detectability = \"Medium\"\n",
    "        else:\n",
    "            detectability = \"Low\"\n",
    "\n",
    "        # ==================\n",
    "        # F. PRINT RESULTS\n",
    "        # ==================\n",
    "        print(f\"\\n   ‚úÖ VALIDATION RESULTS:\")\n",
    "        print(f\"      Schema Anomalies: {n_anomalies}\")\n",
    "        print(f\"      Text Completeness: {text_complete_corrupt:.1%} (Œî = {text_drift:.1%})\")\n",
    "        print(f\"      Label Completeness: {label_complete_corrupt:.1%} (Œî = {label_drift:.1%})\")\n",
    "        print(f\"      Label Distribution L1: {l1_distance:.3f}\")\n",
    "        print(f\"      Jensen-Shannon Divergence: {js_divergence:.3f}\")\n",
    "        print(f\"      KS Statistic: {ks_statistic:.3f} (p={ks_pvalue:.4f})\")\n",
    "        print(f\"      Avg Text Length Change: {length_change:+.1f}%\")\n",
    "        print(f\"      Detectability Score: {detectability_score:.3f} ({detectability})\")\n",
    "\n",
    "        # Store results\n",
    "        tfdv_results.append({\n",
    "            'Experiment': exp_id.replace('_', ' ').title(),\n",
    "            'Schema Anomalies': n_anomalies,\n",
    "            'Text Completeness': f\"{text_complete_corrupt:.1%}\",\n",
    "            'Text Drift': f\"{text_drift:.1%}\",\n",
    "            'Label Completeness': f\"{label_complete_corrupt:.1%}\",\n",
    "            'Label Drift': f\"{label_drift:.1%}\",\n",
    "            'L1 Distance': f\"{l1_distance:.3f}\",\n",
    "            'JS Divergence': f\"{js_divergence:.3f}\",\n",
    "            'KS Statistic': f\"{ks_statistic:.3f}\",\n",
    "            'KS p-value': f\"{ks_pvalue:.4f}\",\n",
    "            'Text Length Change': f\"{length_change:+.1f}%\",\n",
    "            'Detectability Score': f\"{detectability_score:.3f}\",\n",
    "            'Detectability': detectability,\n",
    "            'Anomaly Details': '; '.join(anomaly_types[:3]) if anomaly_types else 'None'\n",
    "        })\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        continue\n",
    "\n",
    "# ============================================\n",
    "# 3. SAVE COMPREHENSIVE RESULTS\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE TFDV RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "df_tfdv = pd.DataFrame(tfdv_results)\n",
    "print(\"\\n\" + df_tfdv.to_string(index=False))\n",
    "\n",
    "# Save\n",
    "tfdv_path = os.path.join(BASE_DIR, \"results/PART2_TFDV_Comprehensive.csv\")\n",
    "os.makedirs(os.path.join(BASE_DIR, \"results\"), exist_ok=True)\n",
    "df_tfdv.to_csv(tfdv_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Saved: {tfdv_path}\")\n",
    "\n",
    "# ============================================\n",
    "# 4. ANALYSIS SUMMARY\n",
    "# ============================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Detectability distribution\n",
    "print(f\"\\nüìä Corruption Detectability:\")\n",
    "for level in ['High', 'Medium', 'Low']:\n",
    "    count = len(df_tfdv[df_tfdv['Detectability'] == level])\n",
    "    pct = count / len(df_tfdv) * 100 if len(df_tfdv) > 0 else 0\n",
    "    print(f\"   {level:8s}: {count}/{len(df_tfdv)} ({pct:.0f}%)\")\n",
    "\n",
    "# Anomaly detection rate\n",
    "detected = len(df_tfdv[df_tfdv['Schema Anomalies'] > 0])\n",
    "print(f\"\\nüìä Detection Metrics:\")\n",
    "print(f\"   Anomaly Detection Rate: {detected}/{len(df_tfdv)} ({detected/len(df_tfdv)*100:.0f}%)\")\n",
    "\n",
    "# Distribution shift significance\n",
    "sig_ks = len(df_tfdv[df_tfdv['KS p-value'].astype(float) < 0.05])\n",
    "print(f\"   Significant Distribution Shifts: {sig_ks}/{len(df_tfdv)} ({sig_ks/len(df_tfdv)*100:.0f}%)\")\n",
    "\n",
    "# Average metrics by detectability\n",
    "print(f\"\\nüìä Average Drift by Detectability:\")\n",
    "for level in ['High', 'Medium', 'Low']:\n",
    "    subset = df_tfdv[df_tfdv['Detectability'] == level]\n",
    "    if len(subset) > 0:\n",
    "        avg_score = subset['Detectability Score'].astype(float).mean()\n",
    "        print(f\"   {level:8s}: Score = {avg_score:.3f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ PART 2 COMPLETE - COMPREHENSIVE TFDV ANALYSIS!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüîë Key Findings:\")\n",
    "print(f\"   ‚Ä¢ {detected}/{len(df_tfdv)} corruptions detected via schema anomalies\")\n",
    "print(f\"   ‚Ä¢ {sig_ks}/{len(df_tfdv)} corruptions show significant distribution shifts\")\n",
    "print(\"   ‚Ä¢ Structural corruptions (missing values/labels) have high detectability\")\n",
    "print(\"   ‚Ä¢ Semantic corruptions (swapped/noise) show low detectability\")\n",
    "print(\"   ‚Ä¢ TFDV is necessary but insufficient - model evaluation required\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CARuPDmkoKqf"
   },
   "source": [
    "# ADULT DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hHdbW8iB6F9x",
    "outputId": "07c9f413-5389-4689-b48d-45e6b29824ab"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ADULT DATASET EVALUATION - TEAM'S EXACT APPROACH\n",
    "================================================\n",
    "\n",
    "Their actual approach (from notebook):\n",
    "1. Use ONLY \"occupation\" column as \"text\"\n",
    "2. Use TfidfVectorizer + LogisticRegression (same as Amazon!)\n",
    "3. Apply their inject/clean modules\n",
    "4. That's it!\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# =========================\n",
    "# CONFIGURATION\n",
    "# =========================\n",
    "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
    "FIXED_DIR = os.path.join(BASE_DIR, \"fixed_adult\")\n",
    "TFIDF_MAX_FEATURES = 5000\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.20\n",
    "\n",
    "# =========================\n",
    "# IMPORT TEAM CODE\n",
    "# =========================\n",
    "sys.path.append(os.path.join(BASE_DIR, \"team_code\"))\n",
    "import inject_extreme as inject\n",
    "import clean\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ADULT DATASET EVALUATION - TEAM'S EXACT APPROACH\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =========================\n",
    "# CREATE DIRECTORIES\n",
    "# =========================\n",
    "def ensure_dirs():\n",
    "    for sub in [\"baseline\", \"corrupted\", \"cleaned\", \"results\"]:\n",
    "        os.makedirs(os.path.join(FIXED_DIR, sub), exist_ok=True)\n",
    "\n",
    "ensure_dirs()\n",
    "\n",
    "# =========================\n",
    "# LOAD ADULT DATASET\n",
    "# =========================\n",
    "print(\"\\nüìä Loading Adult Dataset...\")\n",
    "\n",
    "ADULT_PATH = os.path.join(BASE_DIR, \"data/raw/adult.csv\")\n",
    "df_raw = pd.read_csv(ADULT_PATH)\n",
    "print(f\"   Loaded: {len(df_raw):,} rows\")\n",
    "\n",
    "# Remove rows with missing values\n",
    "df_raw = df_raw.dropna()\n",
    "\n",
    "# Handle column naming\n",
    "if 'class' in df_raw.columns:\n",
    "    target_col = 'class'\n",
    "elif 'income' in df_raw.columns:\n",
    "    target_col = 'income'\n",
    "else:\n",
    "    raise ValueError(\"Target column not found!\")\n",
    "\n",
    "# Clean target column\n",
    "df_raw[target_col] = df_raw[target_col].str.replace('.', '', regex=False).str.strip()\n",
    "\n",
    "print(f\"   After dropna: {len(df_raw):,} rows\")\n",
    "\n",
    "# =========================\n",
    "# PREPARE IN TEXT/LABEL FORMAT (THEIR EXACT WAY)\n",
    "# =========================\n",
    "print(\"\\nüîß Converting to text/label format (EXACTLY like their notebook)...\")\n",
    "\n",
    "# THEIR EXACT APPROACH: Just use occupation as \"text\"!\n",
    "df_baseline = pd.DataFrame()\n",
    "df_baseline['row_id'] = df_raw.reset_index(drop=True).index\n",
    "df_baseline['text'] = df_raw['occupation'].astype(str)  # Just occupation!\n",
    "df_baseline['label'] = df_raw[target_col].map({'<=50K': 1, '>50K': 5})\n",
    "\n",
    "# Drop any rows where mapping failed\n",
    "df_baseline = df_baseline.dropna(subset=['label'])\n",
    "df_baseline['label'] = df_baseline['label'].astype(int)\n",
    "\n",
    "print(f\"   Baseline created: {len(df_baseline):,} rows\")\n",
    "print(f\"   text = occupation column\")\n",
    "print(f\"   label = income (1 for <=50K, 5 for >50K)\")\n",
    "print(f\"\\n   Sample:\")\n",
    "print(df_baseline.head(3))\n",
    "\n",
    "# =========================\n",
    "# FIXED TRAIN/TEST SPLIT\n",
    "# =========================\n",
    "print(\"\\n‚úÇÔ∏è  Creating fixed train/test split...\")\n",
    "\n",
    "df_train_clean, df_test_clean = train_test_split(\n",
    "    df_baseline,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=df_baseline['label']\n",
    ")\n",
    "\n",
    "train_ids = set(df_train_clean[\"row_id\"].tolist())\n",
    "test_ids = set(df_test_clean[\"row_id\"].tolist())\n",
    "\n",
    "print(f\"   Train: {len(train_ids):,} samples\")\n",
    "print(f\"   Test: {len(test_ids):,} samples\")\n",
    "\n",
    "# =========================\n",
    "# BUILD MODEL (THEIR EXACT MODEL - TfidfVectorizer + LogisticRegression)\n",
    "# =========================\n",
    "print(\"\\nüèóÔ∏è  Building model (EXACT same as their Amazon/Adult model)...\")\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Their exact model: TfidfVectorizer + LogisticRegression\"\"\"\n",
    "    return Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            max_features=TFIDF_MAX_FEATURES,\n",
    "            stop_words=\"english\"\n",
    "        )),\n",
    "        (\"clf\", LogisticRegression(max_iter=1000))\n",
    "    ])\n",
    "\n",
    "print(\"   ‚úÖ Model built: TfidfVectorizer + LogisticRegression\")\n",
    "\n",
    "# =========================\n",
    "# UTILITIES\n",
    "# =========================\n",
    "def clear_memory():\n",
    "    gc.collect()\n",
    "\n",
    "def valid_row_mask(df):\n",
    "    \"\"\"Check if rows are valid for training\"\"\"\n",
    "    text = df[\"text\"].astype(str)\n",
    "    label = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
    "    mask = label.notna() & text.notna()\n",
    "    mask &= (text != \"nan\") & (text.str.len() > 0)\n",
    "    mask &= label.apply(lambda x: float(x).is_integer() if pd.notna(x) else False)\n",
    "    return mask\n",
    "\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    return {\n",
    "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
    "        \"precision\": float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "        \"recall\": float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "        \"f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
    "    }\n",
    "\n",
    "def train_and_evaluate(df_train, df_test, allow_invalid=False):\n",
    "    \"\"\"\n",
    "    Train and evaluate model\n",
    "    \"\"\"\n",
    "    if allow_invalid:\n",
    "        # For corrupted data: fill NaN with defaults\n",
    "        train_valid = df_train.copy()\n",
    "        test_valid = df_test.copy()\n",
    "        train_valid['text'] = train_valid['text'].fillna('unknown')\n",
    "        test_valid['text'] = test_valid['text'].fillna('unknown')\n",
    "        train_valid['label'] = pd.to_numeric(train_valid['label'], errors='coerce').fillna(3).astype(int)\n",
    "        test_valid['label'] = pd.to_numeric(test_valid['label'], errors='coerce').fillna(3).astype(int)\n",
    "    else:\n",
    "        # For clean data: strict filtering\n",
    "        train_mask = valid_row_mask(df_train)\n",
    "        test_mask = valid_row_mask(df_test)\n",
    "        train_valid = df_train[train_mask].copy()\n",
    "        test_valid = df_test[test_mask].copy()\n",
    "\n",
    "    X_train = train_valid[\"text\"].astype(str).values\n",
    "    y_train = pd.to_numeric(train_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
    "    X_test = test_valid[\"text\"].astype(str).values\n",
    "    y_test = pd.to_numeric(test_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
    "\n",
    "    model = build_model()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    metrics = compute_metrics(y_test, y_pred)\n",
    "    stats = {\n",
    "        \"train_total\": len(df_train),\n",
    "        \"train_valid\": len(train_valid),\n",
    "        \"test_total\": len(df_test),\n",
    "        \"test_valid\": len(test_valid),\n",
    "    }\n",
    "\n",
    "    del model, X_train, y_train, X_test, y_test, train_valid, test_valid\n",
    "    clear_memory()\n",
    "\n",
    "    return metrics, stats\n",
    "\n",
    "# =========================\n",
    "# BASELINE EVALUATION\n",
    "# =========================\n",
    "print(\"\\nüìà Baseline evaluation...\")\n",
    "\n",
    "metrics_base, stats_base = train_and_evaluate(df_train_clean, df_test_clean, allow_invalid=False)\n",
    "\n",
    "print(f\"   Acc: {metrics_base['accuracy']:.4f} | \"\n",
    "      f\"Prec: {metrics_base['precision']:.4f} | \"\n",
    "      f\"Rec: {metrics_base['recall']:.4f} | \"\n",
    "      f\"F1: {metrics_base['f1']:.4f}\")\n",
    "\n",
    "all_results = []\n",
    "\n",
    "def record_result(exp_id, stage, metrics, stats):\n",
    "    row = {\"experiment\": exp_id, \"stage\": stage, **stats, **metrics}\n",
    "    all_results.append(row)\n",
    "\n",
    "record_result(\"BASELINE\", \"CLEAN\", metrics_base, stats_base)\n",
    "\n",
    "# =========================\n",
    "# CORRUPTION EXPERIMENTS (THEIR EXACT CORRUPTIONS)\n",
    "# =========================\n",
    "print(\"\\nüß™ Running corruption experiments...\")\n",
    "\n",
    "experiments = [\n",
    "    (\"01_missing_values\", inject.apply_missing_values, {}),\n",
    "    (\"02_broken_chars\", inject.apply_broken_characters, {}),\n",
    "    (\"03_swapped_text\", inject.apply_swapped_text, {}),\n",
    "    (\"04_missing_labels\", inject.apply_missing_labels, {}),\n",
    "]\n",
    "\n",
    "for idx, (exp_id, corrupt_func, kwargs) in enumerate(experiments, start=1):\n",
    "    print(f\"\\n[{idx}/{len(experiments)}] {exp_id}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    # === CORRUPT ===\n",
    "    print(\"   üî¥ Corrupting...\")\n",
    "    df_corrupt = corrupt_func(df_baseline.copy(), **kwargs)\n",
    "    df_corrupt.to_csv(os.path.join(FIXED_DIR, f\"corrupted/{exp_id}.csv\"), index=False)\n",
    "\n",
    "    # Split\n",
    "    df_train_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(train_ids)].copy()\n",
    "    df_test_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(test_ids)].copy()\n",
    "\n",
    "    # === EVALUATE CORRUPTED ===\n",
    "    print(\"   üî¥ Training on CORRUPTED...\")\n",
    "    metrics_corrupt, stats_corrupt = train_and_evaluate(df_train_corrupt, df_test_corrupt, allow_invalid=True)\n",
    "    record_result(exp_id, \"CORRUPTED\", metrics_corrupt, stats_corrupt)\n",
    "\n",
    "    print(f\"      Acc: {metrics_corrupt['accuracy']:.4f} | \"\n",
    "          f\"Prec: {metrics_corrupt['precision']:.4f} | \"\n",
    "          f\"Rec: {metrics_corrupt['recall']:.4f} | \"\n",
    "          f\"F1: {metrics_corrupt['f1']:.4f}\")\n",
    "\n",
    "    del df_train_corrupt, df_test_corrupt\n",
    "    clear_memory()\n",
    "\n",
    "    # === CLEAN ===\n",
    "    print(\"   üü¢ Cleaning...\")\n",
    "    df_cleaned = clean.clean_all(df_corrupt)\n",
    "    df_cleaned.to_csv(os.path.join(FIXED_DIR, f\"cleaned/{exp_id}_cleaned.csv\"), index=False)\n",
    "    del df_corrupt\n",
    "    clear_memory()\n",
    "\n",
    "    # Split\n",
    "    df_train_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(train_ids)].copy()\n",
    "    df_test_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(test_ids)].copy()\n",
    "\n",
    "    # === EVALUATE CLEANED ===\n",
    "    print(\"   üü¢ Training on CLEANED...\")\n",
    "    metrics_cleaned, stats_cleaned = train_and_evaluate(df_train_cleaned, df_test_cleaned, allow_invalid=False)\n",
    "    record_result(exp_id, \"CLEANED\", metrics_cleaned, stats_cleaned)\n",
    "\n",
    "    print(f\"      Acc: {metrics_cleaned['accuracy']:.4f} | \"\n",
    "          f\"Prec: {metrics_cleaned['precision']:.4f} | \"\n",
    "          f\"Rec: {metrics_cleaned['recall']:.4f} | \"\n",
    "          f\"F1: {metrics_cleaned['f1']:.4f}\")\n",
    "\n",
    "    # === RECOVERY ===\n",
    "    acc_drop = metrics_base['accuracy'] - metrics_corrupt['accuracy']\n",
    "    acc_recovery = metrics_cleaned['accuracy'] - metrics_corrupt['accuracy']\n",
    "    recovery_pct = (acc_recovery / acc_drop * 100) if acc_drop != 0 else 0\n",
    "\n",
    "    print(f\"      üìä Recovery: {recovery_pct:.1f}%\")\n",
    "\n",
    "    del df_train_cleaned, df_test_cleaned, df_cleaned\n",
    "    clear_memory()\n",
    "\n",
    "# =========================\n",
    "# SAVE RESULTS\n",
    "# =========================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAVING RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_path = os.path.join(FIXED_DIR, \"results/adult_results.csv\")\n",
    "df_results = pd.DataFrame(all_results)\n",
    "df_results.to_csv(results_path, index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved: {results_path}\")\n",
    "print(f\"\\nüìä SUMMARY:\")\n",
    "print(df_results[['experiment', 'stage', 'accuracy', 'f1']].to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ ADULT DATASET EVALUATION COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mwDrWMzttKRo"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "runtime_attributes": {
    "runtime_version": "2025.07"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
