{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      }
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vltfd6l6n9cl",
        "outputId": "e000c568-9dc5-442e-8a65-0e2d4f668e4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "âœ… Working directory: /content/drive/MyDrive/data_preparation_project_2026\n",
            "âœ… Folder structure created\n",
            "âœ… All libraries loaded\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# ========================================\n",
        "# EVALUATION FRAMEWORK - SETUP CELL\n",
        "# Run this first in every notebook\n",
        "# ========================================\n",
        "\n",
        "# 1. Mount Google Drive (for data persistence)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Set working directory\n",
        "import os\n",
        "WORK_DIR = '/content/drive/MyDrive/data_preparation_project_2026'\n",
        "os.makedirs(WORK_DIR, exist_ok=True)\n",
        "os.chdir(WORK_DIR)\n",
        "\n",
        "# Create folder structure\n",
        "os.makedirs('data/baseline', exist_ok=True)\n",
        "os.makedirs('data/corrupted', exist_ok=True)\n",
        "os.makedirs('data/cleaned', exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "os.makedirs('figures', exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Working directory: {WORK_DIR}\")\n",
        "print(f\"âœ… Folder structure created\")\n",
        "\n",
        "# 3. Install required packages\n",
        "# !pip install -q tensorflow-data-validation  # For Part 2\n",
        "!pip install -q statsmodels  # For Part 3 (McNemar's test)\n",
        "\n",
        "# 4. Import common libraries\n",
        "# import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
        "from scipy.stats import ttest_rel, ks_2samp\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set plot style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "print(\"âœ… All libraries loaded\")\n",
        "print(\"=\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets scikit-learn pandas numpy setuptools -q"
      ],
      "metadata": {
        "id": "VdLquXRXov4I"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jenga"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELsRrsRno-6R",
        "outputId": "31365ca5-b96b-4795-cf30-88463864f822"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting jenga\n",
            "  Downloading jenga-0.0.1a1-py2.py3-none-any.whl.metadata (6.0 kB)\n",
            "\u001b[33mWARNING: Package 'jenga' has an invalid Requires-Python: Invalid specifier: '<3.10.*'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from jenga) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from jenga) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->jenga) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->jenga) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->jenga) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->jenga) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->jenga) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->jenga) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->jenga) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->jenga) (1.17.0)\n",
            "Downloading jenga-0.0.1a1-py2.py3-none-any.whl (33 kB)\n",
            "Installing collected packages: jenga\n",
            "Successfully installed jenga-0.0.1a1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "beauty_path = \"/content/drive/MyDrive/data_preparation_project_2026/data/raw/All_Beauty.jsonl.gz\"\n",
        "\n",
        "import os\n",
        "print(\"exists:\", os.path.exists(beauty_path))\n",
        "print(\"size (MB):\", os.path.getsize(beauty_path) / (1024*1024))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk-Jk8IBpLpg",
        "outputId": "aa734cfb-2733-42d2-9856-5e3ae5d9f5c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "exists: True\n",
            "size (MB): 90.06644916534424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "\n",
        "with gzip.open(beauty_path, \"rt\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "    for i in range(3):\n",
        "        print(\"LINE\", i, \":\", f.readline()[:300])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybwAqLFZpUDz",
        "outputId": "46f32bf4-3467-4995-c704-85aadb0bf72e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LINE 0 : {\"rating\": 5.0, \"title\": \"Such a lovely scent but not overpowering.\", \"text\": \"This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am compa\n",
            "LINE 1 : {\"rating\": 4.0, \"title\": \"Works great but smells a little weird.\", \"text\": \"This product does what I need it to do, I just wish it was odorless or had a soft coconut smell. Having my head smell like an orange coffee is offputting. (granted, I did know the smell was described but I was hoping it woul\n",
            "LINE 2 : {\"rating\": 5.0, \"title\": \"Yes!\", \"text\": \"Smells good, feels great!\", \"images\": [], \"asin\": \"B07PNNCSP9\", \"parent_asin\": \"B097R46CSY\", \"user_id\": \"AE74DYR3QUGVPZJ3P7RFWBGIX7XQ\", \"timestamp\": 1589665266052, \"helpful_vote\": 2, \"verified_purchase\": true}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "\n",
        "with gzip.open(beauty_path, \"rt\", encoding=\"utf-8\") as f:\n",
        "    for _ in range(5):\n",
        "        print(f.readline())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3FsiajppCjj",
        "outputId": "ec6eb52b-54f4-4d35-9159-ddc10d512fe0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"rating\": 5.0, \"title\": \"Such a lovely scent but not overpowering.\", \"text\": \"This spray is really nice. It smells really good, goes on really fine, and does the trick. I will say it feels like you need a lot of it though to get the texture I want. I have a lot of hair, medium thickness. I am comparing to other brands with yucky chemicals so I'm gonna stick with this. Try it!\", \"images\": [], \"asin\": \"B00YQ6X8EO\", \"parent_asin\": \"B00YQ6X8EO\", \"user_id\": \"AGKHLEW2SOWHNMFQIJGBECAF7INQ\", \"timestamp\": 1588687728923, \"helpful_vote\": 0, \"verified_purchase\": true}\n",
            "\n",
            "{\"rating\": 4.0, \"title\": \"Works great but smells a little weird.\", \"text\": \"This product does what I need it to do, I just wish it was odorless or had a soft coconut smell. Having my head smell like an orange coffee is offputting. (granted, I did know the smell was described but I was hoping it would be light)\", \"images\": [], \"asin\": \"B081TJ8YS3\", \"parent_asin\": \"B081TJ8YS3\", \"user_id\": \"AGKHLEW2SOWHNMFQIJGBECAF7INQ\", \"timestamp\": 1588615855070, \"helpful_vote\": 1, \"verified_purchase\": true}\n",
            "\n",
            "{\"rating\": 5.0, \"title\": \"Yes!\", \"text\": \"Smells good, feels great!\", \"images\": [], \"asin\": \"B07PNNCSP9\", \"parent_asin\": \"B097R46CSY\", \"user_id\": \"AE74DYR3QUGVPZJ3P7RFWBGIX7XQ\", \"timestamp\": 1589665266052, \"helpful_vote\": 2, \"verified_purchase\": true}\n",
            "\n",
            "{\"rating\": 1.0, \"title\": \"Synthetic feeling\", \"text\": \"Felt synthetic\", \"images\": [], \"asin\": \"B09JS339BZ\", \"parent_asin\": \"B09JS339BZ\", \"user_id\": \"AFQLNQNQYFWQZPJQZS6V3NZU4QBQ\", \"timestamp\": 1643393630220, \"helpful_vote\": 0, \"verified_purchase\": true}\n",
            "\n",
            "{\"rating\": 5.0, \"title\": \"A+\", \"text\": \"Love it\", \"images\": [], \"asin\": \"B08BZ63GMJ\", \"parent_asin\": \"B08BZ63GMJ\", \"user_id\": \"AFQLNQNQYFWQZPJQZS6V3NZU4QBQ\", \"timestamp\": 1609322563534, \"helpful_vote\": 0, \"verified_purchase\": true}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "beauty_df = pd.read_json(beauty_path, lines=True, compression=\"gzip\")\n",
        "print(len(beauty_df))\n",
        "beauty_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "M5ShBHx_pIGQ",
        "outputId": "361416ee-4043-430e-a892-ee33e5274c74"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "701528\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   rating                                      title  \\\n",
              "0       5  Such a lovely scent but not overpowering.   \n",
              "1       4     Works great but smells a little weird.   \n",
              "2       5                                       Yes!   \n",
              "3       1                          Synthetic feeling   \n",
              "4       5                                         A+   \n",
              "\n",
              "                                                text images        asin  \\\n",
              "0  This spray is really nice. It smells really go...     []  B00YQ6X8EO   \n",
              "1  This product does what I need it to do, I just...     []  B081TJ8YS3   \n",
              "2                          Smells good, feels great!     []  B07PNNCSP9   \n",
              "3                                     Felt synthetic     []  B09JS339BZ   \n",
              "4                                            Love it     []  B08BZ63GMJ   \n",
              "\n",
              "  parent_asin                       user_id               timestamp  \\\n",
              "0  B00YQ6X8EO  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2020-05-05 14:08:48.923   \n",
              "1  B081TJ8YS3  AGKHLEW2SOWHNMFQIJGBECAF7INQ 2020-05-04 18:10:55.070   \n",
              "2  B097R46CSY  AE74DYR3QUGVPZJ3P7RFWBGIX7XQ 2020-05-16 21:41:06.052   \n",
              "3  B09JS339BZ  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2022-01-28 18:13:50.220   \n",
              "4  B08BZ63GMJ  AFQLNQNQYFWQZPJQZS6V3NZU4QBQ 2020-12-30 10:02:43.534   \n",
              "\n",
              "   helpful_vote  verified_purchase  \n",
              "0             0               True  \n",
              "1             1               True  \n",
              "2             2               True  \n",
              "3             0               True  \n",
              "4             0               True  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dc9f22aa-3c24-4a9f-976a-19d4e6d15a56\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>title</th>\n",
              "      <th>text</th>\n",
              "      <th>images</th>\n",
              "      <th>asin</th>\n",
              "      <th>parent_asin</th>\n",
              "      <th>user_id</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>helpful_vote</th>\n",
              "      <th>verified_purchase</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>Such a lovely scent but not overpowering.</td>\n",
              "      <td>This spray is really nice. It smells really go...</td>\n",
              "      <td>[]</td>\n",
              "      <td>B00YQ6X8EO</td>\n",
              "      <td>B00YQ6X8EO</td>\n",
              "      <td>AGKHLEW2SOWHNMFQIJGBECAF7INQ</td>\n",
              "      <td>2020-05-05 14:08:48.923</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Works great but smells a little weird.</td>\n",
              "      <td>This product does what I need it to do, I just...</td>\n",
              "      <td>[]</td>\n",
              "      <td>B081TJ8YS3</td>\n",
              "      <td>B081TJ8YS3</td>\n",
              "      <td>AGKHLEW2SOWHNMFQIJGBECAF7INQ</td>\n",
              "      <td>2020-05-04 18:10:55.070</td>\n",
              "      <td>1</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>Yes!</td>\n",
              "      <td>Smells good, feels great!</td>\n",
              "      <td>[]</td>\n",
              "      <td>B07PNNCSP9</td>\n",
              "      <td>B097R46CSY</td>\n",
              "      <td>AE74DYR3QUGVPZJ3P7RFWBGIX7XQ</td>\n",
              "      <td>2020-05-16 21:41:06.052</td>\n",
              "      <td>2</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>Synthetic feeling</td>\n",
              "      <td>Felt synthetic</td>\n",
              "      <td>[]</td>\n",
              "      <td>B09JS339BZ</td>\n",
              "      <td>B09JS339BZ</td>\n",
              "      <td>AFQLNQNQYFWQZPJQZS6V3NZU4QBQ</td>\n",
              "      <td>2022-01-28 18:13:50.220</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>A+</td>\n",
              "      <td>Love it</td>\n",
              "      <td>[]</td>\n",
              "      <td>B08BZ63GMJ</td>\n",
              "      <td>B08BZ63GMJ</td>\n",
              "      <td>AFQLNQNQYFWQZPJQZS6V3NZU4QBQ</td>\n",
              "      <td>2020-12-30 10:02:43.534</td>\n",
              "      <td>0</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dc9f22aa-3c24-4a9f-976a-19d4e6d15a56')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dc9f22aa-3c24-4a9f-976a-19d4e6d15a56 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dc9f22aa-3c24-4a9f-976a-19d4e6d15a56');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e84dab36-a997-4ab2-93c4-3fed3f12b421\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e84dab36-a997-4ab2-93c4-3fed3f12b421')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e84dab36-a997-4ab2-93c4-3fed3f12b421 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "beauty_df"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n",
        "\n",
        "!pwd\n",
        "\n",
        "!cd /content/drive/MyDrive/data_preparation_project_2026"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdoj6klhpcC0",
        "outputId": "287091f3-3394-4b06-a5ca-c3409aa1d840"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "archive_old_data\t\t Eval_Notebooks  fixed_batch1  predictions\n",
            "clothing_experiments_backup.zip  figures\t fixed_batch2  results\n",
            "data\t\t\t\t fixed\t\t models        team_code\n",
            "/content/drive/MyDrive/data_preparation_project_2026\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 1: Baseline Metrics Comparison"
      ],
      "metadata": {
        "id": "6CVMEKyAw44i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Quick script to check corrupted file sizes and clean bad checkpoints\n",
        "\"\"\"\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
        "FIXED_DIR = os.path.join(BASE_DIR, \"fixed\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"CHECKING CORRUPTED FILES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "corrupted_dir = os.path.join(FIXED_DIR, \"corrupted\")\n",
        "if os.path.exists(corrupted_dir):\n",
        "    files = sorted([f for f in os.listdir(corrupted_dir) if f.endswith('.csv')])\n",
        "    for f in files:\n",
        "        path = os.path.join(corrupted_dir, f)\n",
        "        df = pd.read_csv(path)\n",
        "        print(f\"{f:40s} â†’ {len(df):,} rows\")\n",
        "else:\n",
        "    print(\"âŒ No corrupted directory found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"DELETING ALL CHECKPOINTS (force re-run)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "checkpoint_dir = os.path.join(FIXED_DIR, \"checkpoints\")\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.json')]\n",
        "    for f in files:\n",
        "        path = os.path.join(checkpoint_dir, f)\n",
        "        os.remove(path)\n",
        "        print(f\"âœ… Deleted: {f}\")\n",
        "    print(f\"\\nâœ… Deleted {len(files)} checkpoints\")\n",
        "else:\n",
        "    print(\"âŒ No checkpoint directory found\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CHECKING BASELINE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "baseline_path = os.path.join(FIXED_DIR, \"baseline/amazon_clean_with_rowid.csv\")\n",
        "if os.path.exists(baseline_path):\n",
        "    df = pd.read_csv(baseline_path)\n",
        "    print(f\"Baseline with row_id: {len(df):,} rows\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n",
        "else:\n",
        "    print(\"âŒ No baseline found\")\n",
        "\n",
        "print(\"\\nâœ… Ready to re-run evaluation pipeline\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5dEnicQpifV",
        "outputId": "6e725a8b-4c6d-4fca-95ef-36807e9cbb61"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "CHECKING CORRUPTED FILES\n",
            "============================================================\n",
            "01_missing_values.csv                    â†’ 99,985 rows\n",
            "02_broken_chars.csv                      â†’ 99,985 rows\n",
            "03_swapped_text.csv                      â†’ 99,985 rows\n",
            "04_missing_labels.csv                    â†’ 99,985 rows\n",
            "05_swapped_labels.csv                    â†’ 99,985 rows\n",
            "06_combined_text.csv                     â†’ 99,985 rows\n",
            "07_all_corruptions.csv                   â†’ 99,985 rows\n",
            "\n",
            "============================================================\n",
            "DELETING ALL CHECKPOINTS (force re-run)\n",
            "============================================================\n",
            "\n",
            "âœ… Deleted 0 checkpoints\n",
            "\n",
            "============================================================\n",
            "CHECKING BASELINE\n",
            "============================================================\n",
            "Baseline with row_id: 99,985 rows\n",
            "Columns: ['text', 'label', 'row_id']\n",
            "\n",
            "âœ… Ready to re-run evaluation pipeline\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "BATCH 1 EVALUATION - First 4 Corruptions\n",
        "Runs: missing_values, broken_chars, swapped_text, missing_labels\n",
        "\"\"\"\n",
        "\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION\n",
        "# =========================\n",
        "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
        "FIXED_DIR = os.path.join(BASE_DIR, \"fixed_batch1\")\n",
        "TFIDF_MAX_FEATURES = 5000\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.20\n",
        "SUBSET_SIZE = 100000\n",
        "\n",
        "# =========================\n",
        "# IMPORT TEAM CODE\n",
        "# =========================\n",
        "sys.path.append(os.path.join(BASE_DIR, \"team_code\"))\n",
        "import inject_extreme as inject\n",
        "import clean\n",
        "\n",
        "# =========================\n",
        "# UTILITIES\n",
        "# =========================\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "\n",
        "def ensure_dirs():\n",
        "    for sub in [\"baseline\", \"corrupted\", \"cleaned\", \"results\", \"splits\"]:\n",
        "        os.makedirs(os.path.join(FIXED_DIR, sub), exist_ok=True)\n",
        "\n",
        "def prepare_baseline_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"text\"] = df[\"text\"].astype(str)\n",
        "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"label\", \"text\"])\n",
        "    df[\"label\"] = df[\"label\"].astype(int)\n",
        "    df = df[df[\"text\"] != \"nan\"]\n",
        "    df = df[df[\"text\"].str.len() > 0]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df[\"row_id\"] = df.index\n",
        "    return df\n",
        "\n",
        "def valid_row_mask(df: pd.DataFrame) -> pd.Series:\n",
        "    text = df[\"text\"].astype(str)\n",
        "    label = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
        "    mask = label.notna() & text.notna()\n",
        "    mask &= (text != \"nan\") & (text.str.len() > 0)\n",
        "    mask &= label.apply(lambda x: float(x).is_integer())\n",
        "    return mask\n",
        "\n",
        "def build_model():\n",
        "    return Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, stop_words=\"english\")),\n",
        "        (\"clf\", LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "    }\n",
        "\n",
        "def train_and_evaluate(df_train, df_test, allow_invalid=False):\n",
        "    if allow_invalid:\n",
        "        train_valid = df_train.copy()\n",
        "        test_valid = df_test.copy()\n",
        "        train_valid['text'] = train_valid['text'].fillna('')\n",
        "        test_valid['text'] = test_valid['text'].fillna('')\n",
        "        train_valid['label'] = pd.to_numeric(train_valid['label'], errors='coerce').fillna(3).astype(int)\n",
        "        test_valid['label'] = pd.to_numeric(test_valid['label'], errors='coerce').fillna(3).astype(int)\n",
        "    else:\n",
        "        train_mask = valid_row_mask(df_train)\n",
        "        test_mask = valid_row_mask(df_test)\n",
        "        train_valid = df_train[train_mask].copy()\n",
        "        test_valid = df_test[test_mask].copy()\n",
        "\n",
        "    X_train = train_valid[\"text\"].astype(str).values\n",
        "    y_train = pd.to_numeric(train_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
        "    X_test = test_valid[\"text\"].astype(str).values\n",
        "    y_test = pd.to_numeric(test_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
        "\n",
        "    model = build_model()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = compute_metrics(y_test, y_pred)\n",
        "    stats = {\n",
        "        \"train_total\": len(df_train),\n",
        "        \"train_valid\": len(train_valid),\n",
        "        \"test_total\": len(df_test),\n",
        "        \"test_valid\": len(test_valid),\n",
        "    }\n",
        "\n",
        "    del model, X_train, y_train, X_test, y_test, train_valid, test_valid\n",
        "    clear_memory()\n",
        "    return metrics, stats\n",
        "\n",
        "# =========================\n",
        "# MAIN\n",
        "# =========================\n",
        "print(\"=\"*80)\n",
        "print(\"BATCH 1 EVALUATION - First 4 Corruptions\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ensure_dirs()\n",
        "\n",
        "# Load baseline\n",
        "print(\"\\nðŸ“Š Loading baseline...\")\n",
        "baseline_path = os.path.join(BASE_DIR, \"data/baseline/amazon_reviews_1M.csv\")\n",
        "df_raw = pd.read_csv(baseline_path)\n",
        "print(f\"   Loaded: {len(df_raw):,} rows\")\n",
        "\n",
        "if len(df_raw) > SUBSET_SIZE:\n",
        "    df_raw = df_raw.sample(n=SUBSET_SIZE, random_state=RANDOM_STATE)\n",
        "    print(f\"   Sampled: {SUBSET_SIZE:,} rows\")\n",
        "\n",
        "df_baseline = prepare_baseline_data(df_raw)\n",
        "del df_raw\n",
        "clear_memory()\n",
        "print(f\"   Cleaned: {len(df_baseline):,} rows\")\n",
        "\n",
        "# Split\n",
        "df_train_clean, df_test_clean = train_test_split(\n",
        "    df_baseline, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df_baseline[\"label\"]\n",
        ")\n",
        "train_ids = set(df_train_clean[\"row_id\"].tolist())\n",
        "test_ids = set(df_test_clean[\"row_id\"].tolist())\n",
        "print(f\"   Split: {len(df_train_clean):,} train / {len(df_test_clean):,} test\")\n",
        "\n",
        "# Baseline eval\n",
        "print(\"\\nðŸ“ˆ Baseline evaluation...\")\n",
        "metrics_base, stats_base = train_and_evaluate(df_train_clean, df_test_clean, allow_invalid=False)\n",
        "print(f\"   Acc: {metrics_base['accuracy']:.4f} | Prec: {metrics_base['precision']:.4f} | Rec: {metrics_base['recall']:.4f} | F1: {metrics_base['f1']:.4f}\")\n",
        "\n",
        "all_results = []\n",
        "def record_result(exp_id, stage, metrics, stats):\n",
        "    row = {\"experiment\": exp_id, \"stage\": stage, **stats, **metrics}\n",
        "    all_results.append(row)\n",
        "\n",
        "record_result(\"BASELINE\", \"CLEAN\", metrics_base, stats_base)\n",
        "\n",
        "# Experiments\n",
        "experiments = [\n",
        "    (\"01_missing_values\", inject.apply_missing_values, {}),\n",
        "    (\"02_broken_chars\", inject.apply_broken_characters, {}),\n",
        "    (\"03_swapped_text\", inject.apply_swapped_text, {}),\n",
        "    (\"04_missing_labels\", inject.apply_missing_labels, {}),\n",
        "]\n",
        "\n",
        "for idx, (exp_id, corrupt_func, kwargs) in enumerate(experiments, start=1):\n",
        "    print(f\"\\n[{idx}/4] {exp_id}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Corrupt\n",
        "    print(\"   ðŸ”´ Corrupting...\")\n",
        "    df_corrupt = corrupt_func(df_baseline.copy(), **kwargs)\n",
        "    df_corrupt.to_csv(os.path.join(FIXED_DIR, f\"corrupted/{exp_id}.csv\"), index=False)\n",
        "\n",
        "    # Split\n",
        "    df_train_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(train_ids)].copy()\n",
        "    df_test_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(test_ids)].copy()\n",
        "\n",
        "    # Eval corrupted\n",
        "    print(\"   ðŸ”´ Training on CORRUPTED...\")\n",
        "    metrics_corrupt, stats_corrupt = train_and_evaluate(df_train_corrupt, df_test_corrupt, allow_invalid=True)\n",
        "    record_result(exp_id, \"CORRUPTED\", metrics_corrupt, stats_corrupt)\n",
        "    print(f\"      Acc: {metrics_corrupt['accuracy']:.4f} | Prec: {metrics_corrupt['precision']:.4f} | Rec: {metrics_corrupt['recall']:.4f} | F1: {metrics_corrupt['f1']:.4f}\")\n",
        "\n",
        "    del df_train_corrupt, df_test_corrupt\n",
        "    clear_memory()\n",
        "\n",
        "    # Clean\n",
        "    print(\"   ðŸŸ¢ Cleaning...\")\n",
        "    df_cleaned = clean.clean_all(df_corrupt)\n",
        "    df_cleaned.to_csv(os.path.join(FIXED_DIR, f\"cleaned/{exp_id}_cleaned.csv\"), index=False)\n",
        "    del df_corrupt\n",
        "    clear_memory()\n",
        "\n",
        "    # Split\n",
        "    df_train_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(train_ids)].copy()\n",
        "    df_test_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(test_ids)].copy()\n",
        "\n",
        "    # Eval cleaned\n",
        "    print(\"   ðŸŸ¢ Training on CLEANED...\")\n",
        "    metrics_cleaned, stats_cleaned = train_and_evaluate(df_train_cleaned, df_test_cleaned, allow_invalid=False)\n",
        "    record_result(exp_id, \"CLEANED\", metrics_cleaned, stats_cleaned)\n",
        "    print(f\"      Acc: {metrics_cleaned['accuracy']:.4f} | Prec: {metrics_cleaned['precision']:.4f} | Rec: {metrics_cleaned['recall']:.4f} | F1: {metrics_cleaned['f1']:.4f}\")\n",
        "\n",
        "    # Recovery\n",
        "    acc_drop = metrics_base['accuracy'] - metrics_corrupt['accuracy']\n",
        "    acc_recovery = metrics_cleaned['accuracy'] - metrics_corrupt['accuracy']\n",
        "    recovery_pct = (acc_recovery / acc_drop * 100) if acc_drop != 0 else 0\n",
        "    print(f\"      ðŸ“Š Recovery: {recovery_pct:.1f}%\")\n",
        "\n",
        "    del df_train_cleaned, df_test_cleaned, df_cleaned\n",
        "    clear_memory()\n",
        "\n",
        "# Save results\n",
        "results_path = os.path.join(FIXED_DIR, \"results/batch1_results.csv\")\n",
        "pd.DataFrame(all_results).to_csv(results_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BATCH 1 COMPLETE!\")\n",
        "print(f\"âœ… Results: {results_path}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h7WDHlg1ptJ1",
        "outputId": "e018d5b2-3369-4c1f-d724-57de927caa05"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extreme corruption functions loaded (JENGA + Manual)\n",
            "================================================================================\n",
            "BATCH 1 EVALUATION - First 4 Corruptions\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Loading baseline...\n",
            "   Loaded: 701,421 rows\n",
            "   Sampled: 100,000 rows\n",
            "   Cleaned: 99,985 rows\n",
            "   Split: 79,988 train / 19,997 test\n",
            "\n",
            "ðŸ“ˆ Baseline evaluation...\n",
            "   Acc: 0.7090 | Prec: 0.6388 | Rec: 0.7090 | F1: 0.6498\n",
            "\n",
            "[1/4] 01_missing_values\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.6151 | Prec: 0.5280 | Rec: 0.6151 | F1: 0.4908\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.6716 | Prec: 0.5743 | Rec: 0.6716 | F1: 0.5934\n",
            "      ðŸ“Š Recovery: 60.2%\n",
            "\n",
            "[2/4] 02_broken_chars\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.7139 | Prec: 0.6493 | Rec: 0.7139 | F1: 0.6613\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.7147 | Prec: 0.6513 | Rec: 0.7147 | F1: 0.6622\n",
            "      ðŸ“Š Recovery: -16.8%\n",
            "\n",
            "[3/4] 03_swapped_text\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.8598 | Prec: 0.8303 | Rec: 0.8598 | F1: 0.8377\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.7010 | Prec: 0.6269 | Rec: 0.7010 | F1: 0.6358\n",
            "      ðŸ“Š Recovery: 105.3%\n",
            "\n",
            "[4/4] 04_missing_labels\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.5292 | Prec: 0.4786 | Rec: 0.5292 | F1: 0.4762\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.7043 | Prec: 0.6289 | Rec: 0.7043 | F1: 0.6409\n",
            "      ðŸ“Š Recovery: 97.4%\n",
            "\n",
            "================================================================================\n",
            "BATCH 1 COMPLETE!\n",
            "âœ… Results: /content/drive/MyDrive/data_preparation_project_2026/fixed_batch1/results/batch1_results.csv\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "BATCH 2 EVALUATION - Remaining 6 Corruptions\n",
        "Runs: swapped_labels_manual, noise_injection, truncation, combined_jenga, combined_manual, doomsday\n",
        "\"\"\"\n",
        "\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import gc\n",
        "import json\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION\n",
        "# =========================\n",
        "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
        "FIXED_DIR = os.path.join(BASE_DIR, \"fixed_batch2\")\n",
        "TFIDF_MAX_FEATURES = 50000\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.20\n",
        "SUBSET_SIZE = 100000\n",
        "\n",
        "# =========================\n",
        "# IMPORT TEAM CODE\n",
        "# =========================\n",
        "sys.path.append(os.path.join(BASE_DIR, \"team_code\"))\n",
        "import inject_extreme as inject\n",
        "import clean\n",
        "\n",
        "# =========================\n",
        "# UTILITIES\n",
        "# =========================\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "\n",
        "def ensure_dirs():\n",
        "    for sub in [\"baseline\", \"corrupted\", \"cleaned\", \"results\", \"splits\"]:\n",
        "        os.makedirs(os.path.join(FIXED_DIR, sub), exist_ok=True)\n",
        "\n",
        "def prepare_baseline_data(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    df = df.copy()\n",
        "    df[\"text\"] = df[\"text\"].astype(str)\n",
        "    df[\"label\"] = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[\"label\", \"text\"])\n",
        "    df[\"label\"] = df[\"label\"].astype(int)\n",
        "    df = df[df[\"text\"] != \"nan\"]\n",
        "    df = df[df[\"text\"].str.len() > 0]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df[\"row_id\"] = df.index\n",
        "    return df\n",
        "\n",
        "def valid_row_mask(df: pd.DataFrame) -> pd.Series:\n",
        "    text = df[\"text\"].astype(str)\n",
        "    label = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
        "    mask = label.notna() & text.notna()\n",
        "    mask &= (text != \"nan\") & (text.str.len() > 0)\n",
        "    mask &= label.apply(lambda x: float(x).is_integer())\n",
        "    return mask\n",
        "\n",
        "def build_model():\n",
        "    return Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer(max_features=TFIDF_MAX_FEATURES, stop_words=\"english\")),\n",
        "        (\"clf\", LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "    }\n",
        "\n",
        "def train_and_evaluate(df_train, df_test, allow_invalid=False):\n",
        "    if allow_invalid:\n",
        "        train_valid = df_train.copy()\n",
        "        test_valid = df_test.copy()\n",
        "        train_valid['text'] = train_valid['text'].fillna('')\n",
        "        test_valid['text'] = test_valid['text'].fillna('')\n",
        "        train_valid['label'] = pd.to_numeric(train_valid['label'], errors='coerce').fillna(3).astype(int)\n",
        "        test_valid['label'] = pd.to_numeric(test_valid['label'], errors='coerce').fillna(3).astype(int)\n",
        "    else:\n",
        "        train_mask = valid_row_mask(df_train)\n",
        "        test_mask = valid_row_mask(df_test)\n",
        "        train_valid = df_train[train_mask].copy()\n",
        "        test_valid = df_test[test_mask].copy()\n",
        "\n",
        "    X_train = train_valid[\"text\"].astype(str).values\n",
        "    y_train = pd.to_numeric(train_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
        "    X_test = test_valid[\"text\"].astype(str).values\n",
        "    y_test = pd.to_numeric(test_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
        "\n",
        "    model = build_model()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = compute_metrics(y_test, y_pred)\n",
        "    stats = {\n",
        "        \"train_total\": len(df_train),\n",
        "        \"train_valid\": len(train_valid),\n",
        "        \"test_total\": len(df_test),\n",
        "        \"test_valid\": len(test_valid),\n",
        "    }\n",
        "\n",
        "    del model, X_train, y_train, X_test, y_test, train_valid, test_valid\n",
        "    clear_memory()\n",
        "    return metrics, stats\n",
        "\n",
        "# =========================\n",
        "# MAIN\n",
        "# =========================\n",
        "print(\"=\"*80)\n",
        "print(\"BATCH 2 EVALUATION - Remaining 6 Corruptions\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "ensure_dirs()\n",
        "\n",
        "# Load baseline\n",
        "print(\"\\nðŸ“Š Loading baseline...\")\n",
        "baseline_path = os.path.join(BASE_DIR, \"data/baseline/amazon_reviews_1M.csv\")\n",
        "df_raw = pd.read_csv(baseline_path)\n",
        "print(f\"   Loaded: {len(df_raw):,} rows\")\n",
        "\n",
        "if len(df_raw) > SUBSET_SIZE:\n",
        "    df_raw = df_raw.sample(n=SUBSET_SIZE, random_state=RANDOM_STATE)\n",
        "    print(f\"   Sampled: {SUBSET_SIZE:,} rows\")\n",
        "\n",
        "df_baseline = prepare_baseline_data(df_raw)\n",
        "del df_raw\n",
        "clear_memory()\n",
        "print(f\"   Cleaned: {len(df_baseline):,} rows\")\n",
        "\n",
        "# Split\n",
        "df_train_clean, df_test_clean = train_test_split(\n",
        "    df_baseline, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=df_baseline[\"label\"]\n",
        ")\n",
        "train_ids = set(df_train_clean[\"row_id\"].tolist())\n",
        "test_ids = set(df_test_clean[\"row_id\"].tolist())\n",
        "print(f\"   Split: {len(df_train_clean):,} train / {len(df_test_clean):,} test\")\n",
        "\n",
        "# Baseline eval\n",
        "print(\"\\nðŸ“ˆ Baseline evaluation...\")\n",
        "metrics_base, stats_base = train_and_evaluate(df_train_clean, df_test_clean, allow_invalid=False)\n",
        "print(f\"   Acc: {metrics_base['accuracy']:.4f} | Prec: {metrics_base['precision']:.4f} | Rec: {metrics_base['recall']:.4f} | F1: {metrics_base['f1']:.4f}\")\n",
        "\n",
        "all_results = []\n",
        "def record_result(exp_id, stage, metrics, stats):\n",
        "    row = {\"experiment\": exp_id, \"stage\": stage, **stats, **metrics}\n",
        "    all_results.append(row)\n",
        "\n",
        "record_result(\"BASELINE\", \"CLEAN\", metrics_base, stats_base)\n",
        "\n",
        "# Experiments\n",
        "experiments = [\n",
        "    (\"05_swapped_labels_manual\", inject.apply_swapped_labels_manual, {}),\n",
        "    (\"06_noise_injection\", inject.apply_noise_injection, {}),\n",
        "    (\"07_truncation\", inject.apply_truncation, {}),\n",
        "    (\"08_combined_jenga\", inject.apply_combined_jenga, {}),\n",
        "    (\"09_combined_manual\", inject.apply_combined_manual, {}),\n",
        "    (\"10_doomsday\", inject.apply_doomsday, {}),\n",
        "]\n",
        "\n",
        "for idx, (exp_id, corrupt_func, kwargs) in enumerate(experiments, start=1):\n",
        "    print(f\"\\n[{idx}/6] {exp_id}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # Corrupt\n",
        "    print(\"   ðŸ”´ Corrupting...\")\n",
        "    df_corrupt = corrupt_func(df_baseline.copy(), **kwargs)\n",
        "    df_corrupt.to_csv(os.path.join(FIXED_DIR, f\"corrupted/{exp_id}.csv\"), index=False)\n",
        "\n",
        "    # Split\n",
        "    df_train_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(train_ids)].copy()\n",
        "    df_test_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(test_ids)].copy()\n",
        "\n",
        "    # Eval corrupted\n",
        "    print(\"   ðŸ”´ Training on CORRUPTED...\")\n",
        "    metrics_corrupt, stats_corrupt = train_and_evaluate(df_train_corrupt, df_test_corrupt, allow_invalid=True)\n",
        "    record_result(exp_id, \"CORRUPTED\", metrics_corrupt, stats_corrupt)\n",
        "    print(f\"      Acc: {metrics_corrupt['accuracy']:.4f} | Prec: {metrics_corrupt['precision']:.4f} | Rec: {metrics_corrupt['recall']:.4f} | F1: {metrics_corrupt['f1']:.4f}\")\n",
        "\n",
        "    del df_train_corrupt, df_test_corrupt\n",
        "    clear_memory()\n",
        "\n",
        "    # Clean\n",
        "    print(\"   ðŸŸ¢ Cleaning...\")\n",
        "    df_cleaned = clean.clean_all(df_corrupt)\n",
        "    df_cleaned.to_csv(os.path.join(FIXED_DIR, f\"cleaned/{exp_id}_cleaned.csv\"), index=False)\n",
        "    del df_corrupt\n",
        "    clear_memory()\n",
        "\n",
        "    # Split\n",
        "    df_train_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(train_ids)].copy()\n",
        "    df_test_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(test_ids)].copy()\n",
        "\n",
        "    # Eval cleaned\n",
        "    print(\"   ðŸŸ¢ Training on CLEANED...\")\n",
        "    metrics_cleaned, stats_cleaned = train_and_evaluate(df_train_cleaned, df_test_cleaned, allow_invalid=False)\n",
        "    record_result(exp_id, \"CLEANED\", metrics_cleaned, stats_cleaned)\n",
        "    print(f\"      Acc: {metrics_cleaned['accuracy']:.4f} | Prec: {metrics_cleaned['precision']:.4f} | Rec: {metrics_cleaned['recall']:.4f} | F1: {metrics_cleaned['f1']:.4f}\")\n",
        "\n",
        "    # Recovery\n",
        "    acc_drop = metrics_base['accuracy'] - metrics_corrupt['accuracy']\n",
        "    acc_recovery = metrics_cleaned['accuracy'] - metrics_corrupt['accuracy']\n",
        "    recovery_pct = (acc_recovery / acc_drop * 100) if acc_drop != 0 else 0\n",
        "    print(f\"      ðŸ“Š Recovery: {recovery_pct:.1f}%\")\n",
        "\n",
        "    del df_train_cleaned, df_test_cleaned, df_cleaned\n",
        "    clear_memory()\n",
        "\n",
        "# Save results\n",
        "results_path = os.path.join(FIXED_DIR, \"results/batch2_results.csv\")\n",
        "pd.DataFrame(all_results).to_csv(results_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"BATCH 2 COMPLETE!\")\n",
        "print(f\"âœ… Results: {results_path}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wbvms0dgp_UD",
        "outputId": "bc61042f-d274-40b1-bea1-d08972cc1fe9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "BATCH 2 EVALUATION - Remaining 6 Corruptions\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Loading baseline...\n",
            "   Loaded: 701,421 rows\n",
            "   Sampled: 100,000 rows\n",
            "   Cleaned: 99,985 rows\n",
            "   Split: 79,988 train / 19,997 test\n",
            "\n",
            "ðŸ“ˆ Baseline evaluation...\n",
            "   Acc: 0.7082 | Prec: 0.6364 | Rec: 0.7082 | F1: 0.6485\n",
            "\n",
            "[1/6] 05_swapped_labels_manual\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.6025 | Prec: 0.4621 | Rec: 0.6025 | F1: 0.4826\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.6023 | Prec: 0.4627 | Rec: 0.6023 | F1: 0.4824\n",
            "      ðŸ“Š Recovery: -0.2%\n",
            "\n",
            "[2/6] 06_noise_injection\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.6516 | Prec: 0.5763 | Rec: 0.6516 | F1: 0.5622\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.6520 | Prec: 0.5786 | Rec: 0.6520 | F1: 0.5622\n",
            "      ðŸ“Š Recovery: 0.7%\n",
            "\n",
            "[3/6] 07_truncation\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.6674 | Prec: 0.5771 | Rec: 0.6674 | F1: 0.5903\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.6677 | Prec: 0.5780 | Rec: 0.6677 | F1: 0.5909\n",
            "      ðŸ“Š Recovery: 0.8%\n",
            "\n",
            "[4/6] 08_combined_jenga\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.6615 | Prec: 0.5862 | Rec: 0.6615 | F1: 0.5797\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.6988 | Prec: 0.6260 | Rec: 0.6988 | F1: 0.6339\n",
            "      ðŸ“Š Recovery: 80.0%\n",
            "\n",
            "[5/6] 09_combined_manual\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.6098 | Prec: 0.5022 | Rec: 0.6098 | F1: 0.4971\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.6096 | Prec: 0.5021 | Rec: 0.6096 | F1: 0.4971\n",
            "      ðŸ“Š Recovery: -0.2%\n",
            "\n",
            "[6/6] 10_doomsday\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.4639 | Prec: 0.4067 | Rec: 0.4639 | F1: 0.3716\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.6195 | Prec: 0.5246 | Rec: 0.6195 | F1: 0.5189\n",
            "      ðŸ“Š Recovery: 63.7%\n",
            "\n",
            "================================================================================\n",
            "BATCH 2 COMPLETE!\n",
            "âœ… Results: /content/drive/MyDrive/data_preparation_project_2026/fixed_batch2/results/batch2_results.csv\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 2: Statitical Tests"
      ],
      "metadata": {
        "id": "yfY6-d9JwyPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Statistical Analysis - CORRECTED\n",
        "Uses absolute improvements and proper interpretation\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"STATISTICAL ANALYSIS - CORRECTED\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load results\n",
        "batch1 = pd.read_csv(os.path.join(BASE_DIR, \"fixed_batch1/results/batch1_results.csv\"))\n",
        "batch2 = pd.read_csv(os.path.join(BASE_DIR, \"fixed_batch2/results/batch2_results.csv\"))\n",
        "df_all = pd.concat([batch1, batch2], ignore_index=True)\n",
        "\n",
        "print(f\"\\nâœ… Loaded {len(df_all)} result rows\")\n",
        "\n",
        "# Get baseline\n",
        "baseline_acc = df_all[df_all['stage'] == 'CLEAN']['accuracy'].values[0]\n",
        "\n",
        "# ============================================\n",
        "# ANALYSIS\n",
        "# ============================================\n",
        "stats_results = []\n",
        "\n",
        "experiments = df_all['experiment'].unique()\n",
        "experiments = [e for e in experiments if e != 'BASELINE']\n",
        "\n",
        "for exp in experiments:\n",
        "    exp_data = df_all[df_all['experiment'] == exp]\n",
        "\n",
        "    if len(exp_data) < 2:\n",
        "        continue\n",
        "\n",
        "    corrupted = exp_data[exp_data['stage'] == 'CORRUPTED']\n",
        "    cleaned = exp_data[exp_data['stage'] == 'CLEANED']\n",
        "\n",
        "    if len(corrupted) == 0 or len(cleaned) == 0:\n",
        "        continue\n",
        "\n",
        "    acc_corrupt = corrupted['accuracy'].values[0]\n",
        "    acc_clean = cleaned['accuracy'].values[0]\n",
        "\n",
        "    # Calculate metrics\n",
        "    damage = baseline_acc - acc_corrupt\n",
        "    recovery_abs = acc_clean - acc_corrupt\n",
        "    recovery_pct = (recovery_abs / damage * 100) if damage != 0 else 0\n",
        "\n",
        "    # Interpret damage\n",
        "    def interpret_damage(d):\n",
        "        abs_d = abs(d)\n",
        "        if abs_d < 0.03:\n",
        "            return \"Minimal\"\n",
        "        elif abs_d < 0.08:\n",
        "            return \"Moderate\"\n",
        "        elif abs_d < 0.15:\n",
        "            return \"Severe\"\n",
        "        else:\n",
        "            return \"Critical\"\n",
        "\n",
        "    # Interpret recovery\n",
        "    def interpret_recovery(r):\n",
        "        if r < 0:\n",
        "            return \"Negative (Cleaning Hurt)\"\n",
        "        elif r < 0.01:\n",
        "            return \"Negligible\"\n",
        "        elif r < 0.05:\n",
        "            return \"Small\"\n",
        "        elif r < 0.10:\n",
        "            return \"Moderate\"\n",
        "        else:\n",
        "            return \"Large\"\n",
        "\n",
        "    # Statistical significance (using sample size and improvement)\n",
        "    # For n=20k test samples, improvements >1% are typically significant\n",
        "    n_test = 19997  # Your test size\n",
        "\n",
        "    # Standard error of accuracy difference\n",
        "    se = np.sqrt((acc_corrupt * (1 - acc_corrupt) + acc_clean * (1 - acc_clean)) / n_test)\n",
        "\n",
        "    # Z-score\n",
        "    if se > 0:\n",
        "        z_score = recovery_abs / se\n",
        "        # Two-tailed p-value approximation\n",
        "        from scipy.stats import norm\n",
        "        p_value = 2 * (1 - norm.cdf(abs(z_score)))\n",
        "    else:\n",
        "        p_value = 1.0\n",
        "\n",
        "    sig = \"***\" if p_value < 0.001 else (\"**\" if p_value < 0.01 else (\"*\" if p_value < 0.05 else \"ns\"))\n",
        "\n",
        "    stats_results.append({\n",
        "        'Experiment': exp.replace('_', ' ').title(),\n",
        "        'Baseline': f\"{baseline_acc:.3f}\",\n",
        "        'Corrupted': f\"{acc_corrupt:.3f}\",\n",
        "        'Cleaned': f\"{acc_clean:.3f}\",\n",
        "        'Damage': f\"{damage:.3f}\",\n",
        "        'Damage Level': interpret_damage(damage),\n",
        "        'Recovery (Î”)': f\"{recovery_abs:+.3f}\",\n",
        "        'Recovery (%)': f\"{recovery_pct:.1f}%\",\n",
        "        'Recovery Level': interpret_recovery(recovery_abs),\n",
        "        'p-value': f\"{p_value:.4f}\",\n",
        "        'Significant': sig,\n",
        "    })\n",
        "\n",
        "df_stats = pd.DataFrame(stats_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"CORRUPTION IMPACT & RECOVERY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\n\" + df_stats.to_string(index=False))\n",
        "\n",
        "# Save\n",
        "stats_path = os.path.join(BASE_DIR, \"results/statistical_analysis_corrected.csv\")\n",
        "os.makedirs(os.path.join(BASE_DIR, \"results\"), exist_ok=True)\n",
        "df_stats.to_csv(stats_path, index=False)\n",
        "print(f\"\\nâœ… Saved: {stats_path}\")\n",
        "\n",
        "# ============================================\n",
        "# SUMMARY\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY STATISTICS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"\\nðŸ“Š Corruption Damage Levels:\")\n",
        "for level in ['Minimal', 'Moderate', 'Severe', 'Critical']:\n",
        "    count = len(df_stats[df_stats['Damage Level'] == level])\n",
        "    print(f\"   {level:12s}: {count}/{len(df_stats)}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Recovery Effectiveness:\")\n",
        "for level in ['Negative (Cleaning Hurt)', 'Negligible', 'Small', 'Moderate', 'Large']:\n",
        "    count = len(df_stats[df_stats['Recovery Level'] == level])\n",
        "    print(f\"   {level:30s}: {count}/{len(df_stats)}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Statistical Significance:\")\n",
        "for sig in ['***', '**', '*', 'ns']:\n",
        "    count = len(df_stats[df_stats['Significant'] == sig])\n",
        "    label = \"p<0.001\" if sig == '***' else (\"p<0.01\" if sig == '**' else (\"p<0.05\" if sig == '*' else \"pâ‰¥0.05\"))\n",
        "    print(f\"   {sig:4s} ({label:7s}): {count}/{len(df_stats)}\")\n",
        "\n",
        "# ============================================\n",
        "# KEY FINDINGS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"KEY FINDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Most damaging\n",
        "most_damage_idx = df_stats['Damage'].str.replace('-', '').astype(float).idxmax()\n",
        "most_damage = df_stats.iloc[most_damage_idx]\n",
        "print(f\"\\nðŸ’¥ Most Damaging Corruption:\")\n",
        "print(f\"   {most_damage['Experiment']}\")\n",
        "print(f\"   {most_damage['Baseline']} â†’ {most_damage['Corrupted']} (Î” = {most_damage['Damage']})\")\n",
        "print(f\"   Level: {most_damage['Damage Level']}\")\n",
        "\n",
        "# Best recovery\n",
        "best_recovery_idx = df_stats['Recovery (Î”)'].str.replace('+', '').astype(float).idxmax()\n",
        "best_recovery = df_stats.iloc[best_recovery_idx]\n",
        "print(f\"\\nðŸŽ¯ Best Recovery:\")\n",
        "print(f\"   {best_recovery['Experiment']}\")\n",
        "print(f\"   Improvement: {best_recovery['Recovery (Î”)']} ({best_recovery['Recovery (%)']})\")\n",
        "print(f\"   Level: {best_recovery['Recovery Level']}\")\n",
        "print(f\"   Significance: {best_recovery['Significant']} (p={best_recovery['p-value']})\")\n",
        "\n",
        "# Worst recovery (most negative)\n",
        "worst_recovery_idx = df_stats['Recovery (Î”)'].str.replace('+', '').astype(float).idxmin()\n",
        "worst_recovery = df_stats.iloc[worst_recovery_idx]\n",
        "print(f\"\\nâŒ Worst Recovery (Cleaning Hurt):\")\n",
        "print(f\"   {worst_recovery['Experiment']}\")\n",
        "print(f\"   Change: {worst_recovery['Recovery (Î”)']} ({worst_recovery['Recovery (%)']})\")\n",
        "print(f\"   Level: {worst_recovery['Recovery Level']}\")\n",
        "\n",
        "# Count significant improvements\n",
        "sig_count = len(df_stats[df_stats['Significant'].isin(['*', '**', '***'])])\n",
        "print(f\"\\nðŸ“Š Overall: {sig_count}/{len(df_stats)} corruptions showed statistically significant recovery\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nðŸ“– Interpretation Guide:\")\n",
        "print(\"\\n   Damage Levels (absolute accuracy drop):\")\n",
        "print(\"   â€¢ Minimal:   < 3%\")\n",
        "print(\"   â€¢ Moderate:  3-8%\")\n",
        "print(\"   â€¢ Severe:    8-15%\")\n",
        "print(\"   â€¢ Critical:  > 15%\")\n",
        "print(\"\\n   Recovery Levels (absolute accuracy gain from cleaning):\")\n",
        "print(\"   â€¢ Negative:  Cleaning made it worse\")\n",
        "print(\"   â€¢ Negligible: < 1% improvement\")\n",
        "print(\"   â€¢ Small:     1-5% improvement\")\n",
        "print(\"   â€¢ Moderate:  5-10% improvement\")\n",
        "print(\"   â€¢ Large:     > 10% improvement\")\n",
        "print(\"\\n   Significance:\")\n",
        "print(\"   â€¢ ***: p < 0.001 (highly significant)\")\n",
        "print(\"   â€¢ **:  p < 0.01  (very significant)\")\n",
        "print(\"   â€¢ *:   p < 0.05  (significant)\")\n",
        "print(\"   â€¢ ns:  p â‰¥ 0.05  (not significant)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lpsh0LcGsKMC",
        "outputId": "2c16ae15-8722-4701-ce13-e65a42152c3f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STATISTICAL ANALYSIS - CORRECTED\n",
            "================================================================================\n",
            "\n",
            "âœ… Loaded 22 result rows\n",
            "\n",
            "================================================================================\n",
            "CORRUPTION IMPACT & RECOVERY ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "              Experiment Baseline Corrupted Cleaned Damage Damage Level Recovery (Î”) Recovery (%)           Recovery Level p-value Significant\n",
            "       01 Missing Values    0.709     0.615   0.672  0.094       Severe       +0.057        60.2%                 Moderate  0.0000         ***\n",
            "         02 Broken Chars    0.709     0.714   0.715 -0.005      Minimal       +0.001       -16.8%               Negligible  0.8568          ns\n",
            "         03 Swapped Text    0.709     0.860   0.701 -0.151     Critical       -0.159       105.3% Negative (Cleaning Hurt)  0.0000         ***\n",
            "       04 Missing Labels    0.709     0.529   0.704  0.180     Critical       +0.175        97.4%                    Large  0.0000         ***\n",
            "05 Swapped Labels Manual    0.709     0.603   0.602  0.106       Severe       -0.000        -0.2% Negative (Cleaning Hurt)  0.9632          ns\n",
            "      06 Noise Injection    0.709     0.652   0.652  0.057     Moderate       +0.000         0.7%               Negligible  0.9353          ns\n",
            "           07 Truncation    0.709     0.667   0.668  0.042     Moderate       +0.000         0.8%               Negligible  0.9433          ns\n",
            "       08 Combined Jenga    0.709     0.661   0.699  0.048     Moderate       +0.037        78.5%                    Small  0.0000         ***\n",
            "      09 Combined Manual    0.709     0.610   0.610  0.099       Severe       -0.000        -0.2% Negative (Cleaning Hurt)  0.9657          ns\n",
            "             10 Doomsday    0.709     0.464   0.619  0.245     Critical       +0.156        63.5%                    Large  0.0000         ***\n",
            "\n",
            "âœ… Saved: /content/drive/MyDrive/data_preparation_project_2026/results/statistical_analysis_corrected.csv\n",
            "\n",
            "================================================================================\n",
            "SUMMARY STATISTICS\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Corruption Damage Levels:\n",
            "   Minimal     : 1/10\n",
            "   Moderate    : 3/10\n",
            "   Severe      : 3/10\n",
            "   Critical    : 3/10\n",
            "\n",
            "ðŸ“Š Recovery Effectiveness:\n",
            "   Negative (Cleaning Hurt)      : 3/10\n",
            "   Negligible                    : 3/10\n",
            "   Small                         : 1/10\n",
            "   Moderate                      : 1/10\n",
            "   Large                         : 2/10\n",
            "\n",
            "ðŸ“Š Statistical Significance:\n",
            "   ***  (p<0.001): 5/10\n",
            "   **   (p<0.01 ): 0/10\n",
            "   *    (p<0.05 ): 0/10\n",
            "   ns   (pâ‰¥0.05 ): 5/10\n",
            "\n",
            "================================================================================\n",
            "KEY FINDINGS\n",
            "================================================================================\n",
            "\n",
            "ðŸ’¥ Most Damaging Corruption:\n",
            "   10 Doomsday\n",
            "   0.709 â†’ 0.464 (Î” = 0.245)\n",
            "   Level: Critical\n",
            "\n",
            "ðŸŽ¯ Best Recovery:\n",
            "   04 Missing Labels\n",
            "   Improvement: +0.175 (97.4%)\n",
            "   Level: Large\n",
            "   Significance: *** (p=0.0000)\n",
            "\n",
            "âŒ Worst Recovery (Cleaning Hurt):\n",
            "   03 Swapped Text\n",
            "   Change: -0.159 (105.3%)\n",
            "   Level: Negative (Cleaning Hurt)\n",
            "\n",
            "ðŸ“Š Overall: 5/10 corruptions showed statistically significant recovery\n",
            "\n",
            "================================================================================\n",
            "âœ… ANALYSIS COMPLETE\n",
            "================================================================================\n",
            "\n",
            "ðŸ“– Interpretation Guide:\n",
            "\n",
            "   Damage Levels (absolute accuracy drop):\n",
            "   â€¢ Minimal:   < 3%\n",
            "   â€¢ Moderate:  3-8%\n",
            "   â€¢ Severe:    8-15%\n",
            "   â€¢ Critical:  > 15%\n",
            "\n",
            "   Recovery Levels (absolute accuracy gain from cleaning):\n",
            "   â€¢ Negative:  Cleaning made it worse\n",
            "   â€¢ Negligible: < 1% improvement\n",
            "   â€¢ Small:     1-5% improvement\n",
            "   â€¢ Moderate:  5-10% improvement\n",
            "   â€¢ Large:     > 10% improvement\n",
            "\n",
            "   Significance:\n",
            "   â€¢ ***: p < 0.001 (highly significant)\n",
            "   â€¢ **:  p < 0.01  (very significant)\n",
            "   â€¢ *:   p < 0.05  (significant)\n",
            "   â€¢ ns:  p â‰¥ 0.05  (not significant)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TFDV\n",
        "\n",
        "Change Runtime on Collab!"
      ],
      "metadata": {
        "id": "4lMDnWPP2Cuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "\n",
        "!pip install \\\n",
        "  tensorflow==2.15.1 \\\n",
        "  tensorflow-metadata==1.15.0 \\\n",
        "  tensorflow-data-validation==1.15.1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yHJGV3d1vPwW",
        "outputId": "87b29fd3-fa6b-435f-ef62-2257f26d77da"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.3-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.3\n",
            "Collecting tensorflow==2.15.1\n",
            "  Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
            "Collecting tensorflow-metadata==1.15.0\n",
            "  Downloading tensorflow_metadata-1.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting tensorflow-data-validation==1.15.1\n",
            "  Downloading tensorflow_data_validation-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (3.14.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (18.1.1)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow==2.15.1)\n",
            "  Downloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting numpy<2.0.0,>=1.23.5 (from tensorflow==2.15.1)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (24.2)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow==2.15.1)\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (4.14.1)\n",
            "Collecting wrapt<1.15,>=1.11.0 (from tensorflow==2.15.1)\n",
            "  Downloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.15.1) (1.73.1)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow==2.15.1)\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting keras<2.16,>=2.15.0 (from tensorflow==2.15.1)\n",
            "  Downloading keras-2.15.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.56.4 in /usr/local/lib/python3.11/dist-packages (from tensorflow-metadata==1.15.0) (1.70.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow-data-validation==1.15.1) (1.5.1)\n",
            "Collecting pandas<2,>=1.0 (from tensorflow-data-validation==1.15.1)\n",
            "  Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting pyarrow<11,>=10 (from tensorflow-data-validation==1.15.1)\n",
            "  Downloading pyarrow-10.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting pyfarmhash<0.4,>=0.2.2 (from tensorflow-data-validation==1.15.1)\n",
            "  Downloading pyfarmhash-0.3.2.tar.gz (99 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting tfx-bsl<1.16,>=1.15.1 (from tensorflow-data-validation==1.15.1)\n",
            "  Downloading tfx_bsl-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting apache-beam<3,>=2.53 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading apache_beam-2.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: cryptography<48.0.0,>=39.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (43.0.3)\n",
            "Collecting fastavro<2,>=0.23.6 (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading fastavro-1.12.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.8 kB)\n",
            "Collecting fasteners<1.0,>=0.3 (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading fasteners-0.20-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.15.1)\n",
            "  Downloading grpcio-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: httplib2<0.23.0,>=0.8 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.22.0)\n",
            "Collecting jsonpickle<4.0.0,>=3.0.0 (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading jsonpickle-3.4.2-py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting objsize<0.8.0,>=0.6.1 (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading objsize-0.7.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pymongo<5.0.0,>=3.8.0 (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading pymongo-4.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10.0 kB)\n",
            "Requirement already satisfied: proto-plus<2,>=1.7.1 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.26.1)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2018.3 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2025.2)\n",
            "Collecting requests<3.0.0,>=2.32.4 (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: sortedcontainers>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.4.0)\n",
            "Requirement already satisfied: zstandard<1,>=0.18.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.23.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=3.12 in /usr/local/lib/python3.11/dist-packages (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (6.0.2)\n",
            "Collecting beartype<0.22.0,>=0.21.0 (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading beartype-0.21.0-py3-none-any.whl.metadata (33 kB)\n",
            "Collecting pyarrow-hotfix<1 (from apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: cachetools<7,>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (5.5.2)\n",
            "Requirement already satisfied: google-api-core<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.25.1)\n",
            "Collecting google-apitools<0.5.32,>=0.5.31 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: google-auth<3,>=1.18.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.38.0)\n",
            "Requirement already satisfied: google-auth-httplib2<0.3.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.2.0)\n",
            "Requirement already satisfied: google-cloud-datastore<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.21.0)\n",
            "Collecting google-cloud-pubsub<3,>=2.1.0 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_cloud_pubsub-2.34.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-pubsublite<2,>=1.2.0 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_cloud_pubsublite-1.13.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: google-cloud-storage<3,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.34.0)\n",
            "Requirement already satisfied: google-cloud-bigquery-storage<3,>=2.6.3 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.32.0)\n",
            "Requirement already satisfied: google-cloud-core<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.4.3)\n",
            "Collecting google-cloud-bigtable<3,>=2.19.0 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_cloud_bigtable-2.35.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: google-cloud-spanner<4,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.55.0)\n",
            "Collecting google-cloud-dlp<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_cloud_dlp-3.34.0-py3-none-any.whl.metadata (9.9 kB)\n",
            "Collecting google-cloud-kms<4,>=3.0.0 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_cloud_kms-3.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: google-cloud-language<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.17.2)\n",
            "Collecting google-cloud-secret-manager<3,>=2.0 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_cloud_secret_manager-2.26.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting google-cloud-videointelligence<3,>=2.0 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_cloud_videointelligence-2.18.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting google-cloud-vision<4,>=2 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_cloud_vision-3.12.0-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting google-cloud-recommendations-ai<0.11.0,>=0.1.0 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_cloud_recommendations_ai-0.10.18-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.103.0)\n",
            "Collecting cloud-sql-python-connector<2.0.0,>=1.18.2 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading cloud_sql_python_connector-1.20.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting python-tds>=1.16.1 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading python_tds-1.17.1-py3-none-any.whl.metadata (804 bytes)\n",
            "Collecting pg8000>=1.31.5 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading pg8000-1.31.5-py3-none-any.whl.metadata (88 kB)\n",
            "Collecting PyMySQL>=1.1.0 (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading pymysql-1.1.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: keyrings.google-artifactregistry-auth in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.1.2)\n",
            "Requirement already satisfied: orjson<4,>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.10.18)\n",
            "Requirement already satisfied: regex>=2020.6.8 in /usr/local/lib/python3.11/dist-packages (from apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2024.11.6)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (24.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.11.15)\n",
            "Collecting dnspython>=2.0.0 (from cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<48.0.0,>=39.0.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.17.1)\n",
            "Requirement already satisfied: oauth2client>=1.4.12 in /usr/local/lib/python3.11/dist-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (4.1.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.1.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.25.0)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.11.7)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.11/dist-packages (from google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.16)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.71.2)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigquery<4,>=2.0.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.12.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.14.2)\n",
            "Requirement already satisfied: google-crc32c<2.0.0dev,>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from google-cloud-bigtable<3,>=2.19.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.7.1)\n",
            "Collecting opentelemetry-api>=1.27.0 (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading opentelemetry_api-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting opentelemetry-sdk>=1.27.0 (from google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting overrides<8.0.0,>=6.0.1 (from google-cloud-pubsublite<2,>=1.2.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: sqlparse>=0.4.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.5.3)\n",
            "Requirement already satisfied: grpc-interceptor>=0.15.4 in /usr/local/lib/python3.11/dist-packages (from google-cloud-spanner<4,>=3.0.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.15.4)\n",
            "Requirement already satisfied: anyio<5.0.0,>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (4.9.0)\n",
            "Requirement already satisfied: httpx<1.0.0,>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.28.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0.0,>=4.8.0->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.3.1)\n",
            "INFO: pip is looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-status<2.0.0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading grpcio_status-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.75.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.75.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.74.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.73.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.72.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-status to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_status-1.72.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.71.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_status-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_status-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /usr/local/lib/python3.11/dist-packages (from httplib2<0.23.0,>=0.8->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.2.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->google-cloud-aiplatform<2.0,>=1.26.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.4->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.4->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.11/dist-packages (from rsa<5,>=3.1.4->google-auth<3,>=1.18.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (1.2.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.1.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (2.0.0)\n",
            "Collecting google-api-python-client<2,>=1.7.11 (from tfx-bsl<1.16,>=1.15.1->tensorflow-data-validation==1.15.1)\n",
            "  Downloading google_api_python_client-1.12.11-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.16,>=1.15.1->tensorflow-data-validation==1.15.1)\n",
            "  Downloading tensorflow_serving_api-2.19.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting uritemplate<4dev,>=3.0.0 (from google-api-python-client<2,>=1.7.11->tfx-bsl<1.16,>=1.15.1->tensorflow-data-validation==1.15.1)\n",
            "  Downloading uritemplate-3.0.1-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "INFO: pip is looking at multiple versions of tensorflow-serving-api to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorflow-serving-api<3,>=2.13.0 (from tfx-bsl<1.16,>=1.15.1->tensorflow-data-validation==1.15.1)\n",
            "  Downloading tensorflow_serving_api-2.19.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading tensorflow_serving_api-2.18.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading tensorflow_serving_api-2.18.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading tensorflow_serving_api-2.17.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading tensorflow_serving_api-2.17.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading tensorflow_serving_api-2.16.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "  Downloading tensorflow_serving_api-2.15.1-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow==2.15.1) (0.45.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<48.0.0,>=39.0.0->apache-beam<3,>=2.53->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.22)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (8.7.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.23.0)\n",
            "Collecting opentelemetry-semantic-conventions==0.60b1 (from opentelemetry-sdk>=1.27.0->google-cloud-pubsub<3,>=2.1.0->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting scramp>=1.4.5 (from pg8000>=1.31.5->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading scramp-1.4.8-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.3.1)\n",
            "Collecting asn1crypto>=1.5.1 (from scramp>=1.4.5->pg8000>=1.31.5->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1)\n",
            "  Downloading asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow==2.15.1) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (6.6.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->cloud-sql-python-connector<2.0.0,>=1.18.2->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.20.1)\n",
            "Requirement already satisfied: keyring in /usr/local/lib/python3.11/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (25.6.0)\n",
            "Requirement already satisfied: pluggy in /usr/local/lib/python3.11/dist-packages (from keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.6.0)\n",
            "Requirement already satisfied: SecretStorage>=3.2 in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.3.3)\n",
            "Requirement already satisfied: jeepney>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (0.9.0)\n",
            "Requirement already satisfied: jaraco.classes in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (3.4.0)\n",
            "Requirement already satisfied: jaraco.functools in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (4.2.1)\n",
            "Requirement already satisfied: jaraco.context in /usr/local/lib/python3.11/dist-packages (from keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (6.0.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from jaraco.classes->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (10.7.0)\n",
            "Requirement already satisfied: backports.tarfile in /usr/local/lib/python3.11/dist-packages (from jaraco.context->keyring->keyrings.google-artifactregistry-auth->apache-beam[gcp]<3,>=2.53; python_version >= \"3.11\"->tensorflow-data-validation==1.15.1) (1.2.0)\n",
            "Downloading tensorflow-2.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m475.3/475.3 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m  \u001b[33m0:00:13\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_metadata-1.15.0-py3-none-any.whl (28 kB)\n",
            "Downloading tensorflow_data_validation-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m55.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading apache_beam-2.70.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading beartype-0.21.0-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloud_sql_python_connector-1.20.0-py3-none-any.whl (50 kB)\n",
            "Downloading fastavro-1.12.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fasteners-0.20-py3-none-any.whl (18 kB)\n",
            "Downloading google_cloud_bigtable-2.35.0-py3-none-any.whl (540 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m540.3/540.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_cloud_dlp-3.34.0-py3-none-any.whl (220 kB)\n",
            "Downloading google_cloud_kms-3.9.0-py3-none-any.whl (275 kB)\n",
            "Downloading google_cloud_pubsub-2.34.0-py3-none-any.whl (320 kB)\n",
            "Downloading google_cloud_pubsublite-1.13.0-py3-none-any.whl (324 kB)\n",
            "Downloading google_cloud_recommendations_ai-0.10.18-py3-none-any.whl (212 kB)\n",
            "Downloading google_cloud_secret_manager-2.26.0-py3-none-any.whl (223 kB)\n",
            "Downloading google_cloud_videointelligence-2.18.0-py3-none-any.whl (285 kB)\n",
            "Downloading google_cloud_vision-3.12.0-py3-none-any.whl (538 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m538.2/538.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio-1.65.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_status-1.62.3-py3-none-any.whl (14 kB)\n",
            "Downloading jsonpickle-3.4.2-py3-none-any.whl (46 kB)\n",
            "Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ml_dtypes-0.3.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading objsize-0.7.1-py3-none-any.whl (11 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading pandas-1.5.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Downloading pyarrow-10.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m35.8/35.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\n",
            "Downloading pymongo-4.16.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "Downloading tfx_bsl-1.15.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m22.5/22.5 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_api_python_client-1.12.11-py2.py3-none-any.whl (62 kB)\n",
            "Downloading tensorflow_serving_api-2.15.1-py2.py3-none-any.whl (26 kB)\n",
            "Downloading uritemplate-3.0.1-py2.py3-none-any.whl (15 kB)\n",
            "Downloading wrapt-1.14.2-cp311-cp311-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (77 kB)\n",
            "Downloading opentelemetry_api-1.39.1-py3-none-any.whl (66 kB)\n",
            "Downloading opentelemetry_sdk-1.39.1-py3-none-any.whl (132 kB)\n",
            "Downloading opentelemetry_semantic_conventions-0.60b1-py3-none-any.whl (219 kB)\n",
            "Downloading pg8000-1.31.5-py3-none-any.whl (57 kB)\n",
            "Downloading pymysql-1.1.2-py3-none-any.whl (45 kB)\n",
            "Downloading python_tds-1.17.1-py3-none-any.whl (86 kB)\n",
            "Downloading scramp-1.4.8-py3-none-any.whl (13 kB)\n",
            "Downloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
            "Building wheels for collected packages: google-apitools, pyfarmhash\n",
            "  Building wheel for google-apitools (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131075 sha256=53f830f9b1cc0de58a490e11d06147d01ad139fb71025c4655cba1eadbeff689\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/7b/c5/b3dc55a0d70c63727a2fd1332b3cc14193d6d12219aa1372c4\n",
            "  Building wheel for pyfarmhash (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfarmhash: filename=pyfarmhash-0.3.2-cp311-cp311-linux_x86_64.whl size=88745 sha256=2ef122e23cbe6d673d4dcf9dfc93908da8517b9fad7fd6e67810929ba7942857\n",
            "  Stored in directory: /root/.cache/pip/wheels/06/9a/f4/53839ebeb753b56ac36f76456e54854347c7b6e634731fbc03\n",
            "Successfully built google-apitools pyfarmhash\n",
            "Installing collected packages: python-tds, pyfarmhash, asn1crypto, wrapt, uritemplate, tensorflow-estimator, scramp, requests, PyMySQL, pyarrow-hotfix, protobuf, overrides, objsize, numpy, keras, jsonpickle, grpcio, fasteners, fastavro, dnspython, beartype, pymongo, pyarrow, pg8000, pandas, opentelemetry-api, ml-dtypes, tensorflow-metadata, opentelemetry-semantic-conventions, grpcio-status, google-apitools, cloud-sql-python-connector, apache-beam, tensorboard, opentelemetry-sdk, google-api-python-client, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-secret-manager, google-cloud-recommendations-ai, google-cloud-pubsub, google-cloud-kms, google-cloud-dlp, google-cloud-bigtable, tensorflow-serving-api, google-cloud-pubsublite, tfx-bsl, tensorflow-data-validation\n",
            "\u001b[2K  Attempting uninstall: wrapt\n",
            "\u001b[2K    Found existing installation: wrapt 1.17.2\n",
            "\u001b[2K    Uninstalling wrapt-1.17.2:\n",
            "\u001b[2K      Successfully uninstalled wrapt-1.17.2\n",
            "\u001b[2K  Attempting uninstall: uritemplate\n",
            "\u001b[2K    Found existing installation: uritemplate 4.2.0\n",
            "\u001b[2K    Uninstalling uritemplate-4.2.0:\n",
            "\u001b[2K      Successfully uninstalled uritemplate-4.2.0\n",
            "\u001b[2K  Attempting uninstall: requests\n",
            "\u001b[2K    Found existing installation: requests 2.32.3\n",
            "\u001b[2K    Uninstalling requests-2.32.3:\n",
            "\u001b[2K      Successfully uninstalled requests-2.32.3\n",
            "\u001b[2K  Attempting uninstall: protobuf\n",
            "\u001b[2K    Found existing installation: protobuf 5.29.5\n",
            "\u001b[2K    Uninstalling protobuf-5.29.5:\n",
            "\u001b[2K      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 2.0.2\n",
            "\u001b[2K    Uninstalling numpy-2.0.2:\n",
            "\u001b[2K      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[2K  Attempting uninstall: keras\n",
            "\u001b[2K    Found existing installation: keras 3.8.0\n",
            "\u001b[2K    Uninstalling keras-3.8.0:\n",
            "\u001b[2K      Successfully uninstalled keras-3.8.0\n",
            "\u001b[2K  Attempting uninstall: jsonpickle\n",
            "\u001b[2K    Found existing installation: jsonpickle 4.1.1\n",
            "\u001b[2K    Uninstalling jsonpickle-4.1.1:\n",
            "\u001b[2K      Successfully uninstalled jsonpickle-4.1.1\n",
            "\u001b[2K  Attempting uninstall: grpcio\n",
            "\u001b[2K    Found existing installation: grpcio 1.73.1\n",
            "\u001b[2K    Uninstalling grpcio-1.73.1:\n",
            "\u001b[2K      Successfully uninstalled grpcio-1.73.1\n",
            "\u001b[2K  Attempting uninstall: pyarrow\n",
            "\u001b[2K    Found existing installation: pyarrow 18.1.0\n",
            "\u001b[2K    Uninstalling pyarrow-18.1.0:\n",
            "\u001b[2K      Successfully uninstalled pyarrow-18.1.0\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.2.2\n",
            "\u001b[2K    Uninstalling pandas-2.2.2:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[2K  Attempting uninstall: ml-dtypes\n",
            "\u001b[2K    Found existing installation: ml-dtypes 0.4.1\n",
            "\u001b[2K    Uninstalling ml-dtypes-0.4.1:\n",
            "\u001b[2K      Successfully uninstalled ml-dtypes-0.4.1\n",
            "\u001b[2K  Attempting uninstall: tensorflow-metadata\n",
            "\u001b[2K    Found existing installation: tensorflow-metadata 1.17.2\n",
            "\u001b[2K    Uninstalling tensorflow-metadata-1.17.2:\n",
            "\u001b[2K      Successfully uninstalled tensorflow-metadata-1.17.2\n",
            "\u001b[2K  Attempting uninstall: grpcio-status\n",
            "\u001b[2K    Found existing installation: grpcio-status 1.71.2\n",
            "\u001b[2K    Uninstalling grpcio-status-1.71.2:\n",
            "\u001b[2K      Successfully uninstalled grpcio-status-1.71.2\n",
            "\u001b[2K  Attempting uninstall: tensorboard\n",
            "\u001b[2K    Found existing installation: tensorboard 2.18.0\n",
            "\u001b[2K    Uninstalling tensorboard-2.18.0:\n",
            "\u001b[2K      Successfully uninstalled tensorboard-2.18.0\n",
            "\u001b[2K  Attempting uninstall: google-api-python-client\n",
            "\u001b[2K    Found existing installation: google-api-python-client 2.176.0\n",
            "\u001b[2K    Uninstalling google-api-python-client-2.176.0:\n",
            "\u001b[2K      Successfully uninstalled google-api-python-client-2.176.0\n",
            "\u001b[2K  Attempting uninstall: tensorflow\n",
            "\u001b[2K    Found existing installation: tensorflow 2.18.0\n",
            "\u001b[2K    Uninstalling tensorflow-2.18.0:\n",
            "\u001b[2K      Successfully uninstalled tensorflow-2.18.0\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49/49\u001b[0m [tensorflow-data-validation]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.32.5 which is incompatible.\n",
            "bigframes 2.10.0 requires pyarrow>=15.0.2, but you have pyarrow 10.0.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 10.0.1 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pandas>=2, but you have pandas 1.5.3 which is incompatible.\n",
            "dask-expr 1.1.21 requires pyarrow>=14.0.1, but you have pyarrow 10.0.1 which is incompatible.\n",
            "db-dtypes 1.4.3 requires pyarrow>=13.0.0, but you have pyarrow 10.0.1 which is incompatible.\n",
            "jax 0.5.2 requires ml_dtypes>=0.4.0, but you have ml-dtypes 0.3.2 which is incompatible.\n",
            "mizani 0.13.5 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "plotnine 0.14.6 requires pandas>=2.2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "pylibcudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 10.0.1 which is incompatible.\n",
            "tensorflow-decision-forests 1.11.0 requires tensorflow==2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
            "tensorflow-text 2.18.1 requires tensorflow<2.19,>=2.18.0, but you have tensorflow 2.15.1 which is incompatible.\n",
            "tf-keras 2.18.0 requires tensorflow<2.19,>=2.18, but you have tensorflow 2.15.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "xarray 2025.3.1 requires pandas>=2.1, but you have pandas 1.5.3 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMySQL-1.1.2 apache-beam-2.70.0 asn1crypto-1.5.1 beartype-0.21.0 cloud-sql-python-connector-1.20.0 dnspython-2.8.0 fastavro-1.12.1 fasteners-0.20 google-api-python-client-1.12.11 google-apitools-0.5.31 google-cloud-bigtable-2.35.0 google-cloud-dlp-3.34.0 google-cloud-kms-3.9.0 google-cloud-pubsub-2.34.0 google-cloud-pubsublite-1.13.0 google-cloud-recommendations-ai-0.10.18 google-cloud-secret-manager-2.26.0 google-cloud-videointelligence-2.18.0 google-cloud-vision-3.12.0 grpcio-1.65.5 grpcio-status-1.62.3 jsonpickle-3.4.2 keras-2.15.0 ml-dtypes-0.3.2 numpy-1.26.4 objsize-0.7.1 opentelemetry-api-1.39.1 opentelemetry-sdk-1.39.1 opentelemetry-semantic-conventions-0.60b1 overrides-7.7.0 pandas-1.5.3 pg8000-1.31.5 protobuf-4.25.8 pyarrow-10.0.1 pyarrow-hotfix-0.7 pyfarmhash-0.3.2 pymongo-4.16.0 python-tds-1.17.1 requests-2.32.5 scramp-1.4.8 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-data-validation-1.15.1 tensorflow-estimator-2.15.0 tensorflow-metadata-1.15.0 tensorflow-serving-api-2.15.1 tfx-bsl-1.15.1 uritemplate-3.0.1 wrapt-1.14.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google",
                  "numpy",
                  "pandas",
                  "pyarrow"
                ]
              },
              "id": "a40532cf6b9d4317a03d5f9b9ec1b2f0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import tensorflow as tf\n",
        "import tensorflow_data_validation as tfdv\n",
        "\n",
        "print(\" Python:\", sys.version)\n",
        "print(\" TensorFlow:\", tf.__version__)\n",
        "print(\" TFDV:\", tfdv.__version__)\n",
        "print(\"\\n TFDV successfully installed!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ukbkNU_bxL7d",
        "outputId": "0a3b127c-1cda-4602-ff03-ab207d21b072"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:crcmod package not found. This package is required if python-snappy or google-crc32c are not installed. To ensure crcmod is installed, install the tfrecord extra: pip install apache-beam[tfrecord]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Python: 3.11.13 (main, Jun  4 2025, 08:57:29) [GCC 11.4.0]\n",
            " TensorFlow: 2.15.1\n",
            " TFDV: 1.15.1\n",
            "\n",
            " TFDV successfully installed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
        "\n",
        "print(\"CHECKING YOUR ACTUAL STRUCTURE\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Check what exists\n",
        "for batch in ['fixed_batch1', 'fixed_batch2']:\n",
        "    print(f\"\\n{batch}:\")\n",
        "    for sub in ['baseline', 'corrupted', 'cleaned', 'results']:\n",
        "        path = os.path.join(BASE_DIR, batch, sub)\n",
        "        if os.path.exists(path):\n",
        "            files = os.listdir(path)\n",
        "            print(f\"  âœ… {sub}/ ({len(files)} files)\")\n",
        "        else:\n",
        "            print(f\"  âŒ {sub}/ (missing)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LAD4Tobz-Df",
        "outputId": "2942ec0f-cb2d-41a1-ff47-6758bde79ada"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHECKING YOUR ACTUAL STRUCTURE\n",
            "============================================================\n",
            "\n",
            "fixed_batch1:\n",
            "  âœ… baseline/ (0 files)\n",
            "  âœ… corrupted/ (4 files)\n",
            "  âœ… cleaned/ (4 files)\n",
            "  âœ… results/ (1 files)\n",
            "\n",
            "fixed_batch2:\n",
            "  âœ… baseline/ (0 files)\n",
            "  âœ… corrupted/ (6 files)\n",
            "  âœ… cleaned/ (6 files)\n",
            "  âœ… results/ (1 files)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PART 2: TensorFlow Data Validation (TFDV)\n",
        "Detect corruption at the data level before model training\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow_data_validation as tfdv\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PART 2: TENSORFLOW DATA VALIDATION (TFDV)\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nGoal: Detect corruption BEFORE training models\")\n",
        "print(\"Show which corruptions can be caught via data validation\\n\")\n",
        "\n",
        "# ============================================\n",
        "# 1. LOAD BASELINE & GENERATE SCHEMA\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 1: Generate Schema from Clean Baseline\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Try multiple possible baseline locations\n",
        "baseline_paths = [\n",
        "    os.path.join(BASE_DIR, \"fixed/baseline/amazon_clean_with_rowid.csv\"),\n",
        "    os.path.join(BASE_DIR, \"fixed_batch1/baseline/amazon_clean_with_rowid.csv\"),\n",
        "    os.path.join(BASE_DIR, \"data/baseline/amazon_reviews_1M.csv\"),\n",
        "]\n",
        "\n",
        "df_baseline = None\n",
        "baseline_path = None\n",
        "\n",
        "for path in baseline_paths:\n",
        "    if os.path.exists(path):\n",
        "        baseline_path = path\n",
        "        df_baseline = pd.read_csv(path)\n",
        "        break\n",
        "\n",
        "if df_baseline is None:\n",
        "    print(\"âŒ ERROR: Could not find baseline file in any expected location:\")\n",
        "    for path in baseline_paths:\n",
        "        print(f\"   - {path}\")\n",
        "    print(\"\\nPlease check your file paths!\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"âœ… Loaded baseline from: {baseline_path}\")\n",
        "print(f\"   {len(df_baseline):,} rows\")\n",
        "print(f\"   Columns: {list(df_baseline.columns)}\")\n",
        "\n",
        "# Generate statistics from baseline\n",
        "print(\"\\nðŸ“Š Generating baseline statistics...\")\n",
        "baseline_stats = tfdv.generate_statistics_from_dataframe(df_baseline[['text', 'label']])\n",
        "\n",
        "# Infer schema\n",
        "print(\"ðŸ“‹ Inferring schema from baseline...\")\n",
        "schema = tfdv.infer_schema(statistics=baseline_stats)\n",
        "\n",
        "print(\"\\nâœ… Schema inferred:\")\n",
        "print(f\"   Features: {[f.name for f in schema.feature]}\")\n",
        "\n",
        "# ============================================\n",
        "# 2. VALIDATE CORRUPTED DATASETS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: Validate Corrupted Datasets Against Schema\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# All corrupted datasets - try multiple directories\n",
        "corrupted_files = [\n",
        "    ('01_missing_values', ['fixed_batch1', 'fixed']),\n",
        "    ('02_broken_chars', ['fixed_batch1', 'fixed']),\n",
        "    ('03_swapped_text', ['fixed_batch1', 'fixed']),\n",
        "    ('04_missing_labels', ['fixed_batch1', 'fixed']),\n",
        "    ('05_swapped_labels_manual', ['fixed_batch2', 'fixed']),\n",
        "    ('06_noise_injection', ['fixed_batch2', 'fixed']),\n",
        "    ('07_truncation', ['fixed_batch2', 'fixed']),\n",
        "    ('08_combined_jenga', ['fixed_batch2', 'fixed']),\n",
        "    ('09_combined_manual', ['fixed_batch2', 'fixed']),\n",
        "    ('10_doomsday', ['fixed_batch2', 'fixed']),\n",
        "]\n",
        "\n",
        "tfdv_results = []\n",
        "\n",
        "for exp_id, batch_dirs in corrupted_files:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Validating: {exp_id}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    try:\n",
        "        # Try multiple possible paths\n",
        "        df_corrupt = None\n",
        "        corrupt_path = None\n",
        "\n",
        "        for batch_dir in batch_dirs:\n",
        "            path = os.path.join(BASE_DIR, batch_dir, f\"corrupted/{exp_id}.csv\")\n",
        "            if os.path.exists(path):\n",
        "                corrupt_path = path\n",
        "                df_corrupt = pd.read_csv(path)\n",
        "                break\n",
        "\n",
        "        if df_corrupt is None:\n",
        "            print(f\"   âš ï¸  File not found, skipping...\")\n",
        "            continue\n",
        "\n",
        "        print(f\"   Loaded: {len(df_corrupt):,} rows\")\n",
        "\n",
        "        # Generate statistics\n",
        "        print(\"   ðŸ“Š Generating statistics...\")\n",
        "        corrupt_stats = tfdv.generate_statistics_from_dataframe(df_corrupt[['text', 'label']])\n",
        "\n",
        "        # Validate against schema\n",
        "        print(\"   ðŸ” Validating against schema...\")\n",
        "        anomalies = tfdv.validate_statistics(statistics=corrupt_stats, schema=schema)\n",
        "\n",
        "        # Parse anomalies\n",
        "        anomaly_info = MessageToDict(anomalies)\n",
        "\n",
        "        # Count anomalies\n",
        "        n_anomalies = len(anomaly_info.get('anomalyInfo', {}))\n",
        "\n",
        "        # Extract specific anomaly types\n",
        "        anomaly_types = []\n",
        "        missing_features = 0\n",
        "        high_missing = 0\n",
        "        unexpected_values = 0\n",
        "\n",
        "        for feature_name, anomaly_data in anomaly_info.get('anomalyInfo', {}).items():\n",
        "            reason = anomaly_data.get('shortDescription', '')\n",
        "            anomaly_types.append(f\"{feature_name}: {reason}\")\n",
        "\n",
        "            if 'missing' in reason.lower():\n",
        "                missing_features += 1\n",
        "                # Check severity\n",
        "                if 'column dropped' in reason.lower() or 'high' in reason.lower():\n",
        "                    high_missing += 1\n",
        "\n",
        "            if 'unexpected' in reason.lower() or 'out of bounds' in reason.lower():\n",
        "                unexpected_values += 1\n",
        "\n",
        "        # Calculate drift (compare distributions)\n",
        "        print(\"   ðŸ“‰ Calculating distribution drift...\")\n",
        "\n",
        "        # For text: check completeness\n",
        "        text_complete_baseline = df_baseline['text'].notna().sum() / len(df_baseline)\n",
        "        text_complete_corrupt = df_corrupt['text'].notna().sum() / len(df_corrupt)\n",
        "        text_drift = abs(text_complete_baseline - text_complete_corrupt)\n",
        "\n",
        "        # For label: check completeness + distribution\n",
        "        label_complete_baseline = df_baseline['label'].notna().sum() / len(df_baseline)\n",
        "        label_complete_corrupt = df_corrupt['label'].notna().sum() / len(df_corrupt)\n",
        "        label_drift = abs(label_complete_baseline - label_complete_corrupt)\n",
        "\n",
        "        # Distribution shift (KS test would be here, simplified to completeness for now)\n",
        "        overall_drift = (text_drift + label_drift) / 2\n",
        "\n",
        "        # Detectability score (can TFDV catch this corruption?)\n",
        "        if n_anomalies > 0 or overall_drift > 0.05:\n",
        "            detectability = \"High\"\n",
        "        elif overall_drift > 0.01:\n",
        "            detectability = \"Medium\"\n",
        "        else:\n",
        "            detectability = \"Low\"\n",
        "\n",
        "        # Print results\n",
        "        print(f\"\\n   âœ… TFDV Results:\")\n",
        "        print(f\"      Anomalies detected: {n_anomalies}\")\n",
        "        print(f\"      Missing features: {missing_features}\")\n",
        "        print(f\"      Unexpected values: {unexpected_values}\")\n",
        "        print(f\"      Text completeness: {text_complete_corrupt:.1%} (Î” = {text_drift:.1%})\")\n",
        "        print(f\"      Label completeness: {label_complete_corrupt:.1%} (Î” = {label_drift:.1%})\")\n",
        "        print(f\"      Overall drift: {overall_drift:.1%}\")\n",
        "        print(f\"      Detectability: {detectability}\")\n",
        "\n",
        "        # Store results\n",
        "        tfdv_results.append({\n",
        "            'Experiment': exp_id.replace('_', ' ').title(),\n",
        "            'Anomalies': n_anomalies,\n",
        "            'Missing Features': missing_features,\n",
        "            'Unexpected Values': unexpected_values,\n",
        "            'Text Completeness': f\"{text_complete_corrupt:.1%}\",\n",
        "            'Text Drift': f\"{text_drift:.1%}\",\n",
        "            'Label Completeness': f\"{label_complete_corrupt:.1%}\",\n",
        "            'Label Drift': f\"{label_drift:.1%}\",\n",
        "            'Overall Drift': f\"{overall_drift:.1%}\",\n",
        "            'Detectability': detectability,\n",
        "            'Anomaly Details': '; '.join(anomaly_types[:3]) if anomaly_types else 'None'\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ Error: {e}\")\n",
        "        tfdv_results.append({\n",
        "            'Experiment': exp_id.replace('_', ' ').title(),\n",
        "            'Anomalies': 'Error',\n",
        "            'Missing Features': 'Error',\n",
        "            'Unexpected Values': 'Error',\n",
        "            'Text Completeness': 'Error',\n",
        "            'Text Drift': 'Error',\n",
        "            'Label Completeness': 'Error',\n",
        "            'Label Drift': 'Error',\n",
        "            'Overall Drift': 'Error',\n",
        "            'Detectability': 'Error',\n",
        "            'Anomaly Details': str(e)\n",
        "        })\n",
        "\n",
        "# ============================================\n",
        "# 3. SAVE RESULTS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TFDV RESULTS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_tfdv = pd.DataFrame(tfdv_results)\n",
        "print(\"\\n\" + df_tfdv.to_string(index=False))\n",
        "\n",
        "# Save\n",
        "tfdv_path = os.path.join(BASE_DIR, \"results/PART2_TFDV_Analysis.csv\")\n",
        "os.makedirs(os.path.join(BASE_DIR, \"results\"), exist_ok=True)\n",
        "df_tfdv.to_csv(tfdv_path, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Saved: {tfdv_path}\")\n",
        "\n",
        "# ============================================\n",
        "# 4. SUMMARY STATISTICS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"DETECTABILITY ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Filter out errors\n",
        "df_tfdv_valid = df_tfdv[df_tfdv['Detectability'] != 'Error']\n",
        "\n",
        "print(f\"\\nðŸ“Š Corruption Detectability via TFDV:\")\n",
        "for level in ['High', 'Medium', 'Low']:\n",
        "    count = len(df_tfdv_valid[df_tfdv_valid['Detectability'] == level])\n",
        "    total = len(df_tfdv_valid)\n",
        "    pct = count / total * 100 if total > 0 else 0\n",
        "    print(f\"   {level:8s}: {count}/{total} ({pct:.0f}%)\")\n",
        "\n",
        "# Average drift by detectability\n",
        "if len(df_tfdv_valid) > 0:\n",
        "    print(f\"\\nðŸ“Š Average Drift by Detectability:\")\n",
        "    for level in ['High', 'Medium', 'Low']:\n",
        "        subset = df_tfdv_valid[df_tfdv_valid['Detectability'] == level]\n",
        "        if len(subset) > 0:\n",
        "            # Convert drift strings to float\n",
        "            drifts = subset['Overall Drift'].str.rstrip('%').astype(float)\n",
        "            avg_drift = drifts.mean()\n",
        "            print(f\"   {level:8s}: {avg_drift:.1f}% average drift\")\n",
        "\n",
        "# Anomaly detection rate\n",
        "if len(df_tfdv_valid) > 0:\n",
        "    detected = len(df_tfdv_valid[df_tfdv_valid['Anomalies'] > 0])\n",
        "    total = len(df_tfdv_valid)\n",
        "    print(f\"\\nðŸ“Š Detection Rate:\")\n",
        "    print(f\"   {detected}/{total} corruptions generated anomalies ({detected/total*100:.0f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… PART 2 COMPLETE - TFDV ANALYSIS DONE!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nðŸ”‘ Key Insight:\")\n",
        "print(\"   TFDV can detect structural corruptions (missing values, labels)\")\n",
        "print(\"   but struggles with semantic corruptions (swapped labels, noise)\")\n",
        "print(\"   This validates the need for model-based evaluation (Part 1)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_SQwa-_ynf3",
        "outputId": "daa459a2-cdde-4634-d9bd-c3a72254552c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PART 2: TENSORFLOW DATA VALIDATION (TFDV)\n",
            "================================================================================\n",
            "\n",
            "Goal: Detect corruption BEFORE training models\n",
            "Show which corruptions can be caught via data validation\n",
            "\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Generate Schema from Clean Baseline\n",
            "================================================================================\n",
            "âœ… Loaded baseline from: /content/drive/MyDrive/data_preparation_project_2026/fixed/baseline/amazon_clean_with_rowid.csv\n",
            "   99,985 rows\n",
            "   Columns: ['text', 'label', 'row_id']\n",
            "\n",
            "ðŸ“Š Generating baseline statistics...\n",
            "ðŸ“‹ Inferring schema from baseline...\n",
            "\n",
            "âœ… Schema inferred:\n",
            "   Features: ['text', 'label']\n",
            "\n",
            "================================================================================\n",
            "STEP 2: Validate Corrupted Datasets Against Schema\n",
            "================================================================================\n",
            "\n",
            "============================================================\n",
            "Validating: 01_missing_values\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 1\n",
            "      Missing features: 0\n",
            "      Unexpected values: 0\n",
            "      Text completeness: 20.0% (Î” = 80.0%)\n",
            "      Label completeness: 100.0% (Î” = 0.0%)\n",
            "      Overall drift: 40.0%\n",
            "      Detectability: High\n",
            "\n",
            "============================================================\n",
            "Validating: 02_broken_chars\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 0\n",
            "      Missing features: 0\n",
            "      Unexpected values: 0\n",
            "      Text completeness: 100.0% (Î” = 0.0%)\n",
            "      Label completeness: 100.0% (Î” = 0.0%)\n",
            "      Overall drift: 0.0%\n",
            "      Detectability: Low\n",
            "\n",
            "============================================================\n",
            "Validating: 03_swapped_text\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 1\n",
            "      Missing features: 0\n",
            "      Unexpected values: 1\n",
            "      Text completeness: 100.0% (Î” = 0.0%)\n",
            "      Label completeness: 100.0% (Î” = 0.0%)\n",
            "      Overall drift: 0.0%\n",
            "      Detectability: High\n",
            "\n",
            "============================================================\n",
            "Validating: 04_missing_labels\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 1\n",
            "      Missing features: 0\n",
            "      Unexpected values: 0\n",
            "      Text completeness: 100.0% (Î” = 0.0%)\n",
            "      Label completeness: 50.0% (Î” = 50.0%)\n",
            "      Overall drift: 25.0%\n",
            "      Detectability: High\n",
            "\n",
            "============================================================\n",
            "Validating: 05_swapped_labels_manual\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 0\n",
            "      Missing features: 0\n",
            "      Unexpected values: 0\n",
            "      Text completeness: 100.0% (Î” = 0.0%)\n",
            "      Label completeness: 100.0% (Î” = 0.0%)\n",
            "      Overall drift: 0.0%\n",
            "      Detectability: Low\n",
            "\n",
            "============================================================\n",
            "Validating: 06_noise_injection\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 0\n",
            "      Missing features: 0\n",
            "      Unexpected values: 0\n",
            "      Text completeness: 100.0% (Î” = 0.0%)\n",
            "      Label completeness: 100.0% (Î” = 0.0%)\n",
            "      Overall drift: 0.0%\n",
            "      Detectability: Low\n",
            "\n",
            "============================================================\n",
            "Validating: 07_truncation\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 0\n",
            "      Missing features: 0\n",
            "      Unexpected values: 0\n",
            "      Text completeness: 100.0% (Î” = 0.0%)\n",
            "      Label completeness: 100.0% (Î” = 0.0%)\n",
            "      Overall drift: 0.0%\n",
            "      Detectability: Low\n",
            "\n",
            "============================================================\n",
            "Validating: 08_combined_jenga\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 1\n",
            "      Missing features: 0\n",
            "      Unexpected values: 0\n",
            "      Text completeness: 75.5% (Î” = 24.5%)\n",
            "      Label completeness: 100.0% (Î” = 0.0%)\n",
            "      Overall drift: 12.3%\n",
            "      Detectability: High\n",
            "\n",
            "============================================================\n",
            "Validating: 09_combined_manual\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 0\n",
            "      Missing features: 0\n",
            "      Unexpected values: 0\n",
            "      Text completeness: 100.0% (Î” = 0.0%)\n",
            "      Label completeness: 100.0% (Î” = 0.0%)\n",
            "      Overall drift: 0.0%\n",
            "      Detectability: Low\n",
            "\n",
            "============================================================\n",
            "Validating: 10_doomsday\n",
            "============================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š Generating statistics...\n",
            "   ðŸ” Validating against schema...\n",
            "   ðŸ“‰ Calculating distribution drift...\n",
            "\n",
            "   âœ… TFDV Results:\n",
            "      Anomalies detected: 2\n",
            "      Missing features: 0\n",
            "      Unexpected values: 0\n",
            "      Text completeness: 75.0% (Î” = 25.0%)\n",
            "      Label completeness: 75.0% (Î” = 25.0%)\n",
            "      Overall drift: 25.0%\n",
            "      Detectability: High\n",
            "\n",
            "================================================================================\n",
            "TFDV RESULTS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "              Experiment  Anomalies  Missing Features  Unexpected Values Text Completeness Text Drift Label Completeness Label Drift Overall Drift Detectability                               Anomaly Details\n",
            "       01 Missing Values          1                 0                  0             20.0%      80.0%             100.0%        0.0%         40.0%          High                         text: Multiple errors\n",
            "         02 Broken Chars          0                 0                  0            100.0%       0.0%             100.0%        0.0%          0.0%           Low                                          None\n",
            "         03 Swapped Text          1                 0                  1            100.0%       0.0%             100.0%        0.0%          0.0%          High                   label: Unexpected data type\n",
            "       04 Missing Labels          1                 0                  0            100.0%       0.0%              50.0%       50.0%         25.0%          High                        label: Multiple errors\n",
            "05 Swapped Labels Manual          0                 0                  0            100.0%       0.0%             100.0%        0.0%          0.0%           Low                                          None\n",
            "      06 Noise Injection          0                 0                  0            100.0%       0.0%             100.0%        0.0%          0.0%           Low                                          None\n",
            "           07 Truncation          0                 0                  0            100.0%       0.0%             100.0%        0.0%          0.0%           Low                                          None\n",
            "       08 Combined Jenga          1                 0                  0             75.5%      24.5%             100.0%        0.0%         12.3%          High                         text: Multiple errors\n",
            "      09 Combined Manual          0                 0                  0            100.0%       0.0%             100.0%        0.0%          0.0%           Low                                          None\n",
            "             10 Doomsday          2                 0                  0             75.0%      25.0%              75.0%       25.0%         25.0%          High text: Multiple errors; label: Multiple errors\n",
            "\n",
            "âœ… Saved: /content/drive/MyDrive/data_preparation_project_2026/results/PART2_TFDV_Analysis.csv\n",
            "\n",
            "================================================================================\n",
            "DETECTABILITY ANALYSIS\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Corruption Detectability via TFDV:\n",
            "   High    : 5/10 (50%)\n",
            "   Medium  : 0/10 (0%)\n",
            "   Low     : 5/10 (50%)\n",
            "\n",
            "ðŸ“Š Average Drift by Detectability:\n",
            "   High    : 20.5% average drift\n",
            "   Low     : 0.0% average drift\n",
            "\n",
            "ðŸ“Š Detection Rate:\n",
            "   5/10 corruptions generated anomalies (50%)\n",
            "\n",
            "================================================================================\n",
            "âœ… PART 2 COMPLETE - TFDV ANALYSIS DONE!\n",
            "================================================================================\n",
            "\n",
            "ðŸ”‘ Key Insight:\n",
            "   TFDV can detect structural corruptions (missing values, labels)\n",
            "   but struggles with semantic corruptions (swapped labels, noise)\n",
            "   This validates the need for model-based evaluation (Part 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "PART 2: TENSORFLOW DATA VALIDATION (TFDV) - COMPREHENSIVE\n",
        "Complete end-to-end data validation analysis\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import tensorflow_data_validation as tfdv\n",
        "from google.protobuf.json_format import MessageToDict\n",
        "from scipy.stats import ks_2samp, entropy\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"PART 2: TENSORFLOW DATA VALIDATION (TFDV) - COMPREHENSIVE\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nEnd-to-end data quality assessment:\")\n",
        "print(\"1. Schema validation & anomaly detection\")\n",
        "print(\"2. Distribution drift & skew analysis\")\n",
        "print(\"3. Feature-level statistics\")\n",
        "print(\"4. Corruption detectability scoring\")\n",
        "print()\n",
        "\n",
        "# ============================================\n",
        "# 1. LOAD BASELINE & GENERATE SCHEMA\n",
        "# ============================================\n",
        "print(\"=\"*80)\n",
        "print(\"STEP 1: Generate Schema from Clean Baseline\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "baseline_paths = [\n",
        "    os.path.join(BASE_DIR, \"fixed/baseline/amazon_clean_with_rowid.csv\"),\n",
        "    os.path.join(BASE_DIR, \"data/baseline/amazon_reviews_1M.csv\"),\n",
        "]\n",
        "\n",
        "baseline_path = None\n",
        "for path in baseline_paths:\n",
        "    if os.path.exists(path):\n",
        "        baseline_path = path\n",
        "        break\n",
        "\n",
        "if baseline_path is None:\n",
        "    print(\"âŒ ERROR: Could not find baseline file\")\n",
        "    sys.exit(1)\n",
        "\n",
        "print(f\"âœ… Loaded baseline from: {baseline_path}\")\n",
        "df_baseline = pd.read_csv(baseline_path)\n",
        "\n",
        "# Clean if needed\n",
        "if 'amazon_reviews_1M.csv' in baseline_path:\n",
        "    print(\"   ðŸ§¹ Cleaning raw baseline...\")\n",
        "    df_baseline[\"text\"] = df_baseline[\"text\"].astype(str)\n",
        "    df_baseline[\"label\"] = pd.to_numeric(df_baseline[\"label\"], errors=\"coerce\")\n",
        "    df_baseline = df_baseline.dropna(subset=[\"label\", \"text\"])\n",
        "    df_baseline[\"label\"] = df_baseline[\"label\"].astype(int)\n",
        "    df_baseline = df_baseline[df_baseline[\"text\"] != \"nan\"]\n",
        "    df_baseline = df_baseline[df_baseline[\"text\"].str.len() > 0]\n",
        "    if len(df_baseline) > 100000:\n",
        "        df_baseline = df_baseline.sample(n=100000, random_state=42)\n",
        "\n",
        "print(f\"   {len(df_baseline):,} rows\")\n",
        "\n",
        "# Generate baseline statistics\n",
        "print(\"\\nðŸ“Š Generating baseline statistics...\")\n",
        "baseline_stats = tfdv.generate_statistics_from_dataframe(df_baseline[['text', 'label']])\n",
        "\n",
        "# Infer schema\n",
        "print(\"ðŸ“‹ Inferring schema...\")\n",
        "schema = tfdv.infer_schema(statistics=baseline_stats)\n",
        "\n",
        "# Get baseline label distribution\n",
        "baseline_label_dist = df_baseline['label'].value_counts(normalize=True).sort_index()\n",
        "print(f\"\\nðŸ“Š Baseline Label Distribution:\")\n",
        "for label, pct in baseline_label_dist.items():\n",
        "    print(f\"   Label {label}: {pct:.1%}\")\n",
        "\n",
        "print(f\"\\nâœ… Schema inferred with {len(schema.feature)} features\")\n",
        "\n",
        "# ============================================\n",
        "# 2. COMPREHENSIVE VALIDATION\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"STEP 2: Comprehensive Corruption Analysis\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "corrupted_files = [\n",
        "    ('01_missing_values', ['fixed', 'fixed_batch1']),\n",
        "    ('02_broken_chars', ['fixed', 'fixed_batch1']),\n",
        "    ('03_swapped_text', ['fixed', 'fixed_batch1']),\n",
        "    ('04_missing_labels', ['fixed', 'fixed_batch1']),\n",
        "    ('05_swapped_labels_manual', ['fixed', 'fixed_batch2']),\n",
        "    ('06_noise_injection', ['fixed', 'fixed_batch2']),\n",
        "    ('07_truncation', ['fixed', 'fixed_batch2']),\n",
        "    ('08_combined_jenga', ['fixed', 'fixed_batch2']),\n",
        "    ('09_combined_manual', ['fixed', 'fixed_batch2']),\n",
        "    ('10_doomsday', ['fixed', 'fixed_batch2']),\n",
        "]\n",
        "\n",
        "tfdv_results = []\n",
        "\n",
        "for exp_id, batch_dirs in corrupted_files:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Analyzing: {exp_id}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    try:\n",
        "        # Find corrupted file\n",
        "        df_corrupt = None\n",
        "        for batch_dir in batch_dirs:\n",
        "            path = os.path.join(BASE_DIR, batch_dir, f\"corrupted/{exp_id}.csv\")\n",
        "            if os.path.exists(path):\n",
        "                df_corrupt = pd.read_csv(path)\n",
        "                break\n",
        "\n",
        "        if df_corrupt is None:\n",
        "            print(f\"   âš ï¸  File not found, skipping...\")\n",
        "            continue\n",
        "\n",
        "        print(f\"   Loaded: {len(df_corrupt):,} rows\")\n",
        "\n",
        "        # ==================\n",
        "        # A. SCHEMA VALIDATION\n",
        "        # ==================\n",
        "        print(\"   ðŸ“Š A. Schema Validation...\")\n",
        "        corrupt_stats = tfdv.generate_statistics_from_dataframe(df_corrupt[['text', 'label']])\n",
        "        anomalies = tfdv.validate_statistics(statistics=corrupt_stats, schema=schema)\n",
        "\n",
        "        anomaly_info = MessageToDict(anomalies)\n",
        "        n_anomalies = len(anomaly_info.get('anomalyInfo', {}))\n",
        "\n",
        "        anomaly_types = []\n",
        "        for feature_name, anomaly_data in anomaly_info.get('anomalyInfo', {}).items():\n",
        "            reason = anomaly_data.get('shortDescription', '')\n",
        "            anomaly_types.append(f\"{feature_name}: {reason}\")\n",
        "\n",
        "        # ==================\n",
        "        # B. COMPLETENESS DRIFT\n",
        "        # ==================\n",
        "        print(\"   ðŸ“Š B. Completeness Analysis...\")\n",
        "        text_complete_base = df_baseline['text'].notna().sum() / len(df_baseline)\n",
        "        text_complete_corrupt = df_corrupt['text'].notna().sum() / len(df_corrupt)\n",
        "        text_drift = abs(text_complete_base - text_complete_corrupt)\n",
        "\n",
        "        label_complete_base = df_baseline['label'].notna().sum() / len(df_baseline)\n",
        "        label_complete_corrupt = df_corrupt['label'].notna().sum() / len(df_corrupt)\n",
        "        label_drift = abs(label_complete_base - label_complete_corrupt)\n",
        "\n",
        "        # ==================\n",
        "        # C. DISTRIBUTION SKEW\n",
        "        # ==================\n",
        "        print(\"   ðŸ“Š C. Distribution Skew Analysis...\")\n",
        "\n",
        "        # Label distribution comparison\n",
        "        corrupt_label_dist = df_corrupt['label'].value_counts(normalize=True).sort_index()\n",
        "\n",
        "        # Align distributions (fill missing labels with 0)\n",
        "        all_labels = sorted(set(baseline_label_dist.index) | set(corrupt_label_dist.index))\n",
        "        base_aligned = [baseline_label_dist.get(l, 0) for l in all_labels]\n",
        "        corrupt_aligned = [corrupt_label_dist.get(l, 0) for l in all_labels]\n",
        "\n",
        "        # L1 distance (total variation)\n",
        "        l1_distance = sum(abs(b - c) for b, c in zip(base_aligned, corrupt_aligned)) / 2\n",
        "\n",
        "        # Jensen-Shannon divergence (symmetric KL divergence)\n",
        "        # Add small epsilon to avoid log(0)\n",
        "        base_smooth = [p + 1e-10 for p in base_aligned]\n",
        "        corrupt_smooth = [p + 1e-10 for p in corrupt_aligned]\n",
        "        js_divergence = jensenshannon(base_smooth, corrupt_smooth)\n",
        "\n",
        "        # Kolmogorov-Smirnov test (for numeric labels)\n",
        "        ks_statistic, ks_pvalue = ks_2samp(\n",
        "            df_baseline['label'].dropna().values,\n",
        "            df_corrupt['label'].dropna().values\n",
        "        )\n",
        "\n",
        "        # ==================\n",
        "        # D. FEATURE STATISTICS\n",
        "        # ==================\n",
        "        print(\"   ðŸ“Š D. Feature-Level Statistics...\")\n",
        "\n",
        "        # Text length statistics\n",
        "        base_text_lengths = df_baseline['text'].str.len()\n",
        "        corrupt_text_lengths = df_corrupt['text'].str.len()\n",
        "\n",
        "        avg_length_base = base_text_lengths.mean()\n",
        "        avg_length_corrupt = corrupt_text_lengths.mean()\n",
        "        length_change = ((avg_length_corrupt - avg_length_base) / avg_length_base * 100) if avg_length_base > 0 else 0\n",
        "\n",
        "        # ==================\n",
        "        # E. DETECTABILITY SCORING\n",
        "        # ==================\n",
        "        print(\"   ðŸ“Š E. Detectability Scoring...\")\n",
        "\n",
        "        # Composite detectability score\n",
        "        completeness_score = (text_drift + label_drift) / 2\n",
        "        distribution_score = l1_distance\n",
        "        anomaly_score = min(n_anomalies / 2, 1.0)  # Normalize to 0-1\n",
        "\n",
        "        # Weighted composite (40% completeness, 40% distribution, 20% anomalies)\n",
        "        detectability_score = (\n",
        "            0.4 * completeness_score +\n",
        "            0.4 * distribution_score +\n",
        "            0.2 * anomaly_score\n",
        "        )\n",
        "\n",
        "        if detectability_score > 0.2:\n",
        "            detectability = \"High\"\n",
        "        elif detectability_score > 0.05:\n",
        "            detectability = \"Medium\"\n",
        "        else:\n",
        "            detectability = \"Low\"\n",
        "\n",
        "        # ==================\n",
        "        # F. PRINT RESULTS\n",
        "        # ==================\n",
        "        print(f\"\\n   âœ… VALIDATION RESULTS:\")\n",
        "        print(f\"      Schema Anomalies: {n_anomalies}\")\n",
        "        print(f\"      Text Completeness: {text_complete_corrupt:.1%} (Î” = {text_drift:.1%})\")\n",
        "        print(f\"      Label Completeness: {label_complete_corrupt:.1%} (Î” = {label_drift:.1%})\")\n",
        "        print(f\"      Label Distribution L1: {l1_distance:.3f}\")\n",
        "        print(f\"      Jensen-Shannon Divergence: {js_divergence:.3f}\")\n",
        "        print(f\"      KS Statistic: {ks_statistic:.3f} (p={ks_pvalue:.4f})\")\n",
        "        print(f\"      Avg Text Length Change: {length_change:+.1f}%\")\n",
        "        print(f\"      Detectability Score: {detectability_score:.3f} ({detectability})\")\n",
        "\n",
        "        # Store results\n",
        "        tfdv_results.append({\n",
        "            'Experiment': exp_id.replace('_', ' ').title(),\n",
        "            'Schema Anomalies': n_anomalies,\n",
        "            'Text Completeness': f\"{text_complete_corrupt:.1%}\",\n",
        "            'Text Drift': f\"{text_drift:.1%}\",\n",
        "            'Label Completeness': f\"{label_complete_corrupt:.1%}\",\n",
        "            'Label Drift': f\"{label_drift:.1%}\",\n",
        "            'L1 Distance': f\"{l1_distance:.3f}\",\n",
        "            'JS Divergence': f\"{js_divergence:.3f}\",\n",
        "            'KS Statistic': f\"{ks_statistic:.3f}\",\n",
        "            'KS p-value': f\"{ks_pvalue:.4f}\",\n",
        "            'Text Length Change': f\"{length_change:+.1f}%\",\n",
        "            'Detectability Score': f\"{detectability_score:.3f}\",\n",
        "            'Detectability': detectability,\n",
        "            'Anomaly Details': '; '.join(anomaly_types[:3]) if anomaly_types else 'None'\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"   âŒ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        continue\n",
        "\n",
        "# ============================================\n",
        "# 3. SAVE COMPREHENSIVE RESULTS\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPREHENSIVE TFDV RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "df_tfdv = pd.DataFrame(tfdv_results)\n",
        "print(\"\\n\" + df_tfdv.to_string(index=False))\n",
        "\n",
        "# Save\n",
        "tfdv_path = os.path.join(BASE_DIR, \"results/PART2_TFDV_Comprehensive.csv\")\n",
        "os.makedirs(os.path.join(BASE_DIR, \"results\"), exist_ok=True)\n",
        "df_tfdv.to_csv(tfdv_path, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Saved: {tfdv_path}\")\n",
        "\n",
        "# ============================================\n",
        "# 4. ANALYSIS SUMMARY\n",
        "# ============================================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ANALYSIS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Detectability distribution\n",
        "print(f\"\\nðŸ“Š Corruption Detectability:\")\n",
        "for level in ['High', 'Medium', 'Low']:\n",
        "    count = len(df_tfdv[df_tfdv['Detectability'] == level])\n",
        "    pct = count / len(df_tfdv) * 100 if len(df_tfdv) > 0 else 0\n",
        "    print(f\"   {level:8s}: {count}/{len(df_tfdv)} ({pct:.0f}%)\")\n",
        "\n",
        "# Anomaly detection rate\n",
        "detected = len(df_tfdv[df_tfdv['Schema Anomalies'] > 0])\n",
        "print(f\"\\nðŸ“Š Detection Metrics:\")\n",
        "print(f\"   Anomaly Detection Rate: {detected}/{len(df_tfdv)} ({detected/len(df_tfdv)*100:.0f}%)\")\n",
        "\n",
        "# Distribution shift significance\n",
        "sig_ks = len(df_tfdv[df_tfdv['KS p-value'].astype(float) < 0.05])\n",
        "print(f\"   Significant Distribution Shifts: {sig_ks}/{len(df_tfdv)} ({sig_ks/len(df_tfdv)*100:.0f}%)\")\n",
        "\n",
        "# Average metrics by detectability\n",
        "print(f\"\\nðŸ“Š Average Drift by Detectability:\")\n",
        "for level in ['High', 'Medium', 'Low']:\n",
        "    subset = df_tfdv[df_tfdv['Detectability'] == level]\n",
        "    if len(subset) > 0:\n",
        "        avg_score = subset['Detectability Score'].astype(float).mean()\n",
        "        print(f\"   {level:8s}: Score = {avg_score:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… PART 2 COMPLETE - COMPREHENSIVE TFDV ANALYSIS!\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\nðŸ”‘ Key Findings:\")\n",
        "print(f\"   â€¢ {detected}/{len(df_tfdv)} corruptions detected via schema anomalies\")\n",
        "print(f\"   â€¢ {sig_ks}/{len(df_tfdv)} corruptions show significant distribution shifts\")\n",
        "print(\"   â€¢ Structural corruptions (missing values/labels) have high detectability\")\n",
        "print(\"   â€¢ Semantic corruptions (swapped/noise) show low detectability\")\n",
        "print(\"   â€¢ TFDV is necessary but insufficient - model evaluation required\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lMZKqwWhy-sU",
        "outputId": "a0136765-ebab-4fcb-8425-9034761678bb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PART 2: TENSORFLOW DATA VALIDATION (TFDV) - COMPREHENSIVE\n",
            "================================================================================\n",
            "\n",
            "End-to-end data quality assessment:\n",
            "1. Schema validation & anomaly detection\n",
            "2. Distribution drift & skew analysis\n",
            "3. Feature-level statistics\n",
            "4. Corruption detectability scoring\n",
            "\n",
            "================================================================================\n",
            "STEP 1: Generate Schema from Clean Baseline\n",
            "================================================================================\n",
            "âœ… Loaded baseline from: /content/drive/MyDrive/data_preparation_project_2026/fixed/baseline/amazon_clean_with_rowid.csv\n",
            "   99,985 rows\n",
            "\n",
            "ðŸ“Š Generating baseline statistics...\n",
            "ðŸ“‹ Inferring schema...\n",
            "\n",
            "ðŸ“Š Baseline Label Distribution:\n",
            "   Label 1: 14.5%\n",
            "   Label 2: 6.3%\n",
            "   Label 3: 8.0%\n",
            "   Label 4: 11.3%\n",
            "   Label 5: 59.9%\n",
            "\n",
            "âœ… Schema inferred with 2 features\n",
            "\n",
            "================================================================================\n",
            "STEP 2: Comprehensive Corruption Analysis\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 01_missing_values\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 1\n",
            "      Text Completeness: 30.0% (Î” = 70.0%)\n",
            "      Label Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Distribution L1: 0.000\n",
            "      Jensen-Shannon Divergence: 0.000\n",
            "      KS Statistic: 0.000 (p=1.0000)\n",
            "      Avg Text Length Change: -0.7%\n",
            "      Detectability Score: 0.240 (High)\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 02_broken_chars\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 0\n",
            "      Text Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Distribution L1: 0.000\n",
            "      Jensen-Shannon Divergence: 0.000\n",
            "      KS Statistic: 0.000 (p=1.0000)\n",
            "      Avg Text Length Change: +0.0%\n",
            "      Detectability Score: 0.000 (Low)\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 03_swapped_text\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 0\n",
            "      Text Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Distribution L1: 0.000\n",
            "      Jensen-Shannon Divergence: 0.000\n",
            "      KS Statistic: 0.000 (p=1.0000)\n",
            "      Avg Text Length Change: -48.5%\n",
            "      Detectability Score: 0.000 (Low)\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 04_missing_labels\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 1\n",
            "      Text Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Completeness: 60.0% (Î” = 40.0%)\n",
            "      Label Distribution L1: 0.002\n",
            "      Jensen-Shannon Divergence: 0.002\n",
            "      KS Statistic: 0.002 (p=1.0000)\n",
            "      Avg Text Length Change: +0.0%\n",
            "      Detectability Score: 0.181 (Medium)\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 05_swapped_labels_manual\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 0\n",
            "      Text Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Distribution L1: 0.000\n",
            "      Jensen-Shannon Divergence: 0.000\n",
            "      KS Statistic: 0.000 (p=1.0000)\n",
            "      Avg Text Length Change: +0.0%\n",
            "      Detectability Score: 0.000 (Low)\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 06_noise_injection\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 0\n",
            "      Text Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Distribution L1: 0.000\n",
            "      Jensen-Shannon Divergence: 0.000\n",
            "      KS Statistic: 0.000 (p=1.0000)\n",
            "      Avg Text Length Change: -43.9%\n",
            "      Detectability Score: 0.000 (Low)\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 07_truncation\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 0\n",
            "      Text Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Distribution L1: 0.000\n",
            "      Jensen-Shannon Divergence: 0.000\n",
            "      KS Statistic: 0.000 (p=1.0000)\n",
            "      Avg Text Length Change: -75.0%\n",
            "      Detectability Score: 0.000 (Low)\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 08_combined_jenga\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 1\n",
            "      Text Completeness: 75.5% (Î” = 24.5%)\n",
            "      Label Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Distribution L1: 0.000\n",
            "      Jensen-Shannon Divergence: 0.000\n",
            "      KS Statistic: 0.000 (p=1.0000)\n",
            "      Avg Text Length Change: -38.7%\n",
            "      Detectability Score: 0.149 (Medium)\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 09_combined_manual\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 0\n",
            "      Text Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Completeness: 100.0% (Î” = 0.0%)\n",
            "      Label Distribution L1: 0.000\n",
            "      Jensen-Shannon Divergence: 0.000\n",
            "      KS Statistic: 0.000 (p=1.0000)\n",
            "      Avg Text Length Change: -82.2%\n",
            "      Detectability Score: 0.000 (Low)\n",
            "\n",
            "================================================================================\n",
            "Analyzing: 10_doomsday\n",
            "================================================================================\n",
            "   Loaded: 99,985 rows\n",
            "   ðŸ“Š A. Schema Validation...\n",
            "   ðŸ“Š B. Completeness Analysis...\n",
            "   ðŸ“Š C. Distribution Skew Analysis...\n",
            "   ðŸ“Š D. Feature-Level Statistics...\n",
            "   ðŸ“Š E. Detectability Scoring...\n",
            "\n",
            "   âœ… VALIDATION RESULTS:\n",
            "      Schema Anomalies: 2\n",
            "      Text Completeness: 75.0% (Î” = 25.0%)\n",
            "      Label Completeness: 75.0% (Î” = 25.0%)\n",
            "      Label Distribution L1: 0.001\n",
            "      Jensen-Shannon Divergence: 0.001\n",
            "      KS Statistic: 0.001 (p=1.0000)\n",
            "      Avg Text Length Change: -23.5%\n",
            "      Detectability Score: 0.301 (High)\n",
            "\n",
            "================================================================================\n",
            "COMPREHENSIVE TFDV RESULTS\n",
            "================================================================================\n",
            "\n",
            "              Experiment  Schema Anomalies Text Completeness Text Drift Label Completeness Label Drift L1 Distance JS Divergence KS Statistic KS p-value Text Length Change Detectability Score Detectability                               Anomaly Details\n",
            "       01 Missing Values                 1             30.0%      70.0%             100.0%        0.0%       0.000         0.000        0.000     1.0000              -0.7%               0.240          High                         text: Multiple errors\n",
            "         02 Broken Chars                 0            100.0%       0.0%             100.0%        0.0%       0.000         0.000        0.000     1.0000              +0.0%               0.000           Low                                          None\n",
            "         03 Swapped Text                 0            100.0%       0.0%             100.0%        0.0%       0.000         0.000        0.000     1.0000             -48.5%               0.000           Low                                          None\n",
            "       04 Missing Labels                 1            100.0%       0.0%              60.0%       40.0%       0.002         0.002        0.002     1.0000              +0.0%               0.181        Medium                        label: Multiple errors\n",
            "05 Swapped Labels Manual                 0            100.0%       0.0%             100.0%        0.0%       0.000         0.000        0.000     1.0000              +0.0%               0.000           Low                                          None\n",
            "      06 Noise Injection                 0            100.0%       0.0%             100.0%        0.0%       0.000         0.000        0.000     1.0000             -43.9%               0.000           Low                                          None\n",
            "           07 Truncation                 0            100.0%       0.0%             100.0%        0.0%       0.000         0.000        0.000     1.0000             -75.0%               0.000           Low                                          None\n",
            "       08 Combined Jenga                 1             75.5%      24.5%             100.0%        0.0%       0.000         0.000        0.000     1.0000             -38.7%               0.149        Medium                         text: Multiple errors\n",
            "      09 Combined Manual                 0            100.0%       0.0%             100.0%        0.0%       0.000         0.000        0.000     1.0000             -82.2%               0.000           Low                                          None\n",
            "             10 Doomsday                 2             75.0%      25.0%              75.0%       25.0%       0.001         0.001        0.001     1.0000             -23.5%               0.301          High text: Multiple errors; label: Multiple errors\n",
            "\n",
            "âœ… Saved: /content/drive/MyDrive/data_preparation_project_2026/results/PART2_TFDV_Comprehensive.csv\n",
            "\n",
            "================================================================================\n",
            "ANALYSIS SUMMARY\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Corruption Detectability:\n",
            "   High    : 2/10 (20%)\n",
            "   Medium  : 2/10 (20%)\n",
            "   Low     : 6/10 (60%)\n",
            "\n",
            "ðŸ“Š Detection Metrics:\n",
            "   Anomaly Detection Rate: 4/10 (40%)\n",
            "   Significant Distribution Shifts: 0/10 (0%)\n",
            "\n",
            "ðŸ“Š Average Drift by Detectability:\n",
            "   High    : Score = 0.270\n",
            "   Medium  : Score = 0.165\n",
            "   Low     : Score = 0.000\n",
            "\n",
            "================================================================================\n",
            "âœ… PART 2 COMPLETE - COMPREHENSIVE TFDV ANALYSIS!\n",
            "================================================================================\n",
            "\n",
            "ðŸ”‘ Key Findings:\n",
            "   â€¢ 4/10 corruptions detected via schema anomalies\n",
            "   â€¢ 0/10 corruptions show significant distribution shifts\n",
            "   â€¢ Structural corruptions (missing values/labels) have high detectability\n",
            "   â€¢ Semantic corruptions (swapped/noise) show low detectability\n",
            "   â€¢ TFDV is necessary but insufficient - model evaluation required\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ADULT DATASET"
      ],
      "metadata": {
        "id": "CARuPDmkoKqf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "ADULT DATASET EVALUATION - TEAM'S EXACT APPROACH\n",
        "================================================\n",
        "\n",
        "Their actual approach (from notebook):\n",
        "1. Use ONLY \"occupation\" column as \"text\"\n",
        "2. Use TfidfVectorizer + LogisticRegression (same as Amazon!)\n",
        "3. Apply their inject/clean modules\n",
        "4. That's it!\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import gc\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# =========================\n",
        "# CONFIGURATION\n",
        "# =========================\n",
        "BASE_DIR = \"/content/drive/MyDrive/data_preparation_project_2026\"\n",
        "FIXED_DIR = os.path.join(BASE_DIR, \"fixed_adult\")\n",
        "TFIDF_MAX_FEATURES = 5000\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.20\n",
        "\n",
        "# =========================\n",
        "# IMPORT TEAM CODE\n",
        "# =========================\n",
        "sys.path.append(os.path.join(BASE_DIR, \"team_code\"))\n",
        "import inject_extreme as inject\n",
        "import clean\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"ADULT DATASET EVALUATION - TEAM'S EXACT APPROACH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# =========================\n",
        "# CREATE DIRECTORIES\n",
        "# =========================\n",
        "def ensure_dirs():\n",
        "    for sub in [\"baseline\", \"corrupted\", \"cleaned\", \"results\"]:\n",
        "        os.makedirs(os.path.join(FIXED_DIR, sub), exist_ok=True)\n",
        "\n",
        "ensure_dirs()\n",
        "\n",
        "# =========================\n",
        "# LOAD ADULT DATASET\n",
        "# =========================\n",
        "print(\"\\nðŸ“Š Loading Adult Dataset...\")\n",
        "\n",
        "ADULT_PATH = os.path.join(BASE_DIR, \"data/raw/adult.csv\")\n",
        "df_raw = pd.read_csv(ADULT_PATH)\n",
        "print(f\"   Loaded: {len(df_raw):,} rows\")\n",
        "\n",
        "# Remove rows with missing values\n",
        "df_raw = df_raw.dropna()\n",
        "\n",
        "# Handle column naming\n",
        "if 'class' in df_raw.columns:\n",
        "    target_col = 'class'\n",
        "elif 'income' in df_raw.columns:\n",
        "    target_col = 'income'\n",
        "else:\n",
        "    raise ValueError(\"Target column not found!\")\n",
        "\n",
        "# Clean target column\n",
        "df_raw[target_col] = df_raw[target_col].str.replace('.', '', regex=False).str.strip()\n",
        "\n",
        "print(f\"   After dropna: {len(df_raw):,} rows\")\n",
        "\n",
        "# =========================\n",
        "# PREPARE IN TEXT/LABEL FORMAT (THEIR EXACT WAY)\n",
        "# =========================\n",
        "print(\"\\nðŸ”§ Converting to text/label format (EXACTLY like their notebook)...\")\n",
        "\n",
        "# THEIR EXACT APPROACH: Just use occupation as \"text\"!\n",
        "df_baseline = pd.DataFrame()\n",
        "df_baseline['row_id'] = df_raw.reset_index(drop=True).index\n",
        "df_baseline['text'] = df_raw['occupation'].astype(str)  # Just occupation!\n",
        "df_baseline['label'] = df_raw[target_col].map({'<=50K': 1, '>50K': 5})\n",
        "\n",
        "# Drop any rows where mapping failed\n",
        "df_baseline = df_baseline.dropna(subset=['label'])\n",
        "df_baseline['label'] = df_baseline['label'].astype(int)\n",
        "\n",
        "print(f\"   Baseline created: {len(df_baseline):,} rows\")\n",
        "print(f\"   text = occupation column\")\n",
        "print(f\"   label = income (1 for <=50K, 5 for >50K)\")\n",
        "print(f\"\\n   Sample:\")\n",
        "print(df_baseline.head(3))\n",
        "\n",
        "# =========================\n",
        "# FIXED TRAIN/TEST SPLIT\n",
        "# =========================\n",
        "print(\"\\nâœ‚ï¸  Creating fixed train/test split...\")\n",
        "\n",
        "df_train_clean, df_test_clean = train_test_split(\n",
        "    df_baseline,\n",
        "    test_size=TEST_SIZE,\n",
        "    random_state=RANDOM_STATE,\n",
        "    stratify=df_baseline['label']\n",
        ")\n",
        "\n",
        "train_ids = set(df_train_clean[\"row_id\"].tolist())\n",
        "test_ids = set(df_test_clean[\"row_id\"].tolist())\n",
        "\n",
        "print(f\"   Train: {len(train_ids):,} samples\")\n",
        "print(f\"   Test: {len(test_ids):,} samples\")\n",
        "\n",
        "# =========================\n",
        "# BUILD MODEL (THEIR EXACT MODEL - TfidfVectorizer + LogisticRegression)\n",
        "# =========================\n",
        "print(\"\\nðŸ—ï¸  Building model (EXACT same as their Amazon/Adult model)...\")\n",
        "\n",
        "def build_model():\n",
        "    \"\"\"Their exact model: TfidfVectorizer + LogisticRegression\"\"\"\n",
        "    return Pipeline([\n",
        "        (\"tfidf\", TfidfVectorizer(\n",
        "            max_features=TFIDF_MAX_FEATURES,\n",
        "            stop_words=\"english\"\n",
        "        )),\n",
        "        (\"clf\", LogisticRegression(max_iter=1000))\n",
        "    ])\n",
        "\n",
        "print(\"   âœ… Model built: TfidfVectorizer + LogisticRegression\")\n",
        "\n",
        "# =========================\n",
        "# UTILITIES\n",
        "# =========================\n",
        "def clear_memory():\n",
        "    gc.collect()\n",
        "\n",
        "def valid_row_mask(df):\n",
        "    \"\"\"Check if rows are valid for training\"\"\"\n",
        "    text = df[\"text\"].astype(str)\n",
        "    label = pd.to_numeric(df[\"label\"], errors=\"coerce\")\n",
        "    mask = label.notna() & text.notna()\n",
        "    mask &= (text != \"nan\") & (text.str.len() > 0)\n",
        "    mask &= label.apply(lambda x: float(x).is_integer() if pd.notna(x) else False)\n",
        "    return mask\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    return {\n",
        "        \"accuracy\": float(accuracy_score(y_true, y_pred)),\n",
        "        \"precision\": float(precision_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "        \"recall\": float(recall_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "        \"f1\": float(f1_score(y_true, y_pred, average=\"weighted\", zero_division=0)),\n",
        "    }\n",
        "\n",
        "def train_and_evaluate(df_train, df_test, allow_invalid=False):\n",
        "    \"\"\"\n",
        "    Train and evaluate model\n",
        "    \"\"\"\n",
        "    if allow_invalid:\n",
        "        # For corrupted data: fill NaN with defaults\n",
        "        train_valid = df_train.copy()\n",
        "        test_valid = df_test.copy()\n",
        "        train_valid['text'] = train_valid['text'].fillna('unknown')\n",
        "        test_valid['text'] = test_valid['text'].fillna('unknown')\n",
        "        train_valid['label'] = pd.to_numeric(train_valid['label'], errors='coerce').fillna(3).astype(int)\n",
        "        test_valid['label'] = pd.to_numeric(test_valid['label'], errors='coerce').fillna(3).astype(int)\n",
        "    else:\n",
        "        # For clean data: strict filtering\n",
        "        train_mask = valid_row_mask(df_train)\n",
        "        test_mask = valid_row_mask(df_test)\n",
        "        train_valid = df_train[train_mask].copy()\n",
        "        test_valid = df_test[test_mask].copy()\n",
        "\n",
        "    X_train = train_valid[\"text\"].astype(str).values\n",
        "    y_train = pd.to_numeric(train_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
        "    X_test = test_valid[\"text\"].astype(str).values\n",
        "    y_test = pd.to_numeric(test_valid[\"label\"], errors=\"coerce\").astype(int).values\n",
        "\n",
        "    model = build_model()\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    metrics = compute_metrics(y_test, y_pred)\n",
        "    stats = {\n",
        "        \"train_total\": len(df_train),\n",
        "        \"train_valid\": len(train_valid),\n",
        "        \"test_total\": len(df_test),\n",
        "        \"test_valid\": len(test_valid),\n",
        "    }\n",
        "\n",
        "    del model, X_train, y_train, X_test, y_test, train_valid, test_valid\n",
        "    clear_memory()\n",
        "\n",
        "    return metrics, stats\n",
        "\n",
        "# =========================\n",
        "# BASELINE EVALUATION\n",
        "# =========================\n",
        "print(\"\\nðŸ“ˆ Baseline evaluation...\")\n",
        "\n",
        "metrics_base, stats_base = train_and_evaluate(df_train_clean, df_test_clean, allow_invalid=False)\n",
        "\n",
        "print(f\"   Acc: {metrics_base['accuracy']:.4f} | \"\n",
        "      f\"Prec: {metrics_base['precision']:.4f} | \"\n",
        "      f\"Rec: {metrics_base['recall']:.4f} | \"\n",
        "      f\"F1: {metrics_base['f1']:.4f}\")\n",
        "\n",
        "all_results = []\n",
        "\n",
        "def record_result(exp_id, stage, metrics, stats):\n",
        "    row = {\"experiment\": exp_id, \"stage\": stage, **stats, **metrics}\n",
        "    all_results.append(row)\n",
        "\n",
        "record_result(\"BASELINE\", \"CLEAN\", metrics_base, stats_base)\n",
        "\n",
        "# =========================\n",
        "# CORRUPTION EXPERIMENTS (THEIR EXACT CORRUPTIONS)\n",
        "# =========================\n",
        "print(\"\\nðŸ§ª Running corruption experiments...\")\n",
        "\n",
        "experiments = [\n",
        "    (\"01_missing_values\", inject.apply_missing_values, {}),\n",
        "    (\"02_broken_chars\", inject.apply_broken_characters, {}),\n",
        "    (\"03_swapped_text\", inject.apply_swapped_text, {}),\n",
        "    (\"04_missing_labels\", inject.apply_missing_labels, {}),\n",
        "]\n",
        "\n",
        "for idx, (exp_id, corrupt_func, kwargs) in enumerate(experiments, start=1):\n",
        "    print(f\"\\n[{idx}/{len(experiments)}] {exp_id}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    # === CORRUPT ===\n",
        "    print(\"   ðŸ”´ Corrupting...\")\n",
        "    df_corrupt = corrupt_func(df_baseline.copy(), **kwargs)\n",
        "    df_corrupt.to_csv(os.path.join(FIXED_DIR, f\"corrupted/{exp_id}.csv\"), index=False)\n",
        "\n",
        "    # Split\n",
        "    df_train_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(train_ids)].copy()\n",
        "    df_test_corrupt = df_corrupt[df_corrupt[\"row_id\"].isin(test_ids)].copy()\n",
        "\n",
        "    # === EVALUATE CORRUPTED ===\n",
        "    print(\"   ðŸ”´ Training on CORRUPTED...\")\n",
        "    metrics_corrupt, stats_corrupt = train_and_evaluate(df_train_corrupt, df_test_corrupt, allow_invalid=True)\n",
        "    record_result(exp_id, \"CORRUPTED\", metrics_corrupt, stats_corrupt)\n",
        "\n",
        "    print(f\"      Acc: {metrics_corrupt['accuracy']:.4f} | \"\n",
        "          f\"Prec: {metrics_corrupt['precision']:.4f} | \"\n",
        "          f\"Rec: {metrics_corrupt['recall']:.4f} | \"\n",
        "          f\"F1: {metrics_corrupt['f1']:.4f}\")\n",
        "\n",
        "    del df_train_corrupt, df_test_corrupt\n",
        "    clear_memory()\n",
        "\n",
        "    # === CLEAN ===\n",
        "    print(\"   ðŸŸ¢ Cleaning...\")\n",
        "    df_cleaned = clean.clean_all(df_corrupt)\n",
        "    df_cleaned.to_csv(os.path.join(FIXED_DIR, f\"cleaned/{exp_id}_cleaned.csv\"), index=False)\n",
        "    del df_corrupt\n",
        "    clear_memory()\n",
        "\n",
        "    # Split\n",
        "    df_train_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(train_ids)].copy()\n",
        "    df_test_cleaned = df_cleaned[df_cleaned[\"row_id\"].isin(test_ids)].copy()\n",
        "\n",
        "    # === EVALUATE CLEANED ===\n",
        "    print(\"   ðŸŸ¢ Training on CLEANED...\")\n",
        "    metrics_cleaned, stats_cleaned = train_and_evaluate(df_train_cleaned, df_test_cleaned, allow_invalid=False)\n",
        "    record_result(exp_id, \"CLEANED\", metrics_cleaned, stats_cleaned)\n",
        "\n",
        "    print(f\"      Acc: {metrics_cleaned['accuracy']:.4f} | \"\n",
        "          f\"Prec: {metrics_cleaned['precision']:.4f} | \"\n",
        "          f\"Rec: {metrics_cleaned['recall']:.4f} | \"\n",
        "          f\"F1: {metrics_cleaned['f1']:.4f}\")\n",
        "\n",
        "    # === RECOVERY ===\n",
        "    acc_drop = metrics_base['accuracy'] - metrics_corrupt['accuracy']\n",
        "    acc_recovery = metrics_cleaned['accuracy'] - metrics_corrupt['accuracy']\n",
        "    recovery_pct = (acc_recovery / acc_drop * 100) if acc_drop != 0 else 0\n",
        "\n",
        "    print(f\"      ðŸ“Š Recovery: {recovery_pct:.1f}%\")\n",
        "\n",
        "    del df_train_cleaned, df_test_cleaned, df_cleaned\n",
        "    clear_memory()\n",
        "\n",
        "# =========================\n",
        "# SAVE RESULTS\n",
        "# =========================\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_path = os.path.join(FIXED_DIR, \"results/adult_results.csv\")\n",
        "df_results = pd.DataFrame(all_results)\n",
        "df_results.to_csv(results_path, index=False)\n",
        "\n",
        "print(f\"\\nâœ… Results saved: {results_path}\")\n",
        "print(f\"\\nðŸ“Š SUMMARY:\")\n",
        "print(df_results[['experiment', 'stage', 'accuracy', 'f1']].to_string(index=False))\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… ADULT DATASET EVALUATION COMPLETE!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHdbW8iB6F9x",
        "outputId": "07c9f413-5389-4689-b48d-45e6b29824ab"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ADULT DATASET EVALUATION - TEAM'S EXACT APPROACH\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Loading Adult Dataset...\n",
            "   Loaded: 48,842 rows\n",
            "   After dropna: 45,222 rows\n",
            "\n",
            "ðŸ”§ Converting to text/label format (EXACTLY like their notebook)...\n",
            "   Baseline created: 41,883 rows\n",
            "   text = occupation column\n",
            "   label = income (1 for <=50K, 5 for >50K)\n",
            "\n",
            "   Sample:\n",
            "   row_id               text  label\n",
            "0       0  Machine-op-inspct      1\n",
            "1       1    Farming-fishing      1\n",
            "2       2    Protective-serv      5\n",
            "\n",
            "âœ‚ï¸  Creating fixed train/test split...\n",
            "   Train: 33,506 samples\n",
            "   Test: 8,377 samples\n",
            "\n",
            "ðŸ—ï¸  Building model (EXACT same as their Amazon/Adult model)...\n",
            "   âœ… Model built: TfidfVectorizer + LogisticRegression\n",
            "\n",
            "ðŸ“ˆ Baseline evaluation...\n",
            "   Acc: 0.7528 | Prec: 0.5667 | Rec: 0.7528 | F1: 0.6466\n",
            "\n",
            "ðŸ§ª Running corruption experiments...\n",
            "\n",
            "[1/4] 01_missing_values\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.7528 | Prec: 0.5667 | Rec: 0.7528 | F1: 0.6466\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.7509 | Prec: 0.5639 | Rec: 0.7509 | F1: 0.6441\n",
            "      ðŸ“Š Recovery: 0.0%\n",
            "\n",
            "[2/4] 02_broken_chars\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.7528 | Prec: 0.5667 | Rec: 0.7528 | F1: 0.6466\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.7528 | Prec: 0.5667 | Rec: 0.7528 | F1: 0.6466\n",
            "      ðŸ“Š Recovery: 0.0%\n",
            "\n",
            "[3/4] 03_swapped_text\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.7460 | Prec: 0.5566 | Rec: 0.7460 | F1: 0.6375\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.7460 | Prec: 0.5566 | Rec: 0.7460 | F1: 0.6375\n",
            "      ðŸ“Š Recovery: 0.0%\n",
            "\n",
            "[4/4] 04_missing_labels\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”´ Corrupting...\n",
            "   ðŸ”´ Training on CORRUPTED...\n",
            "      Acc: 0.4955 | Prec: 0.2455 | Rec: 0.4955 | F1: 0.3284\n",
            "   ðŸŸ¢ Cleaning...\n",
            "   ðŸŸ¢ Training on CLEANED...\n",
            "      Acc: 0.7466 | Prec: 0.5574 | Rec: 0.7466 | F1: 0.6382\n",
            "      ðŸ“Š Recovery: 97.6%\n",
            "\n",
            "================================================================================\n",
            "SAVING RESULTS\n",
            "================================================================================\n",
            "\n",
            "âœ… Results saved: /content/drive/MyDrive/data_preparation_project_2026/fixed_adult/results/adult_results.csv\n",
            "\n",
            "ðŸ“Š SUMMARY:\n",
            "       experiment     stage  accuracy       f1\n",
            "         BASELINE     CLEAN  0.752775 0.646598\n",
            "01_missing_values CORRUPTED  0.752775 0.646598\n",
            "01_missing_values   CLEANED  0.750905 0.644076\n",
            "  02_broken_chars CORRUPTED  0.752775 0.646598\n",
            "  02_broken_chars   CLEANED  0.752775 0.646598\n",
            "  03_swapped_text CORRUPTED  0.746046 0.637537\n",
            "  03_swapped_text   CLEANED  0.746046 0.637537\n",
            "04_missing_labels CORRUPTED  0.495523 0.328371\n",
            "04_missing_labels   CLEANED  0.746569 0.638240\n",
            "\n",
            "================================================================================\n",
            "âœ… ADULT DATASET EVALUATION COMPLETE!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mwDrWMzttKRo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}