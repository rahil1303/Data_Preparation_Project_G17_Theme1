{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install py-AutoClean datasets cleanlab scikit-learn jenga ftfy pandas numpy setuptools nltk ucimlrepo category_encoders -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "from AutoClean import AutoClean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.set_config(enable_metadata_routing=True)\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from jenga.corruptions.generic import MissingValues, SwappedValues\n",
    "from jenga.corruptions.text import BrokenCharacters\n",
    "import os\n",
    "import ftfy\n",
    "from category_encoders import TargetEncoder\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### Load the data - Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "print(\"Loading Amazon Reviews 2023 (All Beauty)...\")\n",
    "\n",
    "BASE_DIR =  Path.cwd().parent / \"datasets\"\n",
    "AMAZON_DIR = os.path.join(BASE_DIR, \"amazon_reviews_2023_all_beauty\")\n",
    "os.makedirs(AMAZON_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    dataset = load_dataset(\n",
    "        \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "        \"raw_review_All_Beauty\",\n",
    "        split=\"full\",\n",
    "        streaming=False\n",
    "    )\n",
    "    \n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading from HF: {e}\")\n",
    "\n",
    "\n",
    "print(f\"âœ… Loaded {len(df)} reviews\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(df.head())\n",
    "\n",
    "# Sample 50k for faster local processing\n",
    "df_sample = df\n",
    "sample_path = os.path.join(AMAZON_DIR, \"sample_50k.csv\")\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "print(f\"âœ… Saved sample to: {sample_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPT_DIR = os.path.join(AMAZON_DIR, \"corrupted_batches\")\n",
    "ARTIFACTS_DIR = os.path.join(BASE_DIR, \"artifacts\")\n",
    "\n",
    "os.makedirs(AMAZON_DIR, exist_ok=True)\n",
    "os.makedirs(CORRUPT_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Feature extraction and model - Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\n",
    "    \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "    \"raw_review_All_Beauty\",\n",
    "    split=\"full\",\n",
    "    streaming=False\n",
    ")\n",
    "df = dataset.to_pandas()\n",
    "print(f\"âœ… Loaded {len(df)} reviews\")\n",
    "\n",
    "if \"text\" not in df.columns:\n",
    "    df[\"text\"] = (df.get(\"title\", \"\").fillna(\"\").astype(str) + \" \" + \n",
    "                  df.get(\"body\", \"\").fillna(\"\").astype(str)).str.strip()\n",
    "    \n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Find the smallest class count\n",
    "min_count = df['rating'].value_counts().min()\n",
    "\n",
    "# Downsample each class to the minimum count\n",
    "df = (\n",
    "    df.groupby('rating', group_keys=False)\n",
    "      .apply(lambda x: resample(x, replace=False, n_samples=20000, random_state=42))\n",
    ")\n",
    "\n",
    "df = df.dropna(subset=[\"rating\", \"text\"])\n",
    "df = df[df[\"text\"].str.len() > 0]\n",
    "\n",
    "df[\"label\"] = df[\"rating\"].astype(int)\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Summary:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "X = df[\"text\"].values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Train/Test Split:\")\n",
    "print(f\"   Train: {len(X_train)} samples\")\n",
    "print(f\"   Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "def preprocess_review(text):\n",
    "    \"\"\"\n",
    "    Preprocess Amazon review text using NLTK\n",
    "    \n",
    "    Transforms raw review â†’ clean tokens ready for TF-IDF\n",
    "    \"\"\"\n",
    "    # Handle missing/null values\n",
    "    if pd.isna(text) or text == \"nan\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Step 1: Remove URLs (users sometimes paste links)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Step 2: Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Step 3: Remove HTML tags (if any)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Step 4: Remove special characters but keep basic punctuation\n",
    "    # (punctuation can carry sentiment: \"!\" and \"?\" matter for reviews)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s!?.,\\'-]', '', text)\n",
    "    \n",
    "    # Step 5: Tokenization - split into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Step 6: Remove stopwords (common words: \"the\", \"a\", \"is\", \"and\")\n",
    "    # These add noise and don't help predict ratings\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Step 7: Lemmatization - reduce words to root form\n",
    "    # Examples: \"running\" â†’ \"run\", \"better\" â†’ \"good\", \"worst\" â†’ \"bad\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Step 8: Remove single characters (leftover noise like \"i\", \"a\")\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_features = FeatureUnion([\n",
    "    (\"word_tfidf\", TfidfVectorizer(\n",
    "        preprocessor=preprocess_review,\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_features=10000,\n",
    "        sublinear_tf=True,\n",
    "    )),\n",
    "    (\"char_tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=2,\n",
    "        max_features=3000,\n",
    "        sublinear_tf=True,\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Create TF-IDF + LogisticRegression pipeline\"\"\"\n",
    "    return Pipeline([\n",
    "            (\"features\", text_features),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                max_iter=2000,\n",
    "                C=1.0,\n",
    "                random_state=42,\n",
    "            ))\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "baseline_model = build_model()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate baseline\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "baseline_acc = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "y_pred = np.clip(np.round(y_pred_baseline), 1, 5)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nâœ… Baseline Model Trained\")\n",
    "print(f\"ðŸ“Š MAE: {mae:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# Save baseline accuracy\n",
    "baseline_metrics = {\n",
    "    \"accuracy\": baseline_acc,\n",
    "    \"predictions\": y_pred_baseline\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Load the Data - Clothing Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clothing = pd.read_json(\"../data/raw/renttherunway_final_data.json\", lines=True)\n",
    "print(df_clothing.head)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Feature Extraction and Model - Clothing Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fit = df_clothing.dropna(subset=['fit'])\n",
    "if \"text\" not in df_fit.columns:\n",
    "    df_fit[\"text\"] = (\n",
    "        df_fit.get(\"review_text\", \"\").fillna(\"\").astype(str)\n",
    "    ).str.strip()\n",
    "\n",
    "n_per_fit = 20000\n",
    "\n",
    "sample_fit = (\n",
    "    df_fit\n",
    "    .groupby('fit', group_keys=False)\n",
    "    .apply(lambda x: x.sample(\n",
    "        n=min(len(x), n_per_fit),\n",
    "        random_state=42\n",
    "    ))\n",
    ")\n",
    "sample_fit[\"label\"] = sample_fit[\"fit\"]\n",
    "\n",
    "\n",
    "df_rating = df_clothing.dropna(subset=['rating'])\n",
    "\n",
    "sample_rating = (\n",
    "    df_fit\n",
    "    .groupby('rating', group_keys=False)\n",
    "    .apply(lambda x: x.sample(\n",
    "        n=min(len(x), n_per_fit),\n",
    "        random_state=42\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    sample_fit['review_text'], sample_fit['fit'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_f = build_model()\n",
    "baseline_model_f.fit(X_train_f, y_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline_f = baseline_model_f.predict(X_test_f)\n",
    "baseline_acc = accuracy_score(y_test_f, y_baseline_f)\n",
    "print(baseline_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clothing = df_clothing.dropna(subset=['review_text', 'rating'])\n",
    "\n",
    "df_clothing = df_clothing.reset_index(drop=True)\n",
    "\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    sample_rating['review_text'], sample_rating['rating'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_f = build_model()\n",
    "baseline_model_f.fit(X_train_f, y_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline_f = baseline_model_f.predict(X_test_f)\n",
    "\n",
    "mae = mean_absolute_error(y_test_f, y_baseline_f)\n",
    "\n",
    "print(f\"\\nâœ… Baseline Model Trained\")\n",
    "print(f\"ðŸ“Š MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \"clf__C\": [1.0, 0.1], \n",
    "}\n",
    "\n",
    "gridsearch = GridSearchCV(\n",
    "    baseline_model_f,\n",
    "    param_grid=param_grid,\n",
    "    cv=5\n",
    ") \n",
    "gridsearch.fit(X_train_f, y_train_f)\n",
    "y_pred = gridsearch.predict(X_test_f)\n",
    "\n",
    "print(mean_absolute_error(y_test_f, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Adult Income Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "df = pd.read_csv(\"adult_for_manual_edit.csv\")\n",
    "\n",
    "target_column = df.columns[-1]  \n",
    "X = df.drop(columns=[target_column]).copy()\n",
    "y = df[[target_column]].copy()\n",
    "\n",
    "numeric_features = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "X[numeric_features] = X[numeric_features].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "categorical_features = ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
    "\n",
    "numeric_transformer = StandardScaler()\n",
    "categorical_transformer = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ],\n",
    "    sparse_threshold=0\n",
    ")\n",
    "\n",
    "clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', LogisticRegression(\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "y_test = y.values.ravel()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.fillna('0'), y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy on corrupted CSV: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import csv\n",
    "\n",
    "# Input / output CSV paths\n",
    "input_csv = \"adult_for_manual_edit.csv\"\n",
    "output_csv = \"adult_corrupted_text.csv\"\n",
    "\n",
    "# Parameters\n",
    "fraction_corrupt = 0.5   # 10% of numeric cells will be corrupted\n",
    "chars_to_inject = ['#','@','!','x','a']\n",
    "negate_fraction = 0.1   # fraction of numeric cells to make negative\n",
    "\n",
    "# Read the CSV lines\n",
    "with open(input_csv, newline='', encoding='utf-8') as f:\n",
    "    reader = list(csv.reader(f))\n",
    "    header = reader[0]\n",
    "    rows = reader[1:]\n",
    "\n",
    "# Identify numeric columns (basic heuristic: first row parseable as float)\n",
    "numeric_cols = []\n",
    "for i, col in enumerate(header):\n",
    "    try:\n",
    "        float(rows[0][i])\n",
    "        numeric_cols.append(i)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "# Corrupt rows\n",
    "for row in rows:\n",
    "    for col_idx in numeric_cols:\n",
    "        cell = row[col_idx]\n",
    "        try:\n",
    "            val = float(cell)\n",
    "        except:\n",
    "            continue  # skip non-numeric already\n",
    "\n",
    "        # --- 1) Negate some values ---\n",
    "        if random.random() < negate_fraction:\n",
    "            val = -abs(val)\n",
    "\n",
    "        # --- 2) Scale by random factor ---\n",
    "        if random.random() < fraction_corrupt:\n",
    "            factor = random.uniform(1.5, 3.0)\n",
    "            val = val * factor\n",
    "\n",
    "        # --- 3) Replace some values with '?' ---\n",
    "        if random.random() < fraction_corrupt:\n",
    "            row[col_idx] = '?'\n",
    "            continue  # skip other edits\n",
    "\n",
    "        # --- 4) Inject random character into number ---\n",
    "        if random.random() < fraction_corrupt:\n",
    "            row[col_idx] = str(val) + random.choice(chars_to_inject)\n",
    "        else:\n",
    "            row[col_idx] = str(val)\n",
    "\n",
    "# Write out corrupted CSV\n",
    "with open(output_csv, 'w', newline='', encoding='utf-8') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(header)\n",
    "    writer.writerows(rows)\n",
    "\n",
    "print(f\"âœ… Corrupted CSV saved to {output_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# --- Read corrupted CSV ---\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# --- 1) Load corrupted CSV ---\n",
    "df = pd.read_csv(\"adult_corrupted_text.csv\")\n",
    "\n",
    "# --- 2) Split features / target ---\n",
    "target_column = df.columns[-1]  # assume last column is the target\n",
    "X = df.drop(columns=[target_column])\n",
    "y = df[target_column]\n",
    "\n",
    "# --- 3) Combine X and y for resampling ---\n",
    "train = X.copy()\n",
    "train['target'] = y\n",
    "\n",
    "# --- 4) Separate classes ---\n",
    "class_1 = train[train['target'] == '<=50K']\n",
    "class_2 = train[train['target'] == '>50K']\n",
    "\n",
    "# --- 5) Resample to 20k rows each ---\n",
    "class_1_resampled = resample(class_1,\n",
    "                             replace=False,\n",
    "                             n_samples=10000,\n",
    "                             random_state=42)\n",
    "\n",
    "class_2_resampled = resample(class_2,\n",
    "                             replace=False,\n",
    "                             n_samples=10000,\n",
    "                             random_state=42)\n",
    "\n",
    "# --- 6) Combine resampled data ---\n",
    "df_balanced = pd.concat([class_1_resampled, class_2_resampled]).sample(frac=1, random_state=42)  # shuffle\n",
    "\n",
    "# --- 7) Split back into X and y ---\n",
    "X = df_balanced.drop(columns=['target'])\n",
    "y = df_balanced['target']\n",
    "# Minimal numeric coercion for corrupted numbers\n",
    "numeric_features = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "for col in numeric_features:\n",
    "    X[col] = pd.to_numeric(X[col], errors='coerce')\n",
    "\n",
    "categorical_features = ['workclass', 'marital-status', 'occupation', 'relationship', 'race', 'sex']\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', StandardScaler(), numeric_features),\n",
    "    ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_features)\n",
    "], sparse_threshold=0)\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "X_test = X_test.fillna('0')\n",
    "X_train = X_train.fillna('0')\n",
    "# y_test = y_test.loc[X_test.index]\n",
    "\n",
    "clf.fit(X_train, y_train.values.ravel())\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy on corrupted CSV: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "def fit_adult(X_a, y_a):\n",
    "\n",
    "    clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', HistGradientBoostingClassifier(\n",
    "            learning_rate=0.06,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "\n",
    "    y_test = y.values.ravel()\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_a, y_a, test_size=0.3, random_state=42)\n",
    "\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    return y_test, y_pred\n",
    "\n",
    "y = y.loc[X.index]\n",
    "y_test, y_pred = fit_adult(X, y)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "### Data Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define individual corruption methods\n",
    "\"\"\"\n",
    "\n",
    "def apply_missing_values(df, fraction=0.30):\n",
    "    \"\"\"Batch 01: Missing Values in text\"\"\"\n",
    "    df = df.copy()\n",
    "    mv = MissingValues(column=\"text\", fraction=fraction, missingness=\"MCAR\")\n",
    "    return mv.transform(df)\n",
    "\n",
    "def apply_broken_characters(df, fraction=0.25):\n",
    "    \"\"\"Batch 02: Broken Characters in text\"\"\"\n",
    "    df = df.copy()\n",
    "    bc = BrokenCharacters(column=\"text\", fraction=fraction)\n",
    "    return bc.transform(df)\n",
    "\n",
    "def apply_swapped_text(df, fraction=0.20):\n",
    "    \"\"\"Batch 03: Swapped text values\"\"\"\n",
    "    df = df.copy()\n",
    "    sv = SwappedValues(column=\"text\", fraction=fraction)\n",
    "    return sv.transform(df)\n",
    "\n",
    "def apply_missing_labels(df, fraction=0.15):\n",
    "    \"\"\"Batch 04: Missing Labels\"\"\"\n",
    "    df = df.copy()\n",
    "    mv = MissingValues(column=\"label\", fraction=fraction, missingness=\"MCAR\")\n",
    "    return mv.transform(df)\n",
    "\n",
    "def apply_swapped_labels(df, fraction=0.12):\n",
    "    \"\"\"Batch 05: Swapped Labels\"\"\"\n",
    "    df = df.copy()\n",
    "    sv = SwappedValues(column=\"label\", fraction=fraction)\n",
    "    return sv.transform(df)\n",
    "\n",
    "def apply_combined_text_corruption(df):\n",
    "    \"\"\"Batch 06: Broken Chars (10%) + Missing (8%)\"\"\"\n",
    "    df = df.copy()\n",
    "    bc = BrokenCharacters(column=\"text\", fraction=0.10)\n",
    "    df = bc.transform(df)\n",
    "    mv = MissingValues(column=\"text\", fraction=0.08, missingness=\"MCAR\")\n",
    "    return mv.transform(df)\n",
    "\n",
    "def apply_combined_text_labels(df):\n",
    "    \"\"\"Batch 07: Swapped Text (15%) + Swapped Labels (8%)\"\"\"\n",
    "    df = df.copy()\n",
    "    sv_text = SwappedValues(column=\"text\", fraction=0.15)\n",
    "    df = sv_text.transform(df)\n",
    "    sv_label = SwappedValues(column=\"label\", fraction=0.08)\n",
    "    return sv_label.transform(df)\n",
    "\n",
    "def apply_heavy_missing(df):\n",
    "    \"\"\"Batch 08: Heavy Missing - Text (25%) + Labels (10%)\"\"\"\n",
    "    df = df.copy()\n",
    "    mv_text = MissingValues(column=\"text\", fraction=0.25, missingness=\"MCAR\")\n",
    "    df = mv_text.transform(df)\n",
    "    mv_label = MissingValues(column=\"label\", fraction=0.10, missingness=\"MCAR\")\n",
    "    return mv_label.transform(df)\n",
    "\n",
    "def apply_all_corruptions(df):\n",
    "    \"\"\"Batch 09: All - Broken (8%) + Swapped (10%) + Missing (5%)\"\"\"\n",
    "    df = df.copy()\n",
    "    bc = BrokenCharacters(column=\"text\", fraction=0.08)\n",
    "    df = bc.transform(df)\n",
    "    sv_text = SwappedValues(column=\"text\", fraction=0.10)\n",
    "    df = sv_text.transform(df)\n",
    "    mv_text = MissingValues(column=\"text\", fraction=0.05, missingness=\"MCAR\")\n",
    "    df = mv_text.transform(df)\n",
    "    sv_label = SwappedValues(column=\"label\", fraction=0.05)\n",
    "    return sv_label.transform(df)\n",
    "\n",
    "print(\"âœ… Corruption functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrupt_source = df.iloc[len(X_train) + len(X_test):][[\"text\", \"label\"]].copy()\n",
    "if len(df_corrupt_source) < 5000:\n",
    "    df_corrupt_source = df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    df_corrupt_source = df_corrupt_source.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "batches_config = [\n",
    "    (\"01_missing\", apply_missing_values, {}),\n",
    "    (\"02_broken_chars\", apply_broken_characters, {}),\n",
    "    (\"03_swapped\", apply_swapped_text, {}),\n",
    "    (\"04_missing_labels\", apply_missing_labels, {}),\n",
    "    (\"05_swapped_labels\", apply_swapped_labels, {}),\n",
    "    (\"06_combined_text\", apply_combined_text_corruption, {}),\n",
    "    (\"07_combined_both\", apply_combined_text_labels, {}),\n",
    "    (\"08_heavy_missing\", apply_heavy_missing, {}),\n",
    "    (\"09_all_corruptions\", apply_all_corruptions, {}),\n",
    "]\n",
    "\n",
    "corrupted_batches = {}\n",
    "\n",
    "for batch_name, corruption_fn, kwargs in batches_config:\n",
    "    print(f\"\\nðŸ”§ Batch {batch_name.split('_')[0]}: {batch_name.replace('_', ' ').title()}\")\n",
    "    df_batch = corruption_fn(df_corrupt_source, **kwargs)\n",
    "    batch_path = os.path.join(CORRUPT_DIR, f\"batch_{batch_name}.csv\")\n",
    "    df_batch.to_csv(batch_path, index=False)\n",
    "    corrupted_batches[batch_name] = df_batch\n",
    "    print(f\"âœ… {batch_name} generated\")\n",
    "\n",
    "print(f\"\\nâœ… All 9 batches generated in {CORRUPT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.corruption.inject as inject\n",
    "from src.corruption.clean_num import clean_adult_dataset\n",
    "X_copy = X.copy()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def all_numerical_corruptions_with_y(X, y, numeric_columns=None):\n",
    "    X = X.copy()\n",
    "    y = y.copy()\n",
    "    if y.shape[1] != 1:\n",
    "        raise ValueError(\"y must be a Series or single-column DataFrame\")\n",
    "    y_name = y.columns[0]\n",
    "    y_df = y\n",
    "\n",
    "    df = pd.concat([X, y_df], axis=1)\n",
    "\n",
    "    if numeric_columns is None:\n",
    "        numeric_columns = [\n",
    "            c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])\n",
    "        ]\n",
    "\n",
    "    df_corrupted = inject.all_numerical_corruptions(\n",
    "        df, columns=numeric_columns\n",
    "    )\n",
    "\n",
    "    X_corrupted = df_corrupted.drop(columns=[y_name])\n",
    "    y_corrupted = df_corrupted[y_name]\n",
    "\n",
    "    return X_corrupted, y_corrupted\n",
    "\n",
    "\n",
    "X_corrupted, y_corrupted = all_numerical_corruptions_with_y(X_copy, y, numeric_features)\n",
    "y_test, y_pred = fit_adult(X_corrupted, y_corrupted)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "# batches_config = inject.test_functions\n",
    "\n",
    "# corrupted_batches = {}\n",
    "\n",
    "# for batch_name, corruption_fn, kwargs in batches_config:\n",
    "#     print(f\"\\nðŸ”§ Batch {batch_name.split('_')[0]}: {batch_name.replace('_', ' ').title()}\")\n",
    "#     df_batch = corruption_fn(df_corrupt_source, **kwargs)\n",
    "#     batch_path = os.path.join(CORRUPT_DIR, f\"batch_{batch_name}.csv\")\n",
    "#     df_batch.to_csv(batch_path, index=False)\n",
    "#     corrupted_batches[batch_name] = df_batch\n",
    "\n",
    "# print(f\"\\nâœ… All 9 batches generated in {CORRUPT_DIR}\")\n",
    "\n",
    "# corruption_results = {}\n",
    "\n",
    "# for batch_name, df_batch in corrupted_batches.items():\n",
    "\n",
    "#     X_corrupt = df_batch[selected_features]\n",
    "#     y_corrupt = df_batch[\"income\"]\n",
    "\n",
    "#     # Split corrupted data (80% train, 20% test)\n",
    "#     X_corrupt_train, X_corrupt_test, y_corrupt_train, y_corrupt_test = train_test_split(\n",
    "#         X_corrupt, y_corrupt,\n",
    "#         test_size=0.2,\n",
    "#         random_state=42,\n",
    "#         stratify=y_corrupt\n",
    "#     )\n",
    "\n",
    "# y_test, y_pred = fit_adult(df_corrupted, y)\n",
    "# print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# print(\"Cleaning stats:\", clean_stats)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.corruption.clean_num as clean_num \n",
    "clf = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', HistGradientBoostingClassifier(\n",
    "            learning_rate=0.06,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        ))\n",
    "    ])\n",
    "df_combined = pd.concat([X_corrupted, y_corrupted], axis=1)\n",
    "\n",
    "cleaned=clean_num.clean_adult_dataset(df_combined, clf, numeric_cols=numeric_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df, model_stats = cleaned  # unpack\n",
    "\n",
    "label_col = \"income\"  # same as used in cleaning\n",
    "\n",
    "# Split into features and target\n",
    "X_cleaned = cleaned_df.drop(columns=[label_col])\n",
    "y_cleaned = cleaned_df[label_col].copy()\n",
    "\n",
    "# Fit and evaluate\n",
    "y_test, y_pred = fit_adult(X_cleaned, y_cleaned)\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "### Retraining and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.corruption.clean import clean_all\n",
    "\n",
    "\"\"\"\n",
    "Refit baseline model on each corrupted batch and evaluate accuracy\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CELL 4: EVALUATE MODEL ON CORRUPTED BATCHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store results\n",
    "corruption_results = {}\n",
    "\n",
    "for batch_name, df_batch in corrupted_batches.items():\n",
    "\n",
    "    df_batch_clean = clean_all(df_batch)\n",
    "\n",
    "    X_corrupt = df_batch_clean[\"text\"].values\n",
    "    y_corrupt = df_batch_clean[\"label\"].values\n",
    "\n",
    "    # Split corrupted data (80% train, 20% test)\n",
    "    X_corrupt_train, X_corrupt_test, y_corrupt_train, y_corrupt_test = train_test_split(\n",
    "        X_corrupt, y_corrupt,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_corrupt\n",
    "    )\n",
    "    \n",
    "    print(f\"   Train samples: {len(X_corrupt_train)}\")\n",
    "    print(f\"   Test samples: {len(X_corrupt_test)}\")\n",
    "    \n",
    "    # Train new model on corrupted data\n",
    "    corrupted_model = build_model()\n",
    "    corrupted_model.fit(X_corrupt_train, y_corrupt_train)\n",
    "    \n",
    "    # Evaluate on corrupted test set\n",
    "    y_pred_corrupt = corrupted_model.predict(X_corrupt_test)\n",
    "    corrupt_acc = accuracy_score(y_corrupt_test, y_pred_corrupt)\n",
    "    \n",
    "    print(f\"\\n   ðŸ“Š Model Trained on Corrupted Data\")\n",
    "    print(f\"   Accuracy: {corrupt_acc:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    corruption_results[batch_name] = {\n",
    "        \"accuracy\": corrupt_acc,\n",
    "        \"train_size\": len(X_corrupt_train),\n",
    "        \"test_size\": len(X_corrupt_test),\n",
    "        \"predictions\": y_pred_corrupt,\n",
    "        \"true_labels\": y_corrupt_test,\n",
    "        \"model\": corrupted_model\n",
    "    }\n",
    "    \n",
    "    # Compare to baseline\n",
    "    accuracy_drop = baseline_acc - corrupt_acc\n",
    "    drop_percentage = (accuracy_drop / baseline_acc) * 100\n",
    "    print(f\"\\n   ðŸ“‰ Comparison to Baseline:\")\n",
    "    print(f\"      Baseline Accuracy: {baseline_acc:.4f}\")\n",
    "    print(f\"      Drop: {accuracy_drop:.4f} ({drop_percentage:.2f}%)\")\n",
    "    \n",
    "    if drop_percentage > 10:\n",
    "        print(f\"      âš ï¸  SIGNIFICANT DROP - Corruption heavily impacts model\")\n",
    "    elif drop_percentage > 5:\n",
    "        print(f\"      âš¡ MODERATE DROP - Corruption has noticeable impact\")\n",
    "    else:\n",
    "        print(f\"      âœ… MINIMAL DROP - Model is robust to this corruption\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… All corrupted batches evaluated\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
