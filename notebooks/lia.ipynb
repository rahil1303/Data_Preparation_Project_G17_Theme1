{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install datasets scikit-learn jenga pandas numpy setuptools nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "sklearn.set_config(enable_metadata_routing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from jenga.corruptions.generic import MissingValues, SwappedValues\n",
    "from jenga.corruptions.text import BrokenCharacters\n",
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### Load the data - Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "print(\"Loading Amazon Reviews 2023 (All Beauty)...\")\n",
    "\n",
    "BASE_DIR =  Path.cwd().parent / \"datasets\"\n",
    "AMAZON_DIR = os.path.join(BASE_DIR, \"amazon_reviews_2023_all_beauty\")\n",
    "os.makedirs(AMAZON_DIR, exist_ok=True)\n",
    "\n",
    "# Load dataset with streaming=False (local download)\n",
    "try:\n",
    "    dataset = load_dataset(\n",
    "        \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "        \"raw_review_All_Beauty\",\n",
    "        split=\"full\",\n",
    "        streaming=False\n",
    "    )\n",
    "    \n",
    "    df = dataset.to_pandas()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error loading from HF: {e}\")\n",
    "\n",
    "\n",
    "print(f\"âœ… Loaded {len(df)} reviews\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(df.head())\n",
    "\n",
    "# Sample 50k for faster local processing\n",
    "df_sample = df.sample(n=min(100000, len(df)), random_state=42)\n",
    "sample_path = os.path.join(AMAZON_DIR, \"sample_50k.csv\")\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "print(f\"âœ… Saved sample to: {sample_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORRUPT_DIR = os.path.join(AMAZON_DIR, \"corrupted_batches\")\n",
    "ARTIFACTS_DIR = os.path.join(BASE_DIR, \"artifacts\")\n",
    "\n",
    "os.makedirs(AMAZON_DIR, exist_ok=True)\n",
    "os.makedirs(CORRUPT_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "### Feature extraction and model - Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = load_dataset(\n",
    "    \"McAuley-Lab/Amazon-Reviews-2023\",\n",
    "    \"raw_review_All_Beauty\",\n",
    "    split=\"full\",\n",
    "    streaming=False\n",
    ")\n",
    "df = dataset.to_pandas()\n",
    "print(f\"âœ… Loaded {len(df)} reviews\")\n",
    "\n",
    "df = df.sample(n=min(15000, len(df)), random_state=42).reset_index(drop=True)\n",
    "\n",
    "if \"text\" not in df.columns:\n",
    "    df[\"text\"] = (df.get(\"title\", \"\").fillna(\"\").astype(str) + \" \" + \n",
    "                  df.get(\"body\", \"\").fillna(\"\").astype(str)).str.strip()\n",
    "\n",
    "# Clean data\n",
    "df = df.dropna(subset=[\"rating\", \"text\"])\n",
    "df = df[df[\"text\"].str.len() > 0]\n",
    "\n",
    "df[\"label\"] = df[\"rating\"].astype(int)\n",
    "\n",
    "print(f\"\\nðŸ“Š Data Summary:\")\n",
    "print(f\"   Total samples: {len(df)}\")\n",
    "print(f\"   Label distribution: {df['label'].value_counts().to_dict()}\")\n",
    "\n",
    "X = df[\"text\"].values\n",
    "y = df[\"label\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2,  \n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Train/Test Split:\")\n",
    "print(f\"   Train: {len(X_train)} samples\")\n",
    "print(f\"   Test: {len(X_test)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"punkt_tab\")  \n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "def preprocess_review(text):\n",
    "    \"\"\"\n",
    "    Preprocess Amazon review text using NLTK\n",
    "    \n",
    "    Transforms raw review â†’ clean tokens ready for TF-IDF\n",
    "    \"\"\"\n",
    "    # Handle missing/null values\n",
    "    if pd.isna(text) or text == \"nan\":\n",
    "        return \"\"\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Step 1: Remove URLs (users sometimes paste links)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # Step 2: Remove email addresses\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)\n",
    "    \n",
    "    # Step 3: Remove HTML tags (if any)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    \n",
    "    # Step 4: Remove special characters but keep basic punctuation\n",
    "    # (punctuation can carry sentiment: \"!\" and \"?\" matter for reviews)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s!?.,\\'-]', '', text)\n",
    "    \n",
    "    # Step 5: Tokenization - split into words\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Step 6: Remove stopwords (common words: \"the\", \"a\", \"is\", \"and\")\n",
    "    # These add noise and don't help predict ratings\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # Step 7: Lemmatization - reduce words to root form\n",
    "    # Examples: \"running\" â†’ \"run\", \"better\" â†’ \"good\", \"worst\" â†’ \"bad\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    \n",
    "    # Step 8: Remove single characters (leftover noise like \"i\", \"a\")\n",
    "    tokens = [token for token in tokens if len(token) > 1]\n",
    "    \n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text_features = FeatureUnion([\n",
    "    (\"word_tfidf\", TfidfVectorizer(\n",
    "        preprocessor=preprocess_review,\n",
    "        tokenizer=str.split,\n",
    "        lowercase=False,\n",
    "        stop_words=None,\n",
    "        token_pattern=None,\n",
    "        ngram_range=(1, 2),\n",
    "        min_df=2,\n",
    "        max_features=10000,\n",
    "        sublinear_tf=True,\n",
    "    )),\n",
    "    (\"char_tfidf\", TfidfVectorizer(\n",
    "        analyzer=\"char\",\n",
    "        ngram_range=(3, 5),\n",
    "        min_df=3,\n",
    "        max_features=3000,\n",
    "        sublinear_tf=True,\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def build_model():\n",
    "    \"\"\"Create TF-IDF + LogisticRegression pipeline\"\"\"\n",
    "    return Pipeline([\n",
    "            (\"features\", text_features),\n",
    "            (\"clf\", LogisticRegression(\n",
    "                max_iter=2000,\n",
    "                C=1.0,\n",
    "                random_state=42,\n",
    "                n_jobs=-1\n",
    "            ))\n",
    "        ])\n",
    "    # return Pipeline([\n",
    "    #     (\"tfidf\", TfidfVectorizer(\n",
    "    #         lowercase=True,\n",
    "    #         stop_words=\"english\",\n",
    "    #         ngram_range=(1, 2),\n",
    "    #         min_df=2,\n",
    "    #         max_features=5000,\n",
    "    #         # preprocessor=preprocess_amazon_review\n",
    "    #     )),\n",
    "    #     (\"clf\", LogisticRegression(max_iter=2000, C=1.0, class_weight=\"balanced\", random_state=42))\n",
    "    # ])\n",
    "\n",
    "baseline_model = build_model()\n",
    "baseline_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate baseline\n",
    "y_pred_baseline = baseline_model.predict(X_test)\n",
    "baseline_acc = accuracy_score(y_test, y_pred_baseline)\n",
    "\n",
    "y_pred = np.clip(np.round(y_pred_baseline), 1, 5)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nâœ… Baseline Model Trained\")\n",
    "print(f\"ðŸ“Š MAE: {mae:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_baseline))\n",
    "\n",
    "# Save baseline accuracy\n",
    "baseline_metrics = {\n",
    "    \"accuracy\": baseline_acc,\n",
    "    \"predictions\": y_pred_baseline\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### Load the Data - Clothing Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clothing = pd.read_json(\"../data/raw/renttherunway_final_data.json\", lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "### Feature Extraction and Model - Clothing Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    df_clothing['review_text'], df_clothing['fit'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clothing.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_f = build_model()\n",
    "baseline_model_f.fit(X_train_f, y_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline_f = baseline_model_f.predict(X_test_f, y_test_f)\n",
    "print(f\"Baseline Accuracy for clothing dataset: {y_baseline_f:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clothing = df_clothing.dropna(subset=['review_text', 'rating'])\n",
    "\n",
    "df_clothing = df_clothing.reset_index(drop=True)\n",
    "\n",
    "X_train_f, X_test_f, y_train_f, y_test_f = train_test_split(\n",
    "    df_clothing['review_text'], df_clothing['rating'], test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_f = build_model()\n",
    "baseline_model_f.fit(X_train_f, y_train_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline_f = baseline_model_f.predict(X_test_f)\n",
    "y_pred = np.clip(np.round(y_baseline_f), 1, 10)\n",
    "\n",
    "mae = mean_absolute_error(y_test_f, y_pred)\n",
    "\n",
    "print(f\"\\nâœ… Baseline Model Trained\")\n",
    "print(f\"ðŸ“Š MAE: {mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Data Corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Define individual corruption methods\n",
    "\"\"\"\n",
    "\n",
    "def apply_missing_values(df, fraction=0.30):\n",
    "    \"\"\"Batch 01: Missing Values in text\"\"\"\n",
    "    df = df.copy()\n",
    "    mv = MissingValues(column=\"text\", fraction=fraction, missingness=\"MCAR\")\n",
    "    return mv.transform(df)\n",
    "\n",
    "def apply_broken_characters(df, fraction=0.25):\n",
    "    \"\"\"Batch 02: Broken Characters in text\"\"\"\n",
    "    df = df.copy()\n",
    "    bc = BrokenCharacters(column=\"text\", fraction=fraction)\n",
    "    return bc.transform(df)\n",
    "\n",
    "def apply_swapped_text(df, fraction=0.20):\n",
    "    \"\"\"Batch 03: Swapped text values\"\"\"\n",
    "    df = df.copy()\n",
    "    sv = SwappedValues(column=\"text\", fraction=fraction)\n",
    "    return sv.transform(df)\n",
    "\n",
    "def apply_missing_labels(df, fraction=0.15):\n",
    "    \"\"\"Batch 04: Missing Labels\"\"\"\n",
    "    df = df.copy()\n",
    "    mv = MissingValues(column=\"label\", fraction=fraction, missingness=\"MCAR\")\n",
    "    return mv.transform(df)\n",
    "\n",
    "def apply_swapped_labels(df, fraction=0.12):\n",
    "    \"\"\"Batch 05: Swapped Labels\"\"\"\n",
    "    df = df.copy()\n",
    "    sv = SwappedValues(column=\"label\", fraction=fraction)\n",
    "    return sv.transform(df)\n",
    "\n",
    "def apply_combined_text_corruption(df):\n",
    "    \"\"\"Batch 06: Broken Chars (10%) + Missing (8%)\"\"\"\n",
    "    df = df.copy()\n",
    "    bc = BrokenCharacters(column=\"text\", fraction=0.10)\n",
    "    df = bc.transform(df)\n",
    "    mv = MissingValues(column=\"text\", fraction=0.08, missingness=\"MCAR\")\n",
    "    return mv.transform(df)\n",
    "\n",
    "def apply_combined_text_labels(df):\n",
    "    \"\"\"Batch 07: Swapped Text (15%) + Swapped Labels (8%)\"\"\"\n",
    "    df = df.copy()\n",
    "    sv_text = SwappedValues(column=\"text\", fraction=0.15)\n",
    "    df = sv_text.transform(df)\n",
    "    sv_label = SwappedValues(column=\"label\", fraction=0.08)\n",
    "    return sv_label.transform(df)\n",
    "\n",
    "def apply_heavy_missing(df):\n",
    "    \"\"\"Batch 08: Heavy Missing - Text (25%) + Labels (10%)\"\"\"\n",
    "    df = df.copy()\n",
    "    mv_text = MissingValues(column=\"text\", fraction=0.25, missingness=\"MCAR\")\n",
    "    df = mv_text.transform(df)\n",
    "    mv_label = MissingValues(column=\"label\", fraction=0.10, missingness=\"MCAR\")\n",
    "    return mv_label.transform(df)\n",
    "\n",
    "def apply_all_corruptions(df):\n",
    "    \"\"\"Batch 09: All - Broken (8%) + Swapped (10%) + Missing (5%)\"\"\"\n",
    "    df = df.copy()\n",
    "    bc = BrokenCharacters(column=\"text\", fraction=0.08)\n",
    "    df = bc.transform(df)\n",
    "    sv_text = SwappedValues(column=\"text\", fraction=0.10)\n",
    "    df = sv_text.transform(df)\n",
    "    mv_text = MissingValues(column=\"text\", fraction=0.05, missingness=\"MCAR\")\n",
    "    df = mv_text.transform(df)\n",
    "    sv_label = SwappedValues(column=\"label\", fraction=0.05)\n",
    "    return sv_label.transform(df)\n",
    "\n",
    "print(\"âœ… Corruption functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corrupt_source = df.iloc[len(X_train) + len(X_test):][[\"text\", \"label\"]].copy()\n",
    "if len(df_corrupt_source) < 5000:\n",
    "    df_corrupt_source = df.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "else:\n",
    "    df_corrupt_source = df_corrupt_source.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "batches_config = [\n",
    "    (\"01_missing\", apply_missing_values, {}),\n",
    "    (\"02_broken_chars\", apply_broken_characters, {}),\n",
    "    (\"03_swapped\", apply_swapped_text, {}),\n",
    "    (\"04_missing_labels\", apply_missing_labels, {}),\n",
    "    (\"05_swapped_labels\", apply_swapped_labels, {}),\n",
    "    (\"06_combined_text\", apply_combined_text_corruption, {}),\n",
    "    (\"07_combined_both\", apply_combined_text_labels, {}),\n",
    "    (\"08_heavy_missing\", apply_heavy_missing, {}),\n",
    "    (\"09_all_corruptions\", apply_all_corruptions, {}),\n",
    "]\n",
    "\n",
    "corrupted_batches = {}\n",
    "\n",
    "for batch_name, corruption_fn, kwargs in batches_config:\n",
    "    print(f\"\\nðŸ”§ Batch {batch_name.split('_')[0]}: {batch_name.replace('_', ' ').title()}\")\n",
    "    df_batch = corruption_fn(df_corrupt_source, **kwargs)\n",
    "    batch_path = os.path.join(CORRUPT_DIR, f\"batch_{batch_name}.csv\")\n",
    "    df_batch.to_csv(batch_path, index=False)\n",
    "    corrupted_batches[batch_name] = df_batch\n",
    "    print(f\"âœ… {batch_name} generated\")\n",
    "\n",
    "print(f\"\\nâœ… All 9 batches generated in {CORRUPT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_corrupted_data(df):\n",
    "    \"\"\"\n",
    "    Clean corrupted dataframe for model training\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Corrupted dataframe with 'text' and 'label' columns\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Cleaned dataframe ready for training\n",
    "    \"\"\"\n",
    "    df_clean = df.copy()\n",
    "    original_size = len(df_clean)\n",
    "    \n",
    "    # Step 1: Remove NaN rows\n",
    "    df_clean = df_clean.dropna(subset=[\"text\", \"label\"])\n",
    "    after_nan_removal = len(df_clean)\n",
    "    \n",
    "    # Step 2: Convert text to string and remove empty strings\n",
    "    df_clean[\"text\"] = df_clean[\"text\"].astype(str)\n",
    "    df_clean = df_clean[df_clean[\"text\"] != \"nan\"]  # Remove \"nan\" strings\n",
    "    df_clean = df_clean[df_clean[\"text\"].str.len() > 0]  # Remove empty strings\n",
    "    after_text_clean = len(df_clean)\n",
    "    \n",
    "    # Step 3: Convert labels to numeric (invalid labels â†’ NaN)\n",
    "    df_clean[\"label\"] = pd.to_numeric(df_clean[\"label\"], errors=\"coerce\")\n",
    "    after_numeric = len(df_clean)\n",
    "    \n",
    "    # Step 4: Remove rows with NaN labels\n",
    "    df_clean = df_clean.dropna(subset=[\"label\"])\n",
    "    after_label_removal = len(df_clean)\n",
    "    \n",
    "    # Step 5: Convert to integer and validate range [1-5]\n",
    "    df_clean[\"label\"] = df_clean[\"label\"].astype(int)\n",
    "    df_clean = df_clean[df_clean[\"label\"].between(1, 5)]\n",
    "    final_size = len(df_clean)\n",
    "    \n",
    "    # Print cleaning summary\n",
    "    print(f\"\\n   ðŸ§¹ Data Cleaning Summary:\")\n",
    "    print(f\"      Original samples: {original_size}\")\n",
    "    print(f\"      After NaN removal: {after_nan_removal} (-{original_size - after_nan_removal})\")\n",
    "    print(f\"      After text cleaning: {after_text_clean} (-{after_nan_removal - after_text_clean})\")\n",
    "    print(f\"      After numeric conversion: {after_numeric}\")\n",
    "    print(f\"      After bad label removal: {after_label_removal} (-{after_numeric - after_label_removal} bad labels)\")\n",
    "    print(f\"      After range validation: {final_size} (-{after_label_removal - final_size})\")\n",
    "    print(f\"      âš ï¸  Total removed: {original_size - final_size} ({(original_size - final_size) / original_size * 100:.1f}%)\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "print(\"âœ… Data cleaning function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "### Retraining and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Refit baseline model on each corrupted batch and evaluate accuracy\n",
    "\"\"\"\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"CELL 4: EVALUATE MODEL ON CORRUPTED BATCHES\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Store results\n",
    "corruption_results = {}\n",
    "\n",
    "for batch_name, df_batch in corrupted_batches.items():\n",
    "\n",
    "    df_batch_clean = clean_corrupted_data(df_batch)\n",
    "\n",
    "    X_corrupt = df_batch_clean[\"text\"].values\n",
    "    y_corrupt = df_batch_clean[\"label\"].values\n",
    "\n",
    "    # Split corrupted data (80% train, 20% test)\n",
    "    X_corrupt_train, X_corrupt_test, y_corrupt_train, y_corrupt_test = train_test_split(\n",
    "        X_corrupt, y_corrupt,\n",
    "        test_size=0.2,\n",
    "        random_state=42,\n",
    "        stratify=y_corrupt\n",
    "    )\n",
    "    \n",
    "    print(f\"   Train samples: {len(X_corrupt_train)}\")\n",
    "    print(f\"   Test samples: {len(X_corrupt_test)}\")\n",
    "    \n",
    "    # Train new model on corrupted data\n",
    "    corrupted_model = build_model()\n",
    "    corrupted_model.fit(X_corrupt_train, y_corrupt_train)\n",
    "    \n",
    "    # Evaluate on corrupted test set\n",
    "    y_pred_corrupt = corrupted_model.predict(X_corrupt_test)\n",
    "    corrupt_acc = accuracy_score(y_corrupt_test, y_pred_corrupt)\n",
    "    \n",
    "    print(f\"\\n   ðŸ“Š Model Trained on Corrupted Data\")\n",
    "    print(f\"   Accuracy: {corrupt_acc:.4f}\")\n",
    "    \n",
    "    # Store results\n",
    "    corruption_results[batch_name] = {\n",
    "        \"accuracy\": corrupt_acc,\n",
    "        \"train_size\": len(X_corrupt_train),\n",
    "        \"test_size\": len(X_corrupt_test),\n",
    "        \"predictions\": y_pred_corrupt,\n",
    "        \"true_labels\": y_corrupt_test,\n",
    "        \"model\": corrupted_model\n",
    "    }\n",
    "    \n",
    "    # Compare to baseline\n",
    "    accuracy_drop = baseline_acc - corrupt_acc\n",
    "    drop_percentage = (accuracy_drop / baseline_acc) * 100\n",
    "    print(f\"\\n   ðŸ“‰ Comparison to Baseline:\")\n",
    "    print(f\"      Baseline Accuracy: {baseline_acc:.4f}\")\n",
    "    print(f\"      Drop: {accuracy_drop:.4f} ({drop_percentage:.2f}%)\")\n",
    "    \n",
    "    if drop_percentage > 10:\n",
    "        print(f\"      âš ï¸  SIGNIFICANT DROP - Corruption heavily impacts model\")\n",
    "    elif drop_percentage > 5:\n",
    "        print(f\"      âš¡ MODERATE DROP - Corruption has noticeable impact\")\n",
    "    else:\n",
    "        print(f\"      âœ… MINIMAL DROP - Model is robust to this corruption\")\n",
    "\n",
    "print(f\"\\n{'='*70}\")\n",
    "print(f\"âœ… All corrupted batches evaluated\")\n",
    "print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipykernel",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
